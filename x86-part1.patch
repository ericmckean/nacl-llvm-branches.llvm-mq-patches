# HG changeset patch
# Parent 743118e6050146c6aa3856e20c8e7b823441e893
localmod: lib/Target/X86/CMakeLists.txt lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp lib/Target/X86/X86ISelLowering.cpp lib/Target/X86/X86InstrInfo.cpp

diff -r 743118e60501 -r 49313c73f095 lib/Target/X86/CMakeLists.txt
--- a/lib/Target/X86/CMakeLists.txt	Mon Aug 08 17:29:39 2011 -0700
+++ b/lib/Target/X86/CMakeLists.txt	Tue Aug 09 10:58:04 2011 -0700
@@ -25,8 +25,10 @@
   X86ISelDAGToDAG.cpp
   X86ISelLowering.cpp
   X86InstrInfo.cpp
+  X86InstrNaCl.cpp
   X86JITInfo.cpp
   X86MCInstLower.cpp
+  X86NaClRewritePass.cpp
   X86RegisterInfo.cpp
   X86SelectionDAGInfo.cpp
   X86Subtarget.cpp
diff -r 743118e60501 -r 49313c73f095 lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp
--- a/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp	Mon Aug 08 17:29:39 2011 -0700
+++ b/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp	Tue Aug 09 10:58:04 2011 -0700
@@ -10,6 +10,7 @@
 #include "llvm/MC/MCAsmBackend.h"
 #include "MCTargetDesc/X86BaseInfo.h"
 #include "MCTargetDesc/X86FixupKinds.h"
+#include "X86InstrNaCl.h" // @LOCALMOD
 #include "llvm/ADT/Twine.h"
 #include "llvm/MC/MCAssembler.h"
 #include "llvm/MC/MCELFObjectWriter.h"
@@ -327,6 +328,21 @@
     const MCSectionELF &ES = static_cast<const MCSectionELF&>(Section);
     return ES.getFlags() & ELF::SHF_MERGE;
   }
+
+  // @LOCALMOD-BEGIN
+  // FIXME: these really belong in a new class X86NaClAsmBackend
+  unsigned getBundleSize() const {
+    return OSType == Triple::NativeClient ? 32 : 0;
+  }
+
+  bool CustomExpandInst(const MCInst &Inst, MCStreamer &Out) const {
+    if (OSType == Triple::NativeClient) {
+      return CustomExpandInstNaCl(Inst, Out);
+    }
+    return false;
+  }
+  // @LOCALMOD-END
+
 };
 
 class ELFX86_32AsmBackend : public ELFX86AsmBackend {
diff -r 743118e60501 -r 49313c73f095 lib/Target/X86/X86ISelLowering.cpp
--- a/lib/Target/X86/X86ISelLowering.cpp	Mon Aug 08 17:29:39 2011 -0700
+++ b/lib/Target/X86/X86ISelLowering.cpp	Tue Aug 09 10:58:04 2011 -0700
@@ -170,7 +170,9 @@
   Subtarget = &TM.getSubtarget<X86Subtarget>();
   X86ScalarSSEf64 = Subtarget->hasXMMInt() || Subtarget->hasAVX();
   X86ScalarSSEf32 = Subtarget->hasXMM() || Subtarget->hasAVX();
-  X86StackPtr = Subtarget->is64Bit() ? X86::RSP : X86::ESP;
+  // @LOCALMOD-START
+  X86StackPtr = Subtarget->has64BitPointers() ? X86::RSP : X86::ESP;
+  // @LOCALMOD-END
 
   RegInfo = TM.getRegisterInfo();
   TD = getTargetData();
@@ -8389,8 +8391,9 @@
 
   SDValue Flag;
 
-  EVT SPTy = Subtarget->is64Bit() ? MVT::i64 : MVT::i32;
-  unsigned Reg = (Subtarget->is64Bit() ? X86::RAX : X86::EAX);
+  // LOCALMOD
+  EVT SPTy = getPointerTy();
+  unsigned Reg = (SPTy == MVT::i64 ? X86::RAX : X86::EAX);
 
   Chain = DAG.getCopyToReg(Chain, dl, Reg, Size, Flag);
   Flag = Chain.getValue(1);
diff -r 743118e60501 -r 49313c73f095 lib/Target/X86/X86InstrInfo.cpp
--- a/lib/Target/X86/X86InstrInfo.cpp	Mon Aug 08 17:29:39 2011 -0700
+++ b/lib/Target/X86/X86InstrInfo.cpp	Tue Aug 09 10:58:04 2011 -0700
@@ -252,91 +252,98 @@
     MemOp2RegOpTable[MemOp] = std::make_pair(RegOp, AuxInfo);
   }
 
+  // NaCl needs indirect calls to go through a reg to align the target,
+  // so, skip Reg2Mem in some cases.
+  bool isNaCl = TM.getSubtarget<X86Subtarget>().isTargetNaCl();
+
   // If the third value is 1, then it's folding either a load or a store.
-  static const unsigned OpTbl0[][4] = {
-    { X86::BT16ri8,     X86::BT16mi8, 1, 0 },
-    { X86::BT32ri8,     X86::BT32mi8, 1, 0 },
-    { X86::BT64ri8,     X86::BT64mi8, 1, 0 },
-    { X86::CALL32r,     X86::CALL32m, 1, 0 },
-    { X86::CALL64r,     X86::CALL64m, 1, 0 },
-    { X86::WINCALL64r,  X86::WINCALL64m, 1, 0 },
-    { X86::CMP16ri,     X86::CMP16mi, 1, 0 },
-    { X86::CMP16ri8,    X86::CMP16mi8, 1, 0 },
-    { X86::CMP16rr,     X86::CMP16mr, 1, 0 },
-    { X86::CMP32ri,     X86::CMP32mi, 1, 0 },
-    { X86::CMP32ri8,    X86::CMP32mi8, 1, 0 },
-    { X86::CMP32rr,     X86::CMP32mr, 1, 0 },
-    { X86::CMP64ri32,   X86::CMP64mi32, 1, 0 },
-    { X86::CMP64ri8,    X86::CMP64mi8, 1, 0 },
-    { X86::CMP64rr,     X86::CMP64mr, 1, 0 },
-    { X86::CMP8ri,      X86::CMP8mi, 1, 0 },
-    { X86::CMP8rr,      X86::CMP8mr, 1, 0 },
-    { X86::DIV16r,      X86::DIV16m, 1, 0 },
-    { X86::DIV32r,      X86::DIV32m, 1, 0 },
-    { X86::DIV64r,      X86::DIV64m, 1, 0 },
-    { X86::DIV8r,       X86::DIV8m, 1, 0 },
-    { X86::EXTRACTPSrr, X86::EXTRACTPSmr, 0, 16 },
-    { X86::FsMOVAPDrr,  X86::MOVSDmr | TB_NOT_REVERSABLE , 0, 0 },
-    { X86::FsMOVAPSrr,  X86::MOVSSmr | TB_NOT_REVERSABLE , 0, 0 },
-    { X86::IDIV16r,     X86::IDIV16m, 1, 0 },
-    { X86::IDIV32r,     X86::IDIV32m, 1, 0 },
-    { X86::IDIV64r,     X86::IDIV64m, 1, 0 },
-    { X86::IDIV8r,      X86::IDIV8m, 1, 0 },
-    { X86::IMUL16r,     X86::IMUL16m, 1, 0 },
-    { X86::IMUL32r,     X86::IMUL32m, 1, 0 },
-    { X86::IMUL64r,     X86::IMUL64m, 1, 0 },
-    { X86::IMUL8r,      X86::IMUL8m, 1, 0 },
-    { X86::JMP32r,      X86::JMP32m, 1, 0 },
-    { X86::JMP64r,      X86::JMP64m, 1, 0 },
-    { X86::MOV16ri,     X86::MOV16mi, 0, 0 },
-    { X86::MOV16rr,     X86::MOV16mr, 0, 0 },
-    { X86::MOV32ri,     X86::MOV32mi, 0, 0 },
-    { X86::MOV32rr,     X86::MOV32mr, 0, 0 },
-    { X86::MOV64ri32,   X86::MOV64mi32, 0, 0 },
-    { X86::MOV64rr,     X86::MOV64mr, 0, 0 },
-    { X86::MOV8ri,      X86::MOV8mi, 0, 0 },
-    { X86::MOV8rr,      X86::MOV8mr, 0, 0 },
-    { X86::MOV8rr_NOREX, X86::MOV8mr_NOREX, 0, 0 },
-    { X86::MOVAPDrr,    X86::MOVAPDmr, 0, 16 },
-    { X86::MOVAPSrr,    X86::MOVAPSmr, 0, 16 },
-    { X86::MOVDQArr,    X86::MOVDQAmr, 0, 16 },
-    { X86::VMOVAPDYrr,  X86::VMOVAPDYmr, 0, 32 },
-    { X86::VMOVAPSYrr,  X86::VMOVAPSYmr, 0, 32 },
-    { X86::VMOVDQAYrr,  X86::VMOVDQAYmr, 0, 32 },
-    { X86::MOVPDI2DIrr, X86::MOVPDI2DImr, 0, 0 },
-    { X86::MOVPQIto64rr,X86::MOVPQI2QImr, 0, 0 },
-    { X86::MOVSDto64rr, X86::MOVSDto64mr, 0, 0 },
-    { X86::MOVSS2DIrr,  X86::MOVSS2DImr, 0, 0 },
-    { X86::MOVUPDrr,    X86::MOVUPDmr, 0, 0 },
-    { X86::MOVUPSrr,    X86::MOVUPSmr, 0, 0 },
-    { X86::VMOVUPDYrr,  X86::VMOVUPDYmr, 0, 0 },
-    { X86::VMOVUPSYrr,  X86::VMOVUPSYmr, 0, 0 },
-    { X86::MUL16r,      X86::MUL16m, 1, 0 },
-    { X86::MUL32r,      X86::MUL32m, 1, 0 },
-    { X86::MUL64r,      X86::MUL64m, 1, 0 },
-    { X86::MUL8r,       X86::MUL8m, 1, 0 },
-    { X86::SETAEr,      X86::SETAEm, 0, 0 },
-    { X86::SETAr,       X86::SETAm, 0, 0 },
-    { X86::SETBEr,      X86::SETBEm, 0, 0 },
-    { X86::SETBr,       X86::SETBm, 0, 0 },
-    { X86::SETEr,       X86::SETEm, 0, 0 },
-    { X86::SETGEr,      X86::SETGEm, 0, 0 },
-    { X86::SETGr,       X86::SETGm, 0, 0 },
-    { X86::SETLEr,      X86::SETLEm, 0, 0 },
-    { X86::SETLr,       X86::SETLm, 0, 0 },
-    { X86::SETNEr,      X86::SETNEm, 0, 0 },
-    { X86::SETNOr,      X86::SETNOm, 0, 0 },
-    { X86::SETNPr,      X86::SETNPm, 0, 0 },
-    { X86::SETNSr,      X86::SETNSm, 0, 0 },
-    { X86::SETOr,       X86::SETOm, 0, 0 },
-    { X86::SETPr,       X86::SETPm, 0, 0 },
-    { X86::SETSr,       X86::SETSm, 0, 0 },
-    { X86::TAILJMPr,    X86::TAILJMPm, 1, 0 },
-    { X86::TAILJMPr64,  X86::TAILJMPm64, 1, 0 },
-    { X86::TEST16ri,    X86::TEST16mi, 1, 0 },
-    { X86::TEST32ri,    X86::TEST32mi, 1, 0 },
-    { X86::TEST64ri32,  X86::TEST64mi32, 1, 0 },
-    { X86::TEST8ri,     X86::TEST8mi, 1, 0 }
+  // The fourth value is alignment.
+  // If the fifth value is true reg2mem is allowed.
+  // If the sixth value is true mem2reg is allowed.
+  static const unsigned OpTbl0[][6] = {
+    { X86::BT16ri8,       X86::BT16mi8,       1, 0,  true,    true },
+    { X86::BT32ri8,       X86::BT32mi8,       1, 0,  true,    true },
+    { X86::BT64ri8,       X86::BT64mi8,       1, 0,  true,    true },
+    { X86::CALL32r,       X86::CALL32m,       1, 0,  !isNaCl, true },
+    { X86::CALL64r,       X86::CALL64m,       1, 0,  !isNaCl, true },
+    { X86::WINCALL64r,    X86::WINCALL64m,    1, 0,  true,    true },
+    { X86::CMP16ri,       X86::CMP16mi,       1, 0,  true,    true },
+    { X86::CMP16ri8,      X86::CMP16mi8,      1, 0,  true,    true },
+    { X86::CMP16rr,       X86::CMP16mr,       1, 0,  true,    true },
+    { X86::CMP32ri,       X86::CMP32mi,       1, 0,  true,    true },
+    { X86::CMP32ri8,      X86::CMP32mi8,      1, 0,  true,    true },
+    { X86::CMP32rr,       X86::CMP32mr,       1, 0,  true,    true },
+    { X86::CMP64ri32,     X86::CMP64mi32,     1, 0,  true,    true },
+    { X86::CMP64ri8,      X86::CMP64mi8,      1, 0,  true,    true },
+    { X86::CMP64rr,       X86::CMP64mr,       1, 0,  true,    true },
+    { X86::CMP8ri,        X86::CMP8mi,        1, 0,  true,    true },
+    { X86::CMP8rr,        X86::CMP8mr,        1, 0,  true,    true },
+    { X86::DIV16r,        X86::DIV16m,        1, 0,  true,    true },
+    { X86::DIV32r,        X86::DIV32m,        1, 0,  true,    true },
+    { X86::DIV64r,        X86::DIV64m,        1, 0,  true,    true },
+    { X86::DIV8r,         X86::DIV8m,         1, 0,  true,    true },
+    { X86::EXTRACTPSrr,   X86::EXTRACTPSmr,   0, 16, true,    true },
+    { X86::FsMOVAPDrr,    X86::MOVSDmr | TB_NOT_REVERSABLE,       0, 0,  true,    false },
+    { X86::FsMOVAPSrr,    X86::MOVSSmr | TB_NOT_REVERSABLE,       0, 0,  true,    false },
+    { X86::IDIV16r,       X86::IDIV16m,       1, 0,  true,    true },
+    { X86::IDIV32r,       X86::IDIV32m,       1, 0,  true,    true },
+    { X86::IDIV64r,       X86::IDIV64m,       1, 0,  true,    true },
+    { X86::IDIV8r,        X86::IDIV8m,        1, 0,  true,    true },
+    { X86::IMUL16r,       X86::IMUL16m,       1, 0,  true,    true },
+    { X86::IMUL32r,       X86::IMUL32m,       1, 0,  true,    true },
+    { X86::IMUL64r,       X86::IMUL64m,       1, 0,  true,    true },
+    { X86::IMUL8r,        X86::IMUL8m,        1, 0,  true,    true },
+    { X86::JMP32r,        X86::JMP32m,        1, 0,  !isNaCl, true },
+    { X86::JMP64r,        X86::JMP64m,        1, 0,  !isNaCl, true },
+    { X86::MOV16ri,       X86::MOV16mi,       0, 0,  true,    true },
+    { X86::MOV16rr,       X86::MOV16mr,       0, 0,  true,    true },
+    { X86::MOV32ri,       X86::MOV32mi,       0, 0,  true,    true },
+    { X86::MOV32rr,       X86::MOV32mr,       0, 0,  true,    true },
+    { X86::MOV64ri32,     X86::MOV64mi32,     0, 0,  true,    true },
+    { X86::MOV64rr,       X86::MOV64mr,       0, 0,  true,    true },
+    { X86::MOV8ri,        X86::MOV8mi,        0, 0,  true,    true },
+    { X86::MOV8rr,        X86::MOV8mr,        0, 0,  true,    true },
+    { X86::MOV8rr_NOREX,  X86::MOV8mr_NOREX,  0, 0,  true,    true },
+     { X86::MOVAPDrr,    X86::MOVAPDmr,        0, 16, true,    true },
+     { X86::MOVAPSrr,    X86::MOVAPSmr,        0, 16, true,    true },
+     { X86::MOVDQArr,    X86::MOVDQAmr,        0, 16, true,    true },
+    { X86::VMOVAPDYrr,    X86::VMOVAPDYmr,    0, 32, true,    true },
+    { X86::VMOVAPSYrr,    X86::VMOVAPSYmr,    0, 32, true,    true },
+    { X86::VMOVDQAYrr,    X86::VMOVDQAYmr,    0, 32  true,    true },
+    { X86::MOVPDI2DIrr,   X86::MOVPDI2DImr,   0, 0,  true,    true },
+    { X86::MOVPQIto64rr,  X86::MOVPQI2QImr,   0, 0,  true,    true },
+    { X86::MOVSDto64rr,   X86::MOVSDto64mr,   0, 0,  true,    true },
+    { X86::MOVSS2DIrr,    X86::MOVSS2DImr,    0, 0,  true,    true },
+    { X86::MOVUPDrr,      X86::MOVUPDmr,      0, 0,  true,    true },
+    { X86::MOVUPSrr,      X86::MOVUPSmr,      0, 0,  true,    true },
+     { X86::VMOVUPDYrr,  X86::VMOVUPDYmr,      0, 0,  true,    true },
+     { X86::VMOVUPSYrr,  X86::VMOVUPSYmr,      0, 0,  true,    true },
+    { X86::MUL16r,        X86::MUL16m,        1, 0,  true,    true },
+    { X86::MUL32r,        X86::MUL32m,        1, 0,  true,    true },
+    { X86::MUL64r,        X86::MUL64m,        1, 0,  true,    true },
+    { X86::MUL8r,         X86::MUL8m,         1, 0,  true,    true },
+    { X86::SETAEr,        X86::SETAEm,        0, 0,  true,    true },
+    { X86::SETAr,         X86::SETAm,         0, 0,  true,    true },
+    { X86::SETBEr,        X86::SETBEm,        0, 0,  true,    true },
+    { X86::SETBr,         X86::SETBm,         0, 0,  true,    true },
+    { X86::SETEr,         X86::SETEm,         0, 0,  true,    true },
+    { X86::SETGEr,        X86::SETGEm,        0, 0,  true,    true },
+    { X86::SETGr,         X86::SETGm,         0, 0,  true,    true },
+    { X86::SETLEr,        X86::SETLEm,        0, 0,  true,    true },
+    { X86::SETLr,         X86::SETLm,         0, 0,  true,    true },
+    { X86::SETNEr,        X86::SETNEm,        0, 0,  true,    true },
+    { X86::SETNOr,        X86::SETNOm,        0, 0,  true,    true },
+    { X86::SETNPr,        X86::SETNPm,        0, 0,  true,    true },
+    { X86::SETNSr,        X86::SETNSm,        0, 0,  true,    true },
+    { X86::SETOr,         X86::SETOm,         0, 0,  true,    true },
+    { X86::SETPr,         X86::SETPm,         0, 0,  true,    true },
+    { X86::SETSr,         X86::SETSm,         0, 0,  true,    true },
+    { X86::TAILJMPr,      X86::TAILJMPm,      1, 0,  !isNaCl, true },
+    { X86::TAILJMPr64,    X86::TAILJMPm64,    1, 0,  !isNaCl, true },
+    { X86::TEST16ri,      X86::TEST16mi,      1, 0,  true,    true },
+    { X86::TEST32ri,      X86::TEST32mi,      1, 0,  true,    true },
+    { X86::TEST64ri32,    X86::TEST64mi32,    1, 0,  true,    true },
+    { X86::TEST8ri,       X86::TEST8mi,       1, 0,  true,    true }
   };
 
   for (unsigned i = 0, e = array_lengthof(OpTbl0); i != e; ++i) {
@@ -2073,6 +2080,10 @@
       return load ? X86::MOVSSrm : X86::MOVSSmr;
     if (X86::RFP32RegClass.hasSubClassEq(RC))
       return load ? X86::LD_Fp32m : X86::ST_Fp32m;
+    // @LOCALMOD-START
+    if (X86::GR32_TC_64RegClass.hasSubClassEq(RC))
+      return load ? X86::MOV32rm : X86::MOV32mr;
+    // @LOCALMOD-END
     llvm_unreachable("Unknown 4-byte regclass");
   case 8:
     if (X86::GR64RegClass.hasSubClassEq(RC))
