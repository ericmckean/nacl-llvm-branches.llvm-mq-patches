# HG changeset patch
# User David Meyer <pdox@google.com>
# Date 1296702320 28800
# Branch pnacl-sfi
# Node ID 2063d587d444442e03d2b12a8106de7975824e97
# Parent 2b13dadc8fede393faa0a70b1e2ddbe21a36a724
# Parent  502c5f27f8043ecb31fd815e0d6e236620f5258e
Merged up to r124151

 From llvm-pnacl-0000-220-2063d587d444442e03d2b12a8106de7975824e97.patch

 From .hg/patches/llvm-pnacl-0000-220-2063d587d444442e03d2b12a8106de7975824e97.patch.stripped

diff -r 2b13dadc8fed Makefile
--- a/Makefile	Sat Feb 19 00:38:40 2011 +0000
+++ b/Makefile	Thu Jun 09 18:06:40 2011 -0700
@@ -43,6 +43,11 @@
 
 include $(LEVEL)/Makefile.config
 
+ifeq ($(NACL_SANDBOX),1)
+  DIRS := $(filter-out tools/llvm-shlib runtime docs unittests, $(DIRS))
+  OPTIONAL_DIRS :=
+endif
+
 ifneq ($(ENABLE_SHARED),1)
   DIRS := $(filter-out tools/llvm-shlib, $(DIRS))
 endif
@@ -124,6 +129,7 @@
 	(unset SDKROOT; \
 	 $(MAKE) -C BuildTools \
 	  BUILD_DIRS_ONLY=1 \
+	  NACL_SANDBOX=0 \
 	  UNIVERSAL= \
 	  ENABLE_OPTIMIZED=$(ENABLE_OPTIMIZED) \
 	  ENABLE_PROFILING=$(ENABLE_PROFILING) \
diff -r 2b13dadc8fed Makefile.rules
--- a/Makefile.rules	Sat Feb 19 00:38:40 2011 +0000
+++ b/Makefile.rules	Thu Jun 09 18:06:40 2011 -0700
@@ -577,6 +577,11 @@
 endif
 endif
 
+ifeq ($(NACL_SANDBOX),1)
+  LIBS += -lsrpc -lpthread -lm -lnacl -lnosys
+else
+  LIBS +=
+endif
 
 #----------------------------------------------------------
 # Options To Invoke Tools
diff -r 2b13dadc8fed autoconf/config.sub
--- a/autoconf/config.sub	Sat Feb 19 00:38:40 2011 +0000
+++ b/autoconf/config.sub	Thu Jun 09 18:06:40 2011 -0700
@@ -237,6 +237,10 @@
 		basic_machine=m68k-atari
 		os=-mint
 		;;
+        -nacl*)
+                basic_machine=i686-pc
+                os=-nacl
+                ;;
 esac
 
 # Decode aliases for certain CPU-COMPANY combinations.
@@ -317,6 +321,14 @@
 	i*86 | x86_64)
 	  basic_machine=$basic_machine-pc
 	  ;;
+        nacl64*)
+          basic_machine=x86_64-pc
+          os=-nacl
+          ;;
+        nacl*)
+          basic_machine=i686-pc
+          os=-nacl
+          ;;
 	# Object if more than one company name word.
 	*-*-*)
 		echo Invalid configuration \`$1\': machine \`$basic_machine\' not recognized 1>&2
@@ -1311,6 +1323,9 @@
 			;;
 		esac
 		;;
+        -nacl*)
+                os=-nacl
+                ;;
 	-nto-qnx*)
 		;;
 	-nto*)
diff -r 2b13dadc8fed autoconf/configure.ac
--- a/autoconf/configure.ac	Sat Feb 19 00:38:40 2011 +0000
+++ b/autoconf/configure.ac	Thu Jun 09 18:06:40 2011 -0700
@@ -253,6 +253,11 @@
     llvm_cv_no_link_all_option="-Wl,--no-whole-archive"
     llvm_cv_os_type="Freestanding"
     llvm_cv_platform_type="Unix" ;;
+  *-*-nacl*)
+    llvm_cv_link_all_option="-Wl,--whole-archive"
+    llvm_cv_no_link_all_option="-Wl,--no-whole-archive"
+    llvm_cv_os_type="Freestanding"
+    llvm_cv_platform_type="Unix" ;;
   *)
     llvm_cv_link_all_option=""
     llvm_cv_no_link_all_option=""
diff -r 2b13dadc8fed configure
--- a/configure	Sat Feb 19 00:38:40 2011 +0000
+++ b/configure	Thu Jun 09 18:06:40 2011 -0700
@@ -2285,6 +2285,11 @@
     llvm_cv_no_link_all_option="-Wl,--no-whole-archive"
     llvm_cv_os_type="Freestanding"
     llvm_cv_platform_type="Unix" ;;
+  *-*-nacl*)
+    llvm_cv_link_all_option="-Wl,--whole-archive"
+    llvm_cv_no_link_all_option="-Wl,--no-whole-archive"
+    llvm_cv_os_type="Freestanding"
+    llvm_cv_platform_type="Unix" ;;
   *)
     llvm_cv_link_all_option=""
     llvm_cv_no_link_all_option=""
diff -r 2b13dadc8fed include/llvm/ADT/Triple.h
--- a/include/llvm/ADT/Triple.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/ADT/Triple.h	Thu Jun 09 18:06:40 2011 -0700
@@ -84,7 +84,9 @@
     FreeBSD,
     Linux,
     Lv2,        // PS3
-    MinGW32,    // i*86-pc-mingw32, *-w64-mingw32
+    MinGW32,
+    MinGW64,
+    NativeClient,
     NetBSD,
     OpenBSD,
     Psp,
diff -r 2b13dadc8fed include/llvm/CodeGen/AsmPrinter.h
--- a/include/llvm/CodeGen/AsmPrinter.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/CodeGen/AsmPrinter.h	Thu Jun 09 18:06:40 2011 -0700
@@ -258,6 +258,11 @@
     virtual bool
     isBlockOnlyReachableByFallthrough(const MachineBasicBlock *MBB) const;
 
+    // @LOCALMOD-START
+    /// UseReadOnlyJumpTables() - true if JumpTableInfo must be in rodata.
+    virtual bool UseReadOnlyJumpTables() const { return false; }
+    // @LOCALMOD-END
+
     //===------------------------------------------------------------------===//
     // Symbol Lowering Routines.
     //===------------------------------------------------------------------===//
@@ -358,6 +363,15 @@
     /// GetSizeOfEncodedValue - Return the size of the encoding in bytes.
     unsigned GetSizeOfEncodedValue(unsigned Encoding) const;
 
+    /// @LOCALMOD-START
+    /// GetTargetLabelAlign - Get optional alignment for TargetOpcode
+    /// labels E.g., EH_LABEL.
+    virtual unsigned GetTargetLabelAlign(const MachineInstr *MI) const {
+      return 0;
+    }
+    /// @LOCALMOD-END
+
+
     /// EmitReference - Emit a reference to a label with a specified encoding.
     ///
     void EmitReference(const MCSymbol *Sym, unsigned Encoding) const;
diff -r 2b13dadc8fed include/llvm/CodeGen/MachineConstantPool.h
--- a/include/llvm/CodeGen/MachineConstantPool.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/CodeGen/MachineConstantPool.h	Thu Jun 09 18:06:40 2011 -0700
@@ -55,6 +55,17 @@
 
   virtual void AddSelectionDAGCSEId(FoldingSetNodeID &ID) = 0;
 
+  // @LOCALMOD-START
+  /// getJumpTableIndex - Check if this is a reference to a jump table.
+  /// If so, return a pointer to the jump table index value that is stored
+  /// in the constant pool, else return 0.
+  /// The default behavior is to indicate that the value is not a jump table
+  /// index. This is used by BranchFolder::runOnMachineFunction() and only in
+  /// conjunction with ARM targets
+  /// TODO: this should be cleaned up as it does tripple duty: tester, setter, getter
+  virtual unsigned *getJumpTableIndex() { return 0; }
+  // @LOCALMOD-END
+
   /// print - Implement operator<<
   virtual void print(raw_ostream &O) const = 0;
 };
diff -r 2b13dadc8fed include/llvm/CodeGen/MachineInstrBuilder.h
--- a/include/llvm/CodeGen/MachineInstrBuilder.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/CodeGen/MachineInstrBuilder.h	Thu Jun 09 18:06:40 2011 -0700
@@ -206,6 +206,21 @@
   return MachineInstrBuilder(MI);
 }
 
+// @LOCALMOD-BEGIN
+/// BuildMI - This version of the builder inserts the newly-built
+/// instruction before the given position in the given MachineBasicBlock,
+/// does NOT take a destination register, and does not add implicit operands.
+///
+inline MachineInstrBuilder BuildMI_NoImp(MachineBasicBlock &BB,
+                                         MachineBasicBlock::iterator I,
+                                         DebugLoc DL,
+                                         const TargetInstrDesc &TID) {
+  MachineInstr *MI = BB.getParent()->CreateMachineInstr(TID, DL, true);
+  BB.insert(I, MI);
+  return MachineInstrBuilder(MI);
+}
+// @LOCALMOD-END
+
 /// BuildMI - This version of the builder inserts the newly-built
 /// instruction at the end of the given MachineBasicBlock, and does NOT take a
 /// destination register.
diff -r 2b13dadc8fed include/llvm/MC/MCAsmLayout.h
--- a/include/llvm/MC/MCAsmLayout.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/MC/MCAsmLayout.h	Thu Jun 09 18:06:40 2011 -0700
@@ -79,6 +79,11 @@
   /// \brief Get the offset of the given fragment inside its containing section.
   uint64_t getFragmentOffset(const MCFragment *F) const;
 
+  // @LOCALMOD-BEGIN
+  /// \brief Get the bundle padding of the given fragment.
+  uint8_t getFragmentPadding(const MCFragment *F) const;
+  // @LOCALMOD-END
+
   /// @}
   /// @name Utility Functions
   /// @{
diff -r 2b13dadc8fed include/llvm/MC/MCAssembler.h
--- a/include/llvm/MC/MCAssembler.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/MC/MCAssembler.h	Thu Jun 09 18:06:40 2011 -0700
@@ -53,9 +53,18 @@
     FT_Org,
     FT_Dwarf,
     FT_DwarfFrame,
-    FT_LEB
+    FT_LEB,
+    FT_Tiny           // @LOCALMOD
   };
 
+  // @LOCALMOD-BEGIN
+  enum BundleAlignType {
+    BundleAlignNone  = 0,
+    BundleAlignStart = 1,
+    BundleAlignEnd   = 2
+  };
+  // @LOCALMOD-END
+
 private:
   FragmentType Kind;
 
@@ -79,6 +88,16 @@
   /// LayoutOrder - The layout order of this fragment.
   unsigned LayoutOrder;
 
+  // @LOCALMOD-BEGIN
+  BundleAlignType BundleAlign : 2;
+  bool BundleGroupStart       : 1;
+  bool BundleGroupEnd         : 1;
+
+  /// BundlePadding - The computed padding for this fragment. This is ~0
+  /// until initialized.
+  uint8_t BundlePadding;
+  // @LOCALMOD-END
+
   /// @}
 
 protected:
@@ -100,13 +119,45 @@
   unsigned getLayoutOrder() const { return LayoutOrder; }
   void setLayoutOrder(unsigned Value) { LayoutOrder = Value; }
 
+  // @LOCALMOD-BEGIN
+  bool isBundleGroupStart() const { return BundleGroupStart; }
+  void setBundleGroupStart(bool Value) { BundleGroupStart = Value; }
+
+  bool isBundleGroupEnd() const { return BundleGroupEnd; }
+  void setBundleGroupEnd(bool Value) { BundleGroupEnd = Value; }
+
+  BundleAlignType getBundleAlign() const { return BundleAlign; }
+  void setBundleAlign(BundleAlignType Value) { BundleAlign = Value; }
+  // @LOCALMOD-END
+
   static bool classof(const MCFragment *O) { return true; }
 
   void dump();
 };
 
+// @LOCALMOD-BEGIN
+// This is just a tiny data fragment with no fixups.
+// (To help with memory usage)
+class MCTinyFragment : public MCFragment {
+ private:
+  SmallString<6> Contents;
+
+ public:
+
+  MCTinyFragment(MCSectionData *SD = 0) : MCFragment(FT_Tiny, SD) {}
+
+  SmallString<6> &getContents() { return Contents; }
+  const SmallString<6> &getContents() const { return Contents; }
+
+  static bool classof(const MCFragment *F) {
+    return F->getKind() == MCFragment::FT_Tiny;
+  }
+  static bool classof(const MCTinyFragment *) { return true; }
+};
+// @LOCALMOD-END
+
 class MCDataFragment : public MCFragment {
-  SmallString<32> Contents;
+  SmallString<6> Contents;  // @LOCALMOD: Memory efficiency
 
   /// Fixups - The list of fixups in this fragment.
   std::vector<MCFixup> Fixups;
@@ -121,8 +172,8 @@
   /// @name Accessors
   /// @{
 
-  SmallString<32> &getContents() { return Contents; }
-  const SmallString<32> &getContents() const { return Contents; }
+  SmallString<6> &getContents() { return Contents; }  // @LOCALMOD
+  const SmallString<6> &getContents() const { return Contents; } // @LOCALMOD
 
   /// @}
   /// @name Fixup Access
@@ -460,6 +511,21 @@
   /// it.
   unsigned HasInstructions : 1;
 
+  // @LOCALMOD-BEGIN
+  bool BundlingEnabled;
+  bool BundleLocked;
+
+  // Because ".bundle_lock" occurs before the fragment it applies to exists,
+  // we need to keep this flag around so we know to mark the next fragment
+  // as the start of a bundle group. A similar flag is not necessary for the
+  // last fragment, because when a .bundle_unlock occurs, the last fragment
+  // in the group already exists and can be marked directly.
+  bool BundleGroupFirstFrag;
+
+  typedef MCFragment::BundleAlignType BundleAlignType;
+  BundleAlignType BundleAlignNext;
+  // @LOCALMOD-END
+
   /// @}
 
 public:
@@ -481,6 +547,20 @@
   unsigned getLayoutOrder() const { return LayoutOrder; }
   void setLayoutOrder(unsigned Value) { LayoutOrder = Value; }
 
+  // @LOCALMOD-BEGIN
+  bool isBundlingEnabled() const { return BundlingEnabled; }
+
+  bool isBundleLocked() const { return BundleLocked; }
+  void setBundleLocked(bool Value) { BundleLocked = Value; }
+
+  bool isBundleGroupFirstFrag() const { return BundleGroupFirstFrag; }
+  void setBundleGroupFirstFrag(bool Value) { BundleGroupFirstFrag = Value; }
+
+
+  BundleAlignType getBundleAlignNext() const { return BundleAlignNext; }
+  void setBundleAlignNext(BundleAlignType Value) { BundleAlignNext = Value; }
+  // @LOCALMOD-END
+
   /// @name Fragment Access
   /// @{
 
@@ -724,6 +804,13 @@
   bool FragmentNeedsRelaxation(const MCInstFragment *IF,
                                const MCAsmLayout &Layout) const;
 
+  // @LOCALMOD-BEGIN
+  uint8_t ComputeBundlePadding(const MCAsmLayout &Layout,
+                               MCFragment *F,
+                               uint64_t FragmentOffset) const;
+  // @LOCALMOD-END
+
+
   /// LayoutOnce - Perform one layout iteration and return true if any offsets
   /// were adjusted.
   bool LayoutOnce(MCAsmLayout &Layout);
@@ -789,6 +876,12 @@
 
   TargetAsmBackend &getBackend() const { return Backend; }
 
+  // @LOCALMOD-BEGIN
+  uint64_t getBundleSize() const;
+  uint64_t getBundleMask() const;
+  // @LOCALMOD-END
+
+
   MCCodeEmitter &getEmitter() const { return Emitter; }
 
   MCObjectWriter &getWriter() const { return Writer; }
diff -r 2b13dadc8fed include/llvm/MC/MCObjectStreamer.h
--- a/include/llvm/MC/MCObjectStreamer.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/MC/MCObjectStreamer.h	Thu Jun 09 18:06:40 2011 -0700
@@ -61,6 +61,14 @@
   virtual void EmitLabel(MCSymbol *Symbol);
   virtual void EmitValueImpl(const MCExpr *Value, unsigned Size,
                              bool isPCRel, unsigned AddrSpace);
+                             
+  // @LOCALMOD-BEGIN
+  void EmitBundleLock();
+  void EmitBundleUnlock();
+  void EmitBundleAlignStart();
+  void EmitBundleAlignEnd();
+  // @LOCALMOD-END
+
   virtual void EmitULEB128Value(const MCExpr *Value, unsigned AddrSpace = 0);
   virtual void EmitSLEB128Value(const MCExpr *Value, unsigned AddrSpace = 0);
   virtual void EmitWeakReference(MCSymbol *Alias, const MCSymbol *Symbol);
diff -r 2b13dadc8fed include/llvm/MC/MCStreamer.h
--- a/include/llvm/MC/MCStreamer.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/MC/MCStreamer.h	Thu Jun 09 18:06:40 2011 -0700
@@ -413,6 +413,27 @@
 
     /// @}
 
+    // @LOCALMOD-BEGIN
+    /// @name Bundling Directives
+    /// @{
+
+    /// EmitBundleLock - Begin a group of instructions which cannot
+    /// cross a bundle boundary.
+    virtual void EmitBundleLock() = 0;
+
+    /// EmitBundleUnlock - End a bundle-locked group of instructions.
+    virtual void EmitBundleUnlock() = 0;
+
+    /// EmitBundleAlignStart - Guarantee that the next instruction or
+    /// bundle-locked group starts at the beginning of a bundle.
+    virtual void EmitBundleAlignStart() = 0;
+
+    /// EmitBundleAlignEnd - Guarantee that the next instruction or
+    /// bundle-locked group finishes at the end of a bundle.
+    virtual void EmitBundleAlignEnd() = 0;
+    /// @}
+    // @LOCALMOD-END
+
     /// EmitFileDirective - Switch to a new logical file.  This is used to
     /// implement the '.file "foo.c"' assembler directive.
     virtual void EmitFileDirective(StringRef Filename) = 0;
diff -r 2b13dadc8fed include/llvm/Support/ELF.h
--- a/include/llvm/Support/ELF.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/Support/ELF.h	Thu Jun 09 18:06:40 2011 -0700
@@ -179,9 +179,25 @@
   ELFOSABI_C6000_ELFABI = 64, // Bare-metal TMS320C6000
   ELFOSABI_C6000_LINUX = 65,  // Linux TMS320C6000
   ELFOSABI_ARM = 97,          // ARM
+  ELFOSABI_NACL = 123,        // Native Client // @LOCALMOD
   ELFOSABI_STANDALONE = 255   // Standalone (embedded) application
 };
 
+// @LOCALMOD-BEGIN
+// ABIVERSION identification.
+enum {
+  ELFABIVERSION_NACL = 7
+};
+
+// Native-client specific flags
+enum {
+  EF_NACL_ALIGN_LIB =  0x000000,
+  EF_NACL_ALIGN_16 =   0x100000,
+  EF_NACL_ALIGN_32 =   0x200000,
+  EF_NACL_ALIGN_MASK = 0x300000
+};
+// @LOCALMOD-END
+
 // X86_64 relocations.
 enum {
   R_X86_64_NONE       = 0,
diff -r 2b13dadc8fed include/llvm/Support/system_error.h
--- a/include/llvm/Support/system_error.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/Support/system_error.h	Thu Jun 09 18:06:40 2011 -0700
@@ -606,7 +606,7 @@
 #else
   stream_timeout                      = ETIMEDOUT,
 #endif
-  text_file_busy                      = ETXTBSY,
+  text_file_busy                      = EINVAL, // @LOCALMOD
   timed_out                           = ETIMEDOUT,
   too_many_files_open_in_system       = ENFILE,
   too_many_files_open                 = EMFILE,
diff -r 2b13dadc8fed include/llvm/Target/Target.td
--- a/include/llvm/Target/Target.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/Target/Target.td	Thu Jun 09 18:06:40 2011 -0700
@@ -516,6 +516,40 @@
   let neverHasSideEffects = 1;
   let isAsCheapAsAMove = 1;
 }
+// @LOCALMOD-BEGIN
+def BUNDLE_ALIGN_START : Instruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins);
+  let AsmString = "";
+  let neverHasSideEffects = 1;
+  let isAsCheapAsAMove = 1;
+  let isNotDuplicable = 1;
+}
+def BUNDLE_ALIGN_END : Instruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins);
+  let AsmString = "";
+  let neverHasSideEffects = 1;
+  let isAsCheapAsAMove = 1;
+  let isNotDuplicable = 1;
+}
+def BUNDLE_LOCK : Instruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins);
+  let AsmString = "";
+  let neverHasSideEffects = 1;
+  let isAsCheapAsAMove = 1;
+  let isNotDuplicable = 1;
+}
+def BUNDLE_UNLOCK : Instruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins);
+  let AsmString = "";
+  let neverHasSideEffects = 1;
+  let isAsCheapAsAMove = 1;
+  let isNotDuplicable = 1;
+}
+// @LOCALMOD-END
 }
 
 //===----------------------------------------------------------------------===//
diff -r 2b13dadc8fed include/llvm/Target/TargetAsmBackend.h
--- a/include/llvm/Target/TargetAsmBackend.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/Target/TargetAsmBackend.h	Thu Jun 09 18:06:40 2011 -0700
@@ -20,6 +20,7 @@
 class MCInst;
 class MCObjectWriter;
 class MCSection;
+class MCStreamer;
 template<typename T>
 class SmallVectorImpl;
 class raw_ostream;
@@ -116,6 +117,23 @@
   /// HandleAssemblerFlag - Handle any target-specific assembler flags.
   /// By default, do nothing.
   virtual void HandleAssemblerFlag(MCAssemblerFlag Flag) {}
+  
+  // @LOCALMOD-BEGIN
+  /// getBundleSize - Return the size (in bytes) of code bundling units
+  /// for this target. If 0, bundling is disabled. This is used exclusively
+  /// for Native Client.
+  virtual unsigned getBundleSize() const {
+    return 0;
+  }
+
+  /// CustomExpandInst -
+  ///   If the MCInst instruction has a custom expansion, write it to the
+  /// MCStreamer 'Out'. This can be used to perform "last minute" rewrites of
+  /// MCInst instructions for emission.
+  virtual bool CustomExpandInst(const MCInst &Inst, MCStreamer &Out) const {
+    return false;
+  }
+  // @LOCALMOD-END
 };
 
 } // End llvm namespace
diff -r 2b13dadc8fed include/llvm/Target/TargetAsmInfo.h
--- a/include/llvm/Target/TargetAsmInfo.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/Target/TargetAsmInfo.h	Thu Jun 09 18:06:40 2011 -0700
@@ -28,6 +28,7 @@
 class TargetAsmInfo {
   unsigned PointerSize;
   bool IsLittleEndian;
+  unsigned StackSlotSize; // @LOCALMOD
   TargetFrameLowering::StackDirection StackDir;
   const TargetRegisterInfo *TRI;
   std::vector<MachineMove> InitialFrameState;
diff -r 2b13dadc8fed include/llvm/Target/TargetFrameLowering.h
--- a/include/llvm/Target/TargetFrameLowering.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/Target/TargetFrameLowering.h	Thu Jun 09 18:06:40 2011 -0700
@@ -50,11 +50,19 @@
   unsigned StackAlignment;
   unsigned TransientStackAlignment;
   int LocalAreaOffset;
+
+  // @LOCALMOD-BEGIN
+  // TODO(pdox): Refactor this and upstream it, to get rid of the
+  // assumption that StackSlotSize == PointerSize.
+  unsigned StackSlotSize;
+  // @LOCALMOD-END
 public:
-  TargetFrameLowering(StackDirection D, unsigned StackAl, int LAO,
-                      unsigned TransAl = 1)
+  TargetFrameLowering(StackDirection D,
+                      unsigned StackAl, int LAO,
+                      unsigned TransAl = 1,
+                      unsigned SlotSize = 0) // @LOCALMOD
     : StackDir(D), StackAlignment(StackAl), TransientStackAlignment(TransAl),
-      LocalAreaOffset(LAO) {}
+      LocalAreaOffset(LAO), StackSlotSize(SlotSize) {}
 
   virtual ~TargetFrameLowering();
 
@@ -65,6 +73,11 @@
   ///
   StackDirection getStackGrowthDirection() const { return StackDir; }
 
+  // @LOCALMOD-BEGIN
+  /// getStackSlotSize - Return the size of a stack slot
+  unsigned getStackSlotSize() const { return StackSlotSize; }
+  // @LOCALMOD-END
+
   /// getStackAlignment - This method returns the number of bytes to which the
   /// stack pointer must be aligned on entry to a function.  Typically, this
   /// is the largest alignment for any data object in the target.
diff -r 2b13dadc8fed include/llvm/Target/TargetOpcodes.h
--- a/include/llvm/Target/TargetOpcodes.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/include/llvm/Target/TargetOpcodes.h	Thu Jun 09 18:06:40 2011 -0700
@@ -78,7 +78,14 @@
 
     /// COPY - Target-independent register copy. This instruction can also be
     /// used to copy between subregisters of virtual registers.
-    COPY = 13
+    COPY = 13,
+
+    // @LOCALMOD-BEGIN
+    BUNDLE_ALIGN_START = 14,
+    BUNDLE_ALIGN_END = 15,
+    BUNDLE_LOCK = 16,
+    BUNDLE_UNLOCK = 17
+    // @LOCALMOD-END
   };
 } // end namespace TargetOpcode
 } // end namespace llvm
diff -r 2b13dadc8fed lib/Bitcode/Reader/BitcodeReader.cpp
--- a/lib/Bitcode/Reader/BitcodeReader.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Bitcode/Reader/BitcodeReader.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -1395,7 +1395,12 @@
       std::string S;
       if (ConvertToString(Record, 0, S))
         return Error("Invalid MODULE_CODE_DATALAYOUT record");
-      TheModule->setDataLayout(S);
+
+      // @LOCALMOD-BEGIN
+      // Figure out how to properly prevent
+      // NaCl data layout from being overriden
+      //TheModule->setDataLayout(S);
+      // @LOCALMOD-END
       break;
     }
     case bitc::MODULE_CODE_ASM: {  // ASM: [strchr x N]
diff -r 2b13dadc8fed lib/CodeGen/AsmPrinter/AsmPrinter.cpp
--- a/lib/CodeGen/AsmPrinter/AsmPrinter.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/CodeGen/AsmPrinter/AsmPrinter.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -656,9 +656,14 @@
       switch (II->getOpcode()) {
       case TargetOpcode::PROLOG_LABEL:
       case TargetOpcode::EH_LABEL:
-      case TargetOpcode::GC_LABEL:
+      case TargetOpcode::GC_LABEL: {
+        // @LOCALMOD-START
+        unsigned LabelAlign = GetTargetLabelAlign(II);
+        if (LabelAlign) EmitAlignment(LabelAlign);
+        // @LOCALMOD-END
         OutStreamer.EmitLabel(II->getOperand(0).getMCSymbol());
         break;
+      }
       case TargetOpcode::INLINEASM:
         EmitInlineAsm(II);
         break;
@@ -674,6 +679,20 @@
       case TargetOpcode::KILL:
         if (isVerbose()) EmitKill(II, *this);
         break;
+      // @LOCALMOD-BEGIN
+      case TargetOpcode::BUNDLE_ALIGN_START:
+        OutStreamer.EmitBundleAlignStart();
+        break;
+      case TargetOpcode::BUNDLE_ALIGN_END:
+        OutStreamer.EmitBundleAlignEnd();
+        break;
+      case TargetOpcode::BUNDLE_LOCK:
+        OutStreamer.EmitBundleLock();
+        break;
+      case TargetOpcode::BUNDLE_UNLOCK:
+        OutStreamer.EmitBundleUnlock();
+        break;
+      // @LOCALMOD-END
       default:
         EmitInstruction(II);
         break;
@@ -956,6 +975,11 @@
 /// by the current function to the current output stream.  
 ///
 void AsmPrinter::EmitJumpTableInfo() {
+  // @LOCALMOD-BEGIN
+  if (isVerbose()) {
+    OutStreamer.EmitRawText(Twine("# @LOCALMOD: JUMPTABLE\n"));
+  }
+  // @LOCALMOD-END
   const MachineJumpTableInfo *MJTI = MF->getJumpTableInfo();
   if (MJTI == 0) return;
   if (MJTI->getEntryKind() == MachineJumpTableInfo::EK_Inline) return;
@@ -969,12 +993,25 @@
   if (// In PIC mode, we need to emit the jump table to the same section as the
       // function body itself, otherwise the label differences won't make sense.
       // FIXME: Need a better predicate for this: what about custom entries?
-      MJTI->getEntryKind() == MachineJumpTableInfo::EK_LabelDifference32 ||
+      (MJTI->getEntryKind() == MachineJumpTableInfo::EK_LabelDifference32 ||
       // We should also do if the section name is NULL or function is declared
       // in discardable section
       // FIXME: this isn't the right predicate, should be based on the MCSection
       // for the function.
-      F->isWeakForLinker()) {
+      // @LOCALMOD-START
+      // the original code is a hack
+      // jumptables usually end up in .rodata
+      // but for functions with weak linkage there is a chance that the are
+      // not needed. So in order to be discard the function AND the jumptable
+      // they keep them both in .text. This fix only works if we never discard
+      // weak functions. This is guaranteed because the bitcode linker already
+      // throws out unused ones.
+      // TODO: Investigate the other case of concern -- PIC code.
+      // Concern is about jumptables being in a different section: can the
+      // rodata and text be too far apart for a RIP-relative offset?
+       F->isWeakForLinker())
+      && !UseReadOnlyJumpTables()) {
+      // @LOCALMOD-END
     OutStreamer.SwitchSection(getObjFileLowering().SectionForGlobal(F,Mang,TM));
   } else {
     // Otherwise, drop it in the readonly section.
@@ -996,7 +1033,7 @@
     // .set directive for each unique entry.  This reduces the number of
     // relocations the assembler will generate for the jump table.
     if (MJTI->getEntryKind() == MachineJumpTableInfo::EK_LabelDifference32 &&
-        MAI->hasSetDirective()) {
+        MAI->hasSetDirective() && !UseReadOnlyJumpTables()) { // @LOCALMOD
       SmallPtrSet<const MachineBasicBlock*, 16> EmittedSets;
       const TargetLowering *TLI = TM.getTargetLowering();
       const MCExpr *Base = TLI->getPICJumpTableRelocBaseExpr(MF,JTI,OutContext);
@@ -1068,7 +1105,7 @@
     // If we have emitted set directives for the jump table entries, print 
     // them rather than the entries themselves.  If we're emitting PIC, then
     // emit the table entries as differences between two text section labels.
-    if (MAI->hasSetDirective()) {
+    if (MAI->hasSetDirective() && !UseReadOnlyJumpTables()) { // @LOCALMOD
       // If we used .set, reference the .set's symbol.
       Value = MCSymbolRefExpr::Create(GetJTSetSymbol(UID, MBB->getNumber()),
                                       OutContext);
@@ -1865,6 +1902,18 @@
   
   // Otherwise, check the last instruction.
   const MachineInstr &LastInst = Pred->back();
+
+  // @LOCALMOD-BEGIN
+  // This function checks the last instruction in a basic block to
+  // determine whether the block falls through to the next one.
+  // However, if the last instruction is BUNDLE_UNLOCK, then we have
+  // to look at the one before it instead.
+  if (LastInst.getOpcode() == TargetOpcode::BUNDLE_UNLOCK) {
+    MachineBasicBlock::const_iterator MBBI = LastInst;
+    --MBBI;
+    return !MBBI->getDesc().isBarrier();
+  }
+  // @LOCALMOD-END
   return !LastInst.getDesc().isBarrier();
 }
 
diff -r 2b13dadc8fed lib/CodeGen/AsmPrinter/AsmPrinterDwarf.cpp
--- a/lib/CodeGen/AsmPrinter/AsmPrinterDwarf.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/CodeGen/AsmPrinter/AsmPrinterDwarf.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -213,7 +213,7 @@
                                 MCSymbol *BaseLabel, bool isEH) const {
   const TargetRegisterInfo *RI = TM.getRegisterInfo();
   
-  int stackGrowth = TM.getTargetData()->getPointerSize();
+  int stackGrowth = TM.getFrameLowering()->getStackSlotSize(); // @LOCALMOD
   if (TM.getFrameLowering()->getStackGrowthDirection() !=
       TargetFrameLowering::StackGrowsUp)
     stackGrowth *= -1;
@@ -282,7 +282,7 @@
 void AsmPrinter::EmitCFIFrameMoves(const std::vector<MachineMove> &Moves) const {
   const TargetRegisterInfo *RI = TM.getRegisterInfo();
 
-  int stackGrowth = TM.getTargetData()->getPointerSize();
+  int stackGrowth = TM.getFrameLowering()->getStackSlotSize(); // @LOCALMOD
   if (TM.getFrameLowering()->getStackGrowthDirection() !=
       TargetFrameLowering::StackGrowsUp)
     stackGrowth *= -1;
diff -r 2b13dadc8fed lib/CodeGen/AsmPrinter/DwarfDebug.cpp
--- a/lib/CodeGen/AsmPrinter/DwarfDebug.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/CodeGen/AsmPrinter/DwarfDebug.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -3371,7 +3371,8 @@
   if (!Asm->MAI->doesDwarfRequireFrameSection())
     return;
 
-  int stackGrowth = Asm->getTargetData().getPointerSize();
+  int stackGrowth =
+    Asm->TM.getFrameLowering()->getStackSlotSize(); // @LOCALMOD
   if (Asm->TM.getFrameLowering()->getStackGrowthDirection() ==
       TargetFrameLowering::StackGrowsDown)
     stackGrowth *= -1;
diff -r 2b13dadc8fed lib/CodeGen/AsmPrinter/DwarfTableException.cpp
--- a/lib/CodeGen/AsmPrinter/DwarfTableException.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/CodeGen/AsmPrinter/DwarfTableException.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -51,7 +51,8 @@
 /// in every non-empty .debug_frame section.
 void DwarfTableException::EmitCIE(const Function *PersonalityFn, unsigned Index) {
   // Size and sign of stack growth.
-  int stackGrowth = Asm->getTargetData().getPointerSize();
+  int stackGrowth =
+    Asm->TM.getFrameLowering()->getStackSlotSize(); // @LOCALMOD
   if (Asm->TM.getFrameLowering()->getStackGrowthDirection() ==
       TargetFrameLowering::StackGrowsDown)
     stackGrowth *= -1;
diff -r 2b13dadc8fed lib/CodeGen/BranchFolding.cpp
--- a/lib/CodeGen/BranchFolding.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/CodeGen/BranchFolding.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -20,6 +20,7 @@
 #include "BranchFolding.h"
 #include "llvm/Function.h"
 #include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/MachineConstantPool.h" //  @LOCALMOD
 #include "llvm/CodeGen/MachineModuleInfo.h"
 #include "llvm/CodeGen/MachineFunctionPass.h"
 #include "llvm/CodeGen/MachineJumpTableInfo.h"
@@ -215,6 +216,21 @@
       }
   }
 
+    // @LOCALMOD-START
+    // This currently only used on ARM targets where the ConstantPool
+    // subclass is overloading getJumpTableIndex()
+    const std::vector<MachineConstantPoolEntry>& CPs =
+      MF.getConstantPool()->getConstants();
+    for (unsigned i = 0, e = CPs.size(); i != e; ++i) {
+      if (!CPs[i].isMachineConstantPoolEntry()) continue;
+      unsigned *JTIndex = CPs[i].Val.MachineCPVal->getJumpTableIndex();
+      if (!JTIndex) continue;
+      // Remember that this JT is live.
+      JTIsLive.set(*JTIndex);
+    }
+    // @LOCALMOD-END
+
+
   // Finally, remove dead jump tables.  This happens when the
   // indirect jump was unreachable (and thus deleted).
   for (unsigned i = 0, e = JTIsLive.size(); i != e; ++i)
diff -r 2b13dadc8fed lib/CodeGen/SelectionDAG/DAGCombiner.cpp
--- a/lib/CodeGen/SelectionDAG/DAGCombiner.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/CodeGen/SelectionDAG/DAGCombiner.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -7045,6 +7045,11 @@
   if (ConstantFPSDNode *TV = dyn_cast<ConstantFPSDNode>(N2))
     if (ConstantFPSDNode *FV = dyn_cast<ConstantFPSDNode>(N3)) {
       if (TLI.isTypeLegal(N2.getValueType()) &&
+          // @LOCALMOD-START
+          // when we combine two 8byte constants into a 16byte one
+          // we get constant pool entries which are too big
+          TLI.getTargetData()->getTypeAllocSize(FV->getConstantFPValue()->getType()) <= 4 &&
+          // @LOCALMOD-STOP
           (TLI.getOperationAction(ISD::ConstantFP, N2.getValueType()) !=
            TargetLowering::Legal) &&
           // If both constants have multiple uses, then we won't need to do an
diff -r 2b13dadc8fed lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
--- a/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -6005,7 +6005,10 @@
   SDValue V = DAG.getVAArg(TLI.getValueType(I.getType()), getCurDebugLoc(),
                            getRoot(), getValue(I.getOperand(0)),
                            DAG.getSrcValue(I.getOperand(0)),
-                           TD.getABITypeAlignment(I.getType()));
+// @LOCALMOD-BEGIN
+                           TD.getCallFrameTypeAlignment(I.getType()));
+// @LOCALMOD-END
+
   setValue(&I, V);
   DAG.setRoot(V.getValue(1));
 }
diff -r 2b13dadc8fed lib/MC/ELFObjectWriter.cpp
--- a/lib/MC/ELFObjectWriter.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/ELFObjectWriter.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -286,7 +286,18 @@
     virtual void WriteHeader(uint64_t SectionDataSize, unsigned NumberOfSections);
 
     /// Default e_flags = 0
-    virtual void WriteEFlags() { Write32(0); }
+    virtual void WriteEFlags() { 
+      // @LOCALMOD-BEGIN
+      switch (TargetObjectWriter->getOSType()) {
+      case Triple::NativeClient:
+        Write32(ELF::EF_NACL_ALIGN_32);
+        break;
+      default:
+        Write32(0);   // e_flags = whatever the target wants
+        break;
+      }
+      // @LOCALMOD-END
+    }
 
     virtual void WriteSymbolEntry(MCDataFragment *SymtabF, MCDataFragment *ShndxF,
                           uint64_t name, uint8_t info,
@@ -458,9 +469,24 @@
   switch (TargetObjectWriter->getOSType()) {
     case Triple::FreeBSD:  Write8(ELF::ELFOSABI_FREEBSD); break;
     case Triple::Linux:    Write8(ELF::ELFOSABI_LINUX); break;
+    // @LOCALMOD-BEGIN
+    case Triple::NativeClient:
+      Write8(ELF::ELFOSABI_NACL);
+      break;
+    // @LOCALMOD-END
     default:               Write8(ELF::ELFOSABI_NONE); break;
   }
-  Write8(0);                  // e_ident[EI_ABIVERSION]
+
+  // @LOCALMOD-BEGIN
+  switch (TargetObjectWriter->getOSType()) {
+  case Triple::NativeClient:
+    Write8(ELF::ELFABIVERSION_NACL);
+    break;
+  default:
+    Write8(0);                    // e_ident[EI_ABIVERSION]
+    break;
+  }
+  // @LOCALMOD-END
 
   WriteZeros(ELF::EI_NIDENT - ELF::EI_PAD);
 
@@ -726,7 +752,12 @@
   if (&Sec2 != &Section &&
       (Kind == MCSymbolRefExpr::VK_PLT ||
        Kind == MCSymbolRefExpr::VK_GOTPCREL ||
-       Kind == MCSymbolRefExpr::VK_GOTOFF)) {
+       Kind == MCSymbolRefExpr::VK_GOTOFF ||
+  // @LOCALMOD-BEGIN
+  // Fixes an LLVM bug. This bug has already been fixed upstream
+  // and should disappear on the next merge.
+       Kind == MCSymbolRefExpr::VK_NTPOFF)) {
+  // @LOCALMOD-END
     if (Renamed)
       return Renamed;
     return &Symbol;
diff -r 2b13dadc8fed lib/MC/MCAsmStreamer.cpp
--- a/lib/MC/MCAsmStreamer.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/MCAsmStreamer.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -178,6 +178,13 @@
   virtual void EmitValueToOffset(const MCExpr *Offset,
                                  unsigned char Value = 0);
 
+  // @LOCALMOD-BEGIN
+  virtual void EmitBundleLock();
+  virtual void EmitBundleUnlock();
+  virtual void EmitBundleAlignStart();
+  virtual void EmitBundleAlignEnd();
+  // @LOCALMOD-END
+
   virtual void EmitFileDirective(StringRef Filename);
   virtual bool EmitDwarfFileDirective(unsigned FileNo, StringRef Filename);
   virtual void EmitDwarfLocDirective(unsigned FileNo, unsigned Line,
@@ -653,6 +660,27 @@
   EmitEOL();
 }
 
+// @LOCALMOD-BEGIN
+void MCAsmStreamer::EmitBundleLock() {
+  OS << "\t.bundle_lock";
+  EmitEOL();
+}
+
+void MCAsmStreamer::EmitBundleUnlock() {
+  OS << "\t.bundle_unlock";
+  EmitEOL();
+}
+
+void MCAsmStreamer::EmitBundleAlignStart() {
+  OS << "\t.bundle_align_start";
+  EmitEOL();
+}
+
+void MCAsmStreamer::EmitBundleAlignEnd() {
+  OS << "\t.bundle_align_end";
+  EmitEOL();
+}
+// @LOCALMOD-END
 
 void MCAsmStreamer::EmitFileDirective(StringRef Filename) {
   assert(MAI.hasSingleParameterDotFile());
diff -r 2b13dadc8fed lib/MC/MCAssembler.cpp
--- a/lib/MC/MCAssembler.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/MCAssembler.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -71,6 +71,26 @@
 }
 
 void MCAsmLayout::Invalidate(MCFragment *F) {
+  // @LOCALMOD-BEGIN
+  if (F->getParent()->isBundlingEnabled()) {
+    // If this fragment is part of a bundle locked group,
+    // we need to invalidate all the way to the first fragment
+    // in the group.
+    while (F && !F->isBundleGroupStart())
+      F = F->getPrevNode();
+    assert(F);
+    // With padding enabled, we need to invalidate back one
+    // fragment further in in order to force the recalculuation
+    // of the padding and offset.
+    if (F->getPrevNode()) {
+      F = F->getPrevNode();
+    } else {
+      LastValidFragment[F->getParent()] = NULL;
+      return;
+    }
+  }
+  // @LOCALMOD-END
+
   // If this fragment wasn't already up-to-date, we don't need to do anything.
   if (!isFragmentUpToDate(F))
     return;
@@ -106,6 +126,15 @@
   assert(SD->getFragment() && "Invalid getOffset() on undefined symbol!");
   return getFragmentOffset(SD->getFragment()) + SD->getOffset();
 }
+  
+// @LOCALMOD-BEGIN
+uint8_t MCAsmLayout::getFragmentPadding(const MCFragment *F) const {
+  EnsureValid(F);
+  assert(F->BundlePadding != ~UINT8_C(0) && "Padding not set!");
+  return F->BundlePadding;
+}
+// @LOCALMOD-END
+
 
 uint64_t MCAsmLayout::getSectionAddressSize(const MCSectionData *SD) const {
   // The size is the last fragment's end offset.
@@ -131,10 +160,31 @@
 }
 
 MCFragment::MCFragment(FragmentType _Kind, MCSectionData *_Parent)
-  : Kind(_Kind), Parent(_Parent), Atom(0), Offset(~UINT64_C(0))
+  : Kind(_Kind), Parent(_Parent), Atom(0), Offset(~UINT64_C(0)),
+    // @LOCALMOD-BEGIN
+    BundleAlign(BundleAlignNone),
+    BundleGroupStart(false),
+    BundleGroupEnd(false),
+    BundlePadding(~UINT8_C(0))
+    // @LOCALMOD-END
 {
   if (Parent)
     Parent->getFragmentList().push_back(this);
+
+  // @LOCALMOD-BEGIN
+  if (Parent && Parent->isBundlingEnabled()) {
+    BundleAlign = Parent->getBundleAlignNext();
+    Parent->setBundleAlignNext(MCFragment::BundleAlignNone);
+    if (Parent->isBundleLocked()) {
+      BundleGroupStart = Parent->isBundleGroupFirstFrag();
+      BundleGroupEnd = false;
+      Parent->setBundleGroupFirstFrag(false);
+    } else {
+      BundleGroupStart = true;
+      BundleGroupEnd = true;
+    }
+  }
+  // @LOCALMOD-END
 }
 
 /* *** */
@@ -145,10 +195,24 @@
   : Section(&_Section),
     Ordinal(~UINT32_C(0)),
     Alignment(1),
-    HasInstructions(false)
+    HasInstructions(false),
+// @LOCALMOD-BEGIN
+    BundlingEnabled(false),
+    BundleLocked(false),
+    BundleGroupFirstFrag(false),
+    BundleAlignNext(MCFragment::BundleAlignNone)
+// @LOCALMOD-END
 {
   if (A)
     A->getSectionList().push_back(this);
+
+  // @LOCALMOD-BEGIN
+  unsigned BundleSize = A->getBackend().getBundleSize();
+  if (BundleSize && _Section.UseCodeAlign()) {
+    BundlingEnabled = true;
+    setAlignment(BundleSize);
+  }
+  // @LOCALMOD-END
 }
 
 /* *** */
@@ -298,7 +362,10 @@
 
   case MCFragment::FT_LEB:
     return cast<MCLEBFragment>(F).getContents().size();
-
+// @LOCALMOD-BEGIN
+  case MCFragment::FT_Tiny:
+    return cast<MCTinyFragment>(F).getContents().size();
+// @LOCALMOD-END
   case MCFragment::FT_Align: {
     const MCAlignFragment &AF = cast<MCAlignFragment>(F);
     unsigned Offset = Layout.getFragmentOffset(&AF);
@@ -349,15 +416,136 @@
   uint64_t Offset = 0;
   if (Prev)
     Offset += Prev->Offset + getAssembler().ComputeFragmentSize(*this, *Prev);
-
+  // @LOCALMOD-BEGIN
+  F->BundlePadding = getAssembler().ComputeBundlePadding(*this, F, Offset);
+  Offset += F->BundlePadding;
+  // @LOCALMOD-END
   F->Offset = Offset;
   LastValidFragment[F->getParent()] = F;
 }
 
+// @LOCALMOD-BEGIN
+// Returns number of bytes of padding needed to align to bundle start.
+static uint64_t AddressToBundlePadding(uint64_t Address, uint64_t BundleMask) {
+  return (~Address + 1) & BundleMask;
+}
+
+uint64_t MCAssembler::getBundleSize() const {
+  return getBackend().getBundleSize();
+}
+
+uint64_t MCAssembler::getBundleMask() const {
+  uint64_t BundleSize = getBundleSize();
+  uint64_t BundleMask = BundleSize - 1;
+  assert(BundleSize != 0);
+  assert((BundleSize & BundleMask) == 0 &&
+         "Bundle size must be a power of 2!");
+  return BundleMask;
+}
+
+static unsigned ComputeGroupSize(MCFragment *F) {
+  if (!F->isBundleGroupStart()) {
+    return 0;
+  }
+
+  unsigned GroupSize = 0;
+  MCFragment *Cur = F;
+  while (Cur) {
+    switch (Cur->getKind()) {
+    default: llvm_unreachable("Unexpected fragment type in bundle!");
+    case MCFragment::FT_Align:
+    case MCFragment::FT_Org:
+    case MCFragment::FT_Fill:
+      if (Cur == F && Cur->isBundleGroupEnd()) {
+        return 0;
+      }
+      llvm_unreachable(".bundle_lock cannot contain .align, .org, or .fill");
+    case MCFragment::FT_Inst:
+      GroupSize += cast<MCInstFragment>(Cur)->getInstSize();
+      break;
+    case MCFragment::FT_Data:
+      GroupSize += cast<MCDataFragment>(Cur)->getContents().size();
+      break;
+    case MCFragment::FT_Tiny:
+      GroupSize += cast<MCTinyFragment>(Cur)->getContents().size();
+      break;
+    }
+    if (Cur->isBundleGroupEnd())
+      break;
+    Cur = Cur->getNextNode();
+  }
+  return GroupSize;
+}
+
+uint8_t MCAssembler::ComputeBundlePadding(const MCAsmLayout &Layout,
+                                          MCFragment *F,
+                                          uint64_t FragmentOffset) const {
+  if (!F->getParent()->isBundlingEnabled())
+    return 0;
+
+  uint64_t BundleSize = getBundleSize();
+  uint64_t BundleMask = getBundleMask();
+  unsigned GroupSize = ComputeGroupSize(F);
+  assert(GroupSize <= BundleSize &&
+         "Bundle lock contents too large!");
+
+  uint64_t Padding = 0;
+  uint64_t OffsetInBundle = FragmentOffset & BundleMask;
+
+  if (OffsetInBundle + GroupSize > BundleSize ||
+      F->getBundleAlign() == MCFragment::BundleAlignStart) {
+    // Pad up to start of the next bundle
+    Padding += AddressToBundlePadding(OffsetInBundle, BundleMask);
+    OffsetInBundle = 0;
+  }
+  if (F->getBundleAlign() == MCFragment::BundleAlignEnd) {
+    // Push to the end of the bundle
+    Padding += AddressToBundlePadding(OffsetInBundle + GroupSize, BundleMask);
+  }
+  return Padding;
+}
+// @LOCALMOD-END
+
+
+
+
+// @LOCALMOD-BEGIN
+// Write out BundlePadding bytes in NOPs, being careful not to cross a bundle boundary.
+static void WriteBundlePadding(const MCAssembler &Asm,
+                               const MCAsmLayout &Layout,
+                               uint64_t Offset, uint64_t TotalPadding,
+                               MCObjectWriter *OW) {
+  uint64_t BundleSize = Asm.getBundleSize();
+  uint64_t BundleMask = Asm.getBundleMask();
+  uint64_t PaddingLeft = TotalPadding;
+  uint64_t StartPos = Offset;
+
+  bool FirstWrite = true;
+  while (PaddingLeft > 0) {
+    uint64_t NopsToWrite =
+      FirstWrite ? AddressToBundlePadding(StartPos, BundleMask) :
+                   BundleSize;
+    if (NopsToWrite > PaddingLeft)
+      NopsToWrite = PaddingLeft;
+    if (!Asm.getBackend().WriteNopData(NopsToWrite, OW))
+      report_fatal_error("unable to write nop sequence of " +
+                         Twine(NopsToWrite) + " bytes");
+    PaddingLeft -= NopsToWrite;
+    FirstWrite = false;
+  }
+}
+// @LOCALMOD-END
+
 /// WriteFragmentData - Write the \arg F data to the output file.
 static void WriteFragmentData(const MCAssembler &Asm, const MCAsmLayout &Layout,
                               const MCFragment &F) {
   MCObjectWriter *OW = &Asm.getWriter();
+  // @LOCALMOD-BEGIN
+  uint64_t BundlePadding = Layout.getFragmentPadding(&F);
+  uint64_t PaddingOffset = Layout.getFragmentOffset(&F) - BundlePadding;
+  WriteBundlePadding(Asm, Layout, PaddingOffset, BundlePadding, OW);
+  // @LOCALMOD-END
+
   uint64_t Start = OW->getStream().tell();
   (void) Start;
 
@@ -386,6 +574,16 @@
     // bytes left to fill use the the Value and ValueSize to fill the rest.
     // If we are aligning with nops, ask that target to emit the right data.
     if (AF.hasEmitNops()) {
+      // @LOCALMOD-BEGIN
+      if (Asm.getBundleSize()) {
+        WriteBundlePadding(Asm, Layout,
+                           Layout.getFragmentOffset(&F),
+                           FragmentSize,
+                           OW);
+        break;
+      }
+      // @LOCALMOD-END
+
       if (!Asm.getBackend().WriteNopData(Count, OW))
         report_fatal_error("unable to write nop sequence of " +
                           Twine(Count) + " bytes");
@@ -413,6 +611,15 @@
     break;
   }
 
+  // @LOCALMOD-BEGIN
+  case MCFragment::FT_Tiny: {
+    MCTinyFragment &TF = cast<MCTinyFragment>(F);
+    assert(FragmentSize == TF.getContents().size() && "Invalid size!");
+    OW->WriteBytes(TF.getContents().str());
+    break;
+  }
+  // @LOCALMOD-END
+
   case MCFragment::FT_Fill: {
     MCFillFragment &FF = cast<MCFillFragment>(F);
 
@@ -820,6 +1027,9 @@
   case MCFragment::FT_Dwarf: OS << "MCDwarfFragment"; break;
   case MCFragment::FT_DwarfFrame: OS << "MCDwarfCallFrameFragment"; break;
   case MCFragment::FT_LEB:   OS << "MCLEBFragment"; break;
+  // @LOCALMOD-BEGIN
+  case MCFragment::FT_Tiny: OS << "MCTinyFragment"; break;
+  // @LOCALMOD-END
   }
 
   OS << "<MCFragment " << (void*) this << " LayoutOrder:" << LayoutOrder
@@ -872,6 +1082,20 @@
     IF->getInst().dump_pretty(OS);
     break;
   }
+  // @LOCALMOD-BEGIN
+  case MCFragment::FT_Tiny: {
+    const MCTinyFragment *TF = cast<MCTinyFragment>(this);
+    OS << "\n       ";
+    OS << " Contents:[";
+    const SmallVectorImpl<char> &Contents = TF->getContents();
+    for (unsigned i = 0, e = Contents.size(); i != e; ++i) {
+      if (i) OS << ",";
+      OS << hexdigit((Contents[i] >> 4) & 0xF) << hexdigit(Contents[i] & 0xF);
+    }
+    OS << "] (" << Contents.size() << " bytes)";
+    break;
+  }
+  // @LOCALMOD-END
   case MCFragment::FT_Org:  {
     const MCOrgFragment *OF = cast<MCOrgFragment>(this);
     OS << "\n       ";
diff -r 2b13dadc8fed lib/MC/MCELFStreamer.cpp
--- a/lib/MC/MCELFStreamer.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/MCELFStreamer.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -468,7 +468,6 @@
 }
 
 void MCELFStreamer::EmitInstToData(const MCInst &Inst) {
-  MCDataFragment *DF = getOrCreateDataFragment();
 
   SmallVector<MCFixup, 4> Fixups;
   SmallString<256> Code;
@@ -479,12 +478,21 @@
   for (unsigned i = 0, e = Fixups.size(); i != e; ++i)
     fixSymbolsInTLSFixups(Fixups[i].getValue());
 
-  // Add the fixups and data.
-  for (unsigned i = 0, e = Fixups.size(); i != e; ++i) {
-    Fixups[i].setOffset(Fixups[i].getOffset() + DF->getContents().size());
-    DF->addFixup(Fixups[i]);
+  // @LOCALMOD-BEGIN
+  if (Fixups.size() > 0) {
+    MCDataFragment *DF = getOrCreateDataFragment();
+
+    // Add the fixups and data.
+    for (unsigned i = 0, e = Fixups.size(); i != e; ++i) {
+      Fixups[i].setOffset(Fixups[i].getOffset() + DF->getContents().size());
+      DF->addFixup(Fixups[i]);
+    }
+    DF->getContents().append(Code.begin(), Code.end());
+  } else {
+    MCTinyFragment *TF = new MCTinyFragment(getCurrentSectionData());
+    TF->getContents().append(Code.begin(), Code.end());
   }
-  DF->getContents().append(Code.begin(), Code.end());
+  // @LOCALMOD-END
 }
 
 void MCELFStreamer::Finish() {
diff -r 2b13dadc8fed lib/MC/MCLoggingStreamer.cpp
--- a/lib/MC/MCLoggingStreamer.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/MCLoggingStreamer.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -202,6 +202,28 @@
     return Child->EmitValueToOffset(Offset, Value);
   }
 
+  // @LOCALMOD-BEGIN
+  virtual void EmitBundleLock() {
+    LogCall("EmitBundleLock");
+    return Child->EmitBundleLock();
+  }
+
+  virtual void EmitBundleUnlock() {
+    LogCall("EmitBundleUnlock");
+    return Child->EmitBundleUnlock();
+  }
+
+  virtual void EmitBundleAlignStart() {
+    LogCall("EmitBundleAlignStart");
+    return Child->EmitBundleAlignStart();
+  }
+
+  virtual void EmitBundleAlignEnd() {
+    LogCall("EmitBundleAlignEnd");
+    return Child->EmitBundleAlignEnd();
+  }
+  // @LOCALMOD-END
+
   virtual void EmitFileDirective(StringRef Filename) {
     LogCall("EmitFileDirective", "FileName:" + Filename);
     return Child->EmitFileDirective(Filename);
diff -r 2b13dadc8fed lib/MC/MCNullStreamer.cpp
--- a/lib/MC/MCNullStreamer.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/MCNullStreamer.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -83,6 +83,13 @@
     virtual void EmitValueToOffset(const MCExpr *Offset,
                                    unsigned char Value = 0) {}
     
+    // @LOCALMOD-BEGIN
+    virtual void EmitBundleLock() {}
+    virtual void EmitBundleUnlock() {}
+    virtual void EmitBundleAlignStart() {}
+    virtual void EmitBundleAlignEnd() {}
+    // @LOCALMOD-END
+
     virtual void EmitFileDirective(StringRef Filename) {}
     virtual bool EmitDwarfFileDirective(unsigned FileNo,StringRef Filename) {
       return false;
diff -r 2b13dadc8fed lib/MC/MCObjectStreamer.cpp
--- a/lib/MC/MCObjectStreamer.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/MCObjectStreamer.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -16,6 +16,7 @@
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCDwarf.h"
 #include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCSection.h" // @LOCALMOD
 #include "llvm/MC/MCSymbol.h"
 #include "llvm/Target/TargetAsmBackend.h"
 #include "llvm/Target/TargetAsmInfo.h"
@@ -48,6 +49,11 @@
 }
 
 MCDataFragment *MCObjectStreamer::getOrCreateDataFragment() const {
+  // @LOCALMOD-BEGIN
+  if (getCurrentSectionData()->isBundlingEnabled()) {
+    return new MCDataFragment(getCurrentSectionData());
+  }
+  // @LOCALMOD-END
   MCDataFragment *F = dyn_cast_or_null<MCDataFragment>(getCurrentFragment());
   if (!F)
     F = new MCDataFragment(getCurrentSectionData());
@@ -142,6 +148,56 @@
   report_fatal_error("This file format doesn't support weak aliases.");
 }
 
+
+// @LOCALMOD-BEGIN ========================================================
+
+void MCObjectStreamer::EmitBundleAlignStart() {
+  MCSectionData *SD = getCurrentSectionData();
+  assert(SD->isBundlingEnabled() &&
+         ".bundle_align_start called, but bundling disabled!");
+  assert(!SD->isBundleLocked() &&
+         ".bundle_align_start while bundle locked");
+  SD->setBundleAlignNext(MCFragment::BundleAlignStart);
+}
+
+void MCObjectStreamer::EmitBundleAlignEnd() {
+  MCSectionData *SD = getCurrentSectionData();
+  assert(SD->isBundlingEnabled() &&
+         ".bundle_align_end called, but bundling disabled!");
+  assert(!SD->isBundleLocked() &&
+         ".bundle_align_end while bundle locked");
+  SD->setBundleAlignNext(MCFragment::BundleAlignEnd);
+}
+
+void MCObjectStreamer::EmitBundleLock() {
+  MCSectionData *SD = getCurrentSectionData();
+  assert(SD->isBundlingEnabled() &&
+         ".bundle_lock called, but bundling disabled!");
+  assert(!SD->isBundleLocked() &&
+         ".bundle_lock issued when bundle already locked");
+  SD->setBundleLocked(true);
+  SD->setBundleGroupFirstFrag(true);
+}
+
+void MCObjectStreamer::EmitBundleUnlock() {
+  MCSectionData *SD = getCurrentSectionData();
+  assert(SD->isBundlingEnabled() &&
+         ".bundle_unlock called, but bundling disabled!");
+  assert(SD->isBundleLocked() &&
+         ".bundle_unlock called when bundle not locked");
+
+  // If there has been at least one fragment emitted inside
+  // this bundle lock, then we need to mark the last emitted
+  // fragment as the group end.
+  if (!SD->isBundleGroupFirstFrag()) {
+    assert(getCurrentFragment() != NULL);
+    getCurrentFragment()->setBundleGroupEnd(true);
+  }
+  SD->setBundleLocked(false);
+  SD->setBundleGroupFirstFrag(false);
+}
+// @LOCALMOD-END ==========================================================
+
 void MCObjectStreamer::ChangeSection(const MCSection *Section) {
   assert(Section && "Cannot switch to a null section!");
 
@@ -149,6 +205,13 @@
 }
 
 void MCObjectStreamer::EmitInstruction(const MCInst &Inst) {
+
+  // @LOCALMOD-BEGIN
+  if (getAssembler().getBackend().CustomExpandInst(Inst, *this)) {
+    return;
+  }
+  // @LOCALMOD-END
+
   // Scan for values.
   for (unsigned i = Inst.getNumOperands(); i--; )
     if (Inst.getOperand(i).isExpr())
diff -r 2b13dadc8fed lib/MC/MCParser/AsmParser.cpp
--- a/lib/MC/MCParser/AsmParser.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/MC/MCParser/AsmParser.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -193,6 +193,13 @@
   // ".align{,32}", ".p2align{,w,l}"
   bool ParseDirectiveAlign(bool IsPow2, unsigned ValueSize);
 
+  // @LOCALMOD-BEGIN
+  bool ParseDirectiveBundleLock();
+  bool ParseDirectiveBundleUnlock();
+  bool ParseDirectiveBundleAlignStart();
+  bool ParseDirectiveBundleAlignEnd();
+  // @LOCALMOD-END
+
   /// ParseDirectiveSymbolAttribute - Parse a directive like ".globl" which
   /// accepts a single symbol (which should be a label or an external).
   bool ParseDirectiveSymbolAttribute(MCSymbolAttr Attr);
@@ -411,6 +418,13 @@
   if (!NoInitialTextSection)
     Out.InitSections();
 
+  // @LOCALMOD-BEGIN
+  // This is needed to make crtn compile, but do we really need this?
+  // TODO(pdox): Figure out if there's a better way or place to define this.
+  MCSymbol *Sym = getContext().GetOrCreateSymbol(StringRef("NACLENTRYALIGN"));
+  Out.EmitAssignment(Sym, MCConstantExpr::Create(5, getContext()));
+  // @LOCALMOD-END
+
   // Prime the lexer.
   Lex();
 
@@ -507,8 +521,31 @@
   return false;
 }
 
+
+
+
+// @LOCALMOD-NOTE
+// Code for parsing left/right bracket expressions needs to be upstreamed
+// pending verification.
+
+/// ParseBracExpr - Parse a brac expression and return it.
+/// NOTE: This assumes the leading '[' has already been consumed.
+///
+/// bracexpr ::= expr]
+///
+bool AsmParser::ParseBracExpr(const MCExpr *&Res, SMLoc &EndLoc) {
+  if (ParseExpression(Res)) return true;
+  if (Lexer.isNot(AsmToken::RBrac))
+    return TokError("expected ']' in bracket expression");
+  EndLoc = Lexer.getLoc();
+  Lex();
+  return false;
+}
+// @LOCALMOD-END
+
 /// ParsePrimaryExpr - Parse a primary expression and return it.
 ///  primaryexpr ::= (parenexpr
+///  primaryexpr ::= [bracketexpr
 ///  primaryexpr ::= symbol
 ///  primaryexpr ::= number
 ///  primaryexpr ::= '.'
@@ -1034,6 +1071,17 @@
     if (IDVal == ".p2alignl")
       return ParseDirectiveAlign(/*IsPow2=*/true, /*ExprSize=*/4);
 
+    // @LOCALMOD-BEGIN
+    if (IDVal == ".bundle_lock")
+      return ParseDirectiveBundleLock();
+    if (IDVal == ".bundle_unlock")
+      return ParseDirectiveBundleUnlock();
+    if (IDVal == ".bundle_align_start")
+      return ParseDirectiveBundleAlignStart();
+    if (IDVal == ".bundle_align_end")
+      return ParseDirectiveBundleAlignEnd();
+    // @LOCALMOD-END
+
     if (IDVal == ".org")
       return ParseDirectiveOrg();
 
@@ -1784,6 +1832,50 @@
   return false;
 }
 
+// @LOCALMOD-BEGIN
+bool AsmParser::ParseDirectiveBundleLock() {
+  CheckForValidSection();
+
+  if (getLexer().isNot(AsmToken::EndOfStatement))
+    return TokError("unexpected token in '.bundle_lock' directive");
+  Lex();
+  getStreamer().EmitBundleLock();
+  return false;
+}
+
+bool AsmParser::ParseDirectiveBundleUnlock() {
+  CheckForValidSection();
+
+  if (getLexer().isNot(AsmToken::EndOfStatement))
+    return TokError("unexpected token in '.bundle_unlock' directive");
+  Lex();
+  getStreamer().EmitBundleUnlock();
+  return false;
+}
+
+bool AsmParser::ParseDirectiveBundleAlignStart() {
+  CheckForValidSection();
+
+  if (getLexer().isNot(AsmToken::EndOfStatement))
+    return TokError("unexpected token in '.bundle_align_start' directive");
+  Lex();
+  getStreamer().EmitBundleAlignStart();
+  return false;
+}
+
+bool AsmParser::ParseDirectiveBundleAlignEnd() {
+  CheckForValidSection();
+
+  if (getLexer().isNot(AsmToken::EndOfStatement))
+    return TokError("unexpected token in '.bundle_align_end' directive");
+  Lex();
+  getStreamer().EmitBundleAlignEnd();
+  return false;
+}
+
+// @LOCALMOD-END
+
+
 /// ParseDirectiveSymbolAttribute
 ///  ::= { ".globl", ".weak", ... } [ identifier ( , identifier )* ]
 bool AsmParser::ParseDirectiveSymbolAttribute(MCSymbolAttr Attr) {
diff -r 2b13dadc8fed lib/Makefile
--- a/lib/Makefile	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Makefile	Thu Jun 09 18:06:40 2011 -0700
@@ -13,5 +13,10 @@
 PARALLEL_DIRS := VMCore AsmParser Bitcode Archive Analysis Transforms CodeGen \
                 Target ExecutionEngine Linker MC CompilerDriver Object
 
+ifeq ($(NACL_SANDBOX),1)
+  PARALLEL_DIRS := $(filter-out Archive ExecutionEngine Linker CompilerDriver, \
+                $(PARALLEL_DIRS))
+endif
+
 include $(LEVEL)/Makefile.common
 
diff -r 2b13dadc8fed lib/Object/COFFObjectFile.cpp
--- a/lib/Object/COFFObjectFile.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Object/COFFObjectFile.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -255,7 +255,7 @@
 
   // Check for string table entry. First byte is '/'.
   if (name[0] == '/') {
-    uint32_t Offset;
+    unsigned long long Offset; /* @LOCALMOD */
     name.getAsInteger(10, Offset);
     return StringRef(getString(Offset));
   }
diff -r 2b13dadc8fed lib/Support/CrashRecoveryContext.cpp
--- a/lib/Support/CrashRecoveryContext.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/CrashRecoveryContext.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -151,6 +151,7 @@
 
   gCrashRecoveryEnabled = true;
 
+#if !defined(__native_client__)
   // Setup the signal handler.
   struct sigaction Handler;
   Handler.sa_handler = CrashRecoverySignalHandler;
@@ -160,6 +161,9 @@
   for (unsigned i = 0; i != NumSignals; ++i) {
     sigaction(Signals[i], &Handler, &PrevActions[i]);
   }
+#else
+#warning Cannot setup the signal handler on this machine
+#endif
 }
 
 void CrashRecoveryContext::Disable() {
@@ -170,9 +174,11 @@
 
   gCrashRecoveryEnabled = false;
 
+#if !defined(__native_client__)
   // Restore the previous signal handlers.
   for (unsigned i = 0; i != NumSignals; ++i)
     sigaction(Signals[i], &PrevActions[i], 0);
+#endif
 }
 
 #endif
diff -r 2b13dadc8fed lib/Support/DynamicLibrary.cpp
--- a/lib/Support/DynamicLibrary.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/DynamicLibrary.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -22,6 +22,8 @@
 #include <map>
 #include <vector>
 
+#if !defined(__native_client__)
+
 // Collection of symbol name/value pairs to be searched prior to any libraries.
 static std::map<std::string, void*> *ExplicitSymbols = 0;
 
@@ -168,3 +170,6 @@
 }
 
 #endif // LLVM_ON_WIN32
+
+#endif // (__native_client__)
+
diff -r 2b13dadc8fed lib/Support/MemoryBuffer.cpp
--- a/lib/Support/MemoryBuffer.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/MemoryBuffer.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -234,6 +234,9 @@
       return success;
     }
   }
+#else
+  assert(FileSize != -1 && "invalid file size!");
+#endif
 
   MemoryBuffer *Buf = MemoryBuffer::getNewUninitMemBuffer(FileSize, Filename);
   if (!Buf) {
diff -r 2b13dadc8fed lib/Support/Mutex.cpp
--- a/lib/Support/Mutex.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Mutex.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -75,7 +75,7 @@
     errorcode = pthread_mutexattr_settype(&attr, kind);
     assert(errorcode == 0);
 
-#if !defined(__FreeBSD__) && !defined(__OpenBSD__) && !defined(__NetBSD__) && !defined(__DragonFly__)
+#if !defined(__FreeBSD__) && !defined(__OpenBSD__) && !defined(__NetBSD__) && !defined(__DragonFly__) && !defined(__native_client__)
     // Make it a process local mutex
     errorcode = pthread_mutexattr_setpshared(&attr, PTHREAD_PROCESS_PRIVATE);
     assert(errorcode == 0);
diff -r 2b13dadc8fed lib/Support/Unix/Host.inc
--- a/lib/Support/Unix/Host.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/Host.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -19,19 +19,25 @@
 #include "llvm/Config/config.h"
 #include "llvm/ADT/StringRef.h"
 #include "Unix.h"
+#if !defined(__native_client__)
 #include <sys/utsname.h>
+#endif // (__native_client__)
 #include <cctype>
 #include <string>
 
 using namespace llvm;
 
 static std::string getOSVersion() {
+#if !defined(__native_client__)
   struct utsname info;
 
   if (uname(&info))
     return "";
 
   return info.release;
+#else // (__native_client__)
+  return "";
+#endif // (__native_client__)
 }
 
 std::string sys::getHostTriple() {
diff -r 2b13dadc8fed lib/Support/Unix/Memory.inc
--- a/lib/Support/Unix/Memory.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/Memory.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -46,6 +46,10 @@
   fd = zero_fd;
 #endif
 
+#if defined(__native_client__)
+#define MAP_ANON MAP_ANONYMOUS
+#endif // (__native_client__)
+
   int flags = MAP_PRIVATE |
 #ifdef HAVE_MMAP_ANONYMOUS
   MAP_ANONYMOUS
diff -r 2b13dadc8fed lib/Support/Unix/Path.inc
--- a/lib/Support/Unix/Path.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/Path.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -128,7 +128,9 @@
 
 Path
 Path::GetTemporaryDirectory(std::string *ErrMsg) {
-#if defined(HAVE_MKDTEMP)
+#if defined(__native_client__)
+  return Path("");
+#elif defined(HAVE_MKDTEMP)
   // The best way is with mkdtemp but that's not available on many systems,
   // Linux and FreeBSD have it. Others probably won't.
   char pathname[] = "/tmp/llvm_XXXXXX";
@@ -251,6 +253,7 @@
 
 Path
 Path::GetCurrentDirectory() {
+#if !defined(__native_client__)
   char pathname[MAXPATHLEN];
   if (!getcwd(pathname,MAXPATHLEN)) {
     assert (false && "Could not query current working directory.");
@@ -258,6 +261,9 @@
   }
 
   return Path(pathname);
+#else // (__native_client__)
+  return Path("./");
+#endif // (__native_client__)
 }
 
 #if defined(__FreeBSD__) || defined (__NetBSD__) || \
@@ -318,7 +324,9 @@
 /// GetMainExecutable - Return the path to the main executable, given the
 /// value of argv[0] from program startup.
 Path Path::GetMainExecutable(const char *argv0, void *MainAddr) {
-#if defined(__APPLE__)
+#if defined(__native_client__)
+   return Path(std::string("./") + std::string(argv0));
+#elif defined(__APPLE__)
   // On OS X the executable path is saved to the stack by dyld. Reading it
   // from there is much faster than calling dladdr, especially for large
   // binaries with symbols.
@@ -411,7 +419,11 @@
 
 bool
 Path::exists() const {
+#if !defined(__native_client__)
   return 0 == access(path.c_str(), F_OK );
+#else // (__native_client__)
+  return true;
+#endif // (__native_client__)
 }
 
 bool
@@ -424,21 +436,33 @@
 
 bool
 Path::isSymLink() const {
+#if defined(__native_client__)
+  return false;
+#else
   struct stat buf;
   if (0 != lstat(path.c_str(), &buf))
     return false;
   return S_ISLNK(buf.st_mode);
+#endif
 }
 
 
 bool
 Path::canRead() const {
+#if !defined(__native_client__)
   return 0 == access(path.c_str(), R_OK);
+#else // (__native_client__)
+  return true;
+#endif // (__native_client__)
 }
 
 bool
 Path::canWrite() const {
+#if !defined(__native_client__)
   return 0 == access(path.c_str(), W_OK);
+#else // (__native_client__)
+  return true;
+#endif // (__native_client__)
 }
 
 bool
@@ -457,6 +481,7 @@
 
 bool
 Path::canExecute() const {
+#if !defined(__native_client__)
   if (0 != access(path.c_str(), R_OK | X_OK ))
     return false;
   struct stat buf;
@@ -464,6 +489,7 @@
     return false;
   if (!S_ISREG(buf.st_mode))
     return false;
+#endif // (__native_client__)
   return true;
 }
 
@@ -511,6 +537,7 @@
 }
 
 static bool AddPermissionBits(const Path &File, int bits) {
+#if !defined(__native_client__)
   // Get the umask value from the operating system.  We want to use it
   // when changing the file's permissions. Since calling umask() sets
   // the umask and returns its old value, we must call it a second
@@ -526,6 +553,7 @@
   // that the umask would not disable.
   if ((chmod(File.c_str(), (buf.st_mode | (bits & ~mask)))) == -1)
       return false;
+#endif // (__native_client__)
   return true;
 }
 
@@ -549,6 +577,7 @@
 
 bool
 Path::getDirectoryContents(std::set<Path>& result, std::string* ErrMsg) const {
+#if !defined(__native_client__)
   DIR* direntries = ::opendir(path.c_str());
   if (direntries == 0)
     return MakeErrMsg(ErrMsg, path + ": can't open directory");
@@ -574,6 +603,7 @@
   }
 
   closedir(direntries);
+#endif
   return false;
 }
 
@@ -626,7 +656,7 @@
 }
 
 static bool createDirectoryHelper(char* beg, char* end, bool create_parents) {
-
+#if !defined(__native_client__)
   if (access(beg, R_OK | W_OK) == 0)
     return false;
 
@@ -651,6 +681,9 @@
   }
 
   return mkdir(beg, S_IRWXU | S_IRWXG) != 0;
+#else // (__native_client__)
+  return false;
+#endif // (__native_client__)
 }
 
 bool
@@ -674,11 +707,13 @@
 
 bool
 Path::createFileOnDisk(std::string* ErrMsg) {
+#if !defined(__native_client__)
   // Create the file
   int fd = ::creat(path.c_str(), S_IRUSR | S_IWUSR);
   if (fd < 0)
     return MakeErrMsg(ErrMsg, path + ": can't create file");
   ::close(fd);
+#endif // (__native_client__)
   return false;
 }
 
@@ -698,6 +733,7 @@
 
 bool
 Path::eraseFromDisk(bool remove_contents, std::string *ErrStr) const {
+#if !defined(__native_client__)
   // Get the status so we can determine if it's a file or directory.
   struct stat buf;
   if (0 != stat(path.c_str(), &buf)) {
@@ -742,6 +778,11 @@
   if (rmdir(pathname.c_str()) != 0)
     return MakeErrMsg(ErrStr, pathname + ": can't erase directory");
   return false;
+#else // (__native_client__)
+  MakeErrMsg(ErrStr, ": PNACL does not know how to erase directories!");
+  return false;
+#endif // (__native_client__)
+
 }
 
 bool
@@ -754,6 +795,7 @@
 
 bool
 Path::setStatusInfoOnDisk(const FileStatus &si, std::string *ErrStr) const {
+#if !defined(__native_client__)
   struct utimbuf utb;
   utb.actime = si.modTime.toPosixTime();
   utb.modtime = utb.actime;
@@ -761,6 +803,7 @@
     return MakeErrMsg(ErrStr, path + ": can't set file modification time");
   if (0 != ::chmod(path.c_str(),si.mode))
     return MakeErrMsg(ErrStr, path + ": can't set mode");
+#endif // (__native_client__)
   return false;
 }
 
diff -r 2b13dadc8fed lib/Support/Unix/PathV2.inc
--- a/lib/Support/Unix/PathV2.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/PathV2.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -246,6 +246,9 @@
 }
 
 error_code resize_file(const Twine &path, uint64_t size) {
+#ifdef __native_client__
+  llvm_unreachable("resize_file() not implemented for Native Client");
+#else
   SmallString<128> path_storage;
   StringRef p = path.toNullTerminatedStringRef(path_storage);
 
@@ -253,6 +256,7 @@
     return error_code(errno, system_category());
 
   return success;
+#endif
 }
 
 error_code exists(const Twine &path, bool &result) {
@@ -343,6 +347,9 @@
 
 error_code unique_file(const Twine &model, int &result_fd,
                              SmallVectorImpl<char> &result_path) {
+#ifdef __native_client__
+  llvm_unreachable("unique_file() not implemented for Native Client");
+#else
   SmallString<128> Model;
   model.toVector(Model);
   // Null terminate.
@@ -431,6 +438,7 @@
 
   result_fd = RandomFD;
   return success;
+#endif
 }
 
 error_code directory_iterator_construct(directory_iterator &it, StringRef path){
diff -r 2b13dadc8fed lib/Support/Unix/Process.inc
--- a/lib/Support/Unix/Process.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/Process.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -33,6 +33,8 @@
 #  include <termios.h>
 #endif
 
+#include <sys/unistd.h>
+
 //===----------------------------------------------------------------------===//
 //=== WARNING: Implementation here must contain only generic UNIX code that
 //===          is guaranteed to work on *all* UNIX variants.
@@ -51,9 +53,10 @@
   const int page_size = 0x1000;
 #elif defined(HAVE_GETPAGESIZE)
   const int page_size = ::getpagesize();
-#elif defined(HAVE_SYSCONF)
+#elif defined(HAVE_SYSCONF)  && !defined(__native_client__)
   long page_size = ::sysconf(_SC_PAGE_SIZE);
 #else
+  const int page_size = 0;
 #warning Cannot get the page size on this machine
 #endif
   return static_cast<unsigned>(page_size);
@@ -108,7 +111,7 @@
                       TimeValue& sys_time)
 {
   elapsed = TimeValue::now();
-#if defined(HAVE_GETRUSAGE)
+#if defined(HAVE_GETRUSAGE) && !defined(__native_client__)
   struct rusage usage;
   ::getrusage(RUSAGE_SELF, &usage);
   user_time = TimeValue(
@@ -129,11 +132,23 @@
 }
 
 int Process::GetCurrentUserId() {
+#if !defined(__native_client__)
   return getuid();
+#else // (__native_client__)
+// TODO(abetul): What the proper return value should be for this function?
+// What about having a reserved user_id or the user "nobody" for PNACL?
+  return -1;
+#endif // (__native_client__)
 }
 
 int Process::GetCurrentGroupId() {
+#if !defined(__native_client__)
   return getgid();
+#else // (__native_client__)
+// TODO(abetul): What the proper return value should be for this function?
+// What about having a reserved/unused group_id?  
+  return -1;
+#endif // (__native_client__)
 }
 
 #ifdef HAVE_MACH_MACH_H
diff -r 2b13dadc8fed lib/Support/Unix/Program.inc
--- a/lib/Support/Unix/Program.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/Program.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -103,6 +103,10 @@
 }
 
 static bool RedirectIO(const Path *Path, int FD, std::string* ErrMsg) {
+#if defined(__native_client__)
+  MakeErrMsg(ErrMsg, "Cannot redirect I/O in NaCl");
+  return true;
+#else // (__native_client__)
   if (Path == 0) // Noop
     return false;
   const char *File;
@@ -128,6 +132,7 @@
   }
   close(InFD);      // Close the original FD
   return false;
+#endif // (__native_client__)
 }
 
 #ifdef HAVE_POSIX_SPAWN
@@ -229,6 +234,7 @@
   }
 #endif
 
+#if !defined(__native_client__)
   // Create a child process.
   int child = fork();
   switch (child) {
@@ -289,6 +295,10 @@
   Data_ = reinterpret_cast<void*>(child);
 
   return true;
+#else // (__native_client__)
+  MakeErrMsg(ErrMsg, "PNACL does not know how to execute child processes!");
+  return false;
+#endif // (__native_client__)
 }
 
 int
@@ -296,6 +306,7 @@
               unsigned secondsToWait,
               std::string* ErrMsg)
 {
+#if !defined(__native_client__)
 #ifdef HAVE_SYS_WAIT_H
   struct sigaction Act, Old;
 
@@ -386,10 +397,16 @@
     *ErrMsg = "Program::Wait is not implemented on this platform yet!";
   return -1;
 #endif
+#else // (__native_client__)
+// TODO(abetul): What should the proper return value be here?
+  MakeErrMsg(ErrMsg, "PNACL does not know how to wait for a child process!");
+  return -1;
+#endif // (__native_client__)
 }
 
 bool
 Program::Kill(std::string* ErrMsg) {
+#if !defined(__native_client__)
   if (Data_ == 0) {
     MakeErrMsg(ErrMsg, "Process not started!");
     return true;
@@ -404,6 +421,12 @@
   }
 
   return false;
+
+#else // (__native_client__)
+  MakeErrMsg(ErrMsg, "PNACL does not know how to kill processes!");
+  return true;
+#endif // (__native_client__)
+
 }
 
 bool Program::ChangeStdinToBinary(){
diff -r 2b13dadc8fed lib/Support/Unix/Signals.inc
--- a/lib/Support/Unix/Signals.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/Signals.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -77,6 +77,7 @@
 
 
 static void RegisterHandler(int Signal) {
+#if !defined(__native_client__)
   assert(NumRegisteredSignals <
          sizeof(RegisteredSignalInfo)/sizeof(RegisteredSignalInfo[0]) &&
          "Out of space for signal handlers!");
@@ -92,6 +93,7 @@
             &RegisteredSignalInfo[NumRegisteredSignals].SA);
   RegisteredSignalInfo[NumRegisteredSignals].SigNo = Signal;
   ++NumRegisteredSignals;
+#endif // (__native_client__)
 }
 
 static void RegisterHandlers() {
@@ -103,11 +105,13 @@
 }
 
 static void UnregisterHandlers() {
+#if !defined(__native_client__)
   // Restore all of the signal handlers to how they were before we showed up.
   for (unsigned i = 0, e = NumRegisteredSignals; i != e; ++i)
     sigaction(RegisteredSignalInfo[i].SigNo,
               &RegisteredSignalInfo[i].SA, 0);
   NumRegisteredSignals = 0;
+#endif // (__native_client__)
 }
 
 
@@ -128,10 +132,12 @@
   // instead of recursing in the signal handler.
   UnregisterHandlers();
 
+#if !defined(__native_client__)
   // Unmask all potentially blocked kill signals.
   sigset_t SigMask;
   sigfillset(&SigMask);
   sigprocmask(SIG_UNBLOCK, &SigMask, 0);
+#endif
 
   SignalsMutex.acquire();
   RemoveFilesToRemove();
diff -r 2b13dadc8fed lib/Support/Unix/TimeValue.inc
--- a/lib/Support/Unix/TimeValue.inc	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Support/Unix/TimeValue.inc	Thu Jun 09 18:06:40 2011 -0700
@@ -37,8 +37,13 @@
 }
 
 TimeValue TimeValue::now() {
+#if defined(__native_client__)
+  return MinTime;
+#else // (__native_client__)
   struct timeval the_time;
+
   timerclear(&the_time);
+
   if (0 != ::gettimeofday(&the_time,0)) {
     // This is *really* unlikely to occur because the only gettimeofday
     // errors concern the timezone parameter which we're passing in as 0.
@@ -51,6 +56,7 @@
     static_cast<TimeValue::SecondsType>( the_time.tv_sec + PosixZeroTime.seconds_ ),
     static_cast<TimeValue::NanoSecondsType>( the_time.tv_usec *
       NANOSECONDS_PER_MICROSECOND ) );
+#endif // (__native_client__)
 }
 
 }
diff -r 2b13dadc8fed lib/Target/ARM/ARM.h
--- a/lib/Target/ARM/ARM.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARM.h	Thu Jun 09 18:06:40 2011 -0700
@@ -20,6 +20,9 @@
 #include "llvm/Target/TargetMachine.h"
 #include <cassert>
 
+// @LOCALMOD (for LowerARMMachineInstrToMCInstPCRel)
+#include "llvm/MC/MCSymbol.h"
+
 namespace llvm {
 
 class ARMBaseTargetMachine;
@@ -53,11 +56,29 @@
 FunctionPass *createThumb2ITBlockPass();
 FunctionPass *createThumb2SizeReductionPass();
 
+/* @LOCALMOD-START */
+FunctionPass *createARMNaClRewritePass();
+/* @LOCALMOD-END */
+
 extern Target TheARMTarget, TheThumbTarget;
 
 void LowerARMMachineInstrToMCInst(const MachineInstr *MI, MCInst &OutMI,
                                   ARMAsmPrinter &AP);
 
+/* @LOCALMOD-START */
+// Used to lower the pc-relative MOVi16PIC / MOVTi16PIC pseudo instructions
+// into the real MOVi16 / MOVTi16 instructions.
+// See comment on MOVi16PIC for more details.
+void LowerARMMachineInstrToMCInstPCRel(const MachineInstr *MI,
+                                       MCInst &OutMI,
+                                       ARMAsmPrinter &AP,
+                                       unsigned ImmIndex,
+                                       unsigned PCIndex,
+                                       MCSymbol *PCLabel,
+                                       unsigned PCAdjustment);
+/* @LOCALMOD-END */
+
+
 } // end namespace llvm;
 
 #endif
diff -r 2b13dadc8fed lib/Target/ARM/ARMAsmBackend.cpp
--- a/lib/Target/ARM/ARMAsmBackend.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMAsmBackend.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -390,6 +390,12 @@
   void ApplyFixup(const MCFixup &Fixup, char *Data, unsigned DataSize,
                   uint64_t Value) const;
 
+  // @LOCALMOD-BEGIN
+  unsigned getBundleSize() const {
+    return (OSType == Triple::NativeClient) ? 16 : 0;
+  }
+ // @LOCALMOD-END
+
   MCObjectWriter *createObjectWriter(raw_ostream &OS) const {
     return createELFObjectWriter(new ARMELFObjectWriter(OSType), OS,
                               /*IsLittleEndian*/ true);
diff -r 2b13dadc8fed lib/Target/ARM/ARMAsmPrinter.cpp
--- a/lib/Target/ARM/ARMAsmPrinter.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMAsmPrinter.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -56,6 +56,13 @@
 #include <cctype>
 using namespace llvm;
 
+// @LOCALMOD-START
+namespace llvm {
+  extern cl::opt<bool> FlagSfiBranch;
+  extern cl::opt<bool> FlagSfiData;
+}
+// @LOCALMOD-END
+
 namespace {
 
   // Per section and per symbol attributes are not supported.
@@ -167,12 +174,79 @@
   return Location;
 }
 
+// @LOCALMOD-START
+// Make sure all jump targets are aligned and also all constant pools
+void NaclAlignAllJumpTargetsAndConstantPools(MachineFunction &MF) {
+  // JUMP TABLE TARGETS
+  MachineJumpTableInfo *jt_info = MF.getJumpTableInfo();
+  if (jt_info) {
+    const std::vector<MachineJumpTableEntry> &JT = jt_info->getJumpTables();
+    for (unsigned i=0; i < JT.size(); ++i) {
+      std::vector<MachineBasicBlock*> MBBs = JT[i].MBBs;
+
+      for (unsigned j=0; j < MBBs.size(); ++j) {
+        if (MBBs[j]->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY) {
+          continue;
+        }
+        MBBs[j]->setAlignment(16);
+      }
+    }
+  }
+
+  // FIRST ENTRY IN A ConstanPool
+  bool last_bb_was_constant_pool = false;
+  for (MachineFunction::iterator I = MF.begin(), E = MF.end();
+       I != E; ++I) {
+    if (I->isLandingPad()) {
+        I->setAlignment(16);
+    }
+
+    if (I->empty()) continue;
+
+    bool is_constant_pool = I->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY;
+
+    if (last_bb_was_constant_pool != is_constant_pool) {
+      I->setAlignment(16);
+    }
+
+    last_bb_was_constant_pool = is_constant_pool;
+  }
+}
+
+unsigned ARMAsmPrinter::GetTargetLabelAlign(const MachineInstr *MI) const {
+  if (true /* Subtarget->isTargetNaCl() */) {
+    switch (MI->getOpcode()) {
+      default: return 0;
+      // These labels may indicate an indirect entry point that is
+      // externally reachable and hence must be bundle aligned.
+      // Note: these labels appear to be always at basic block beginnings
+      // so it may be possible to simply set the MBB alignment.
+      // However, it is unclear whether this always holds.
+      case TargetOpcode::EH_LABEL:
+      case TargetOpcode::GC_LABEL:
+        return 4;
+    }
+  }
+  return 0;
+}
+// @LOCALMOD-END
+
+
 void ARMAsmPrinter::EmitFunctionEntryLabel() {
   if (AFI->isThumbFunction()) {
     OutStreamer.EmitAssemblerFlag(MCAF_Code16);
     OutStreamer.EmitThumbFunc(Subtarget->isTargetDarwin()? CurrentFnSym : 0);
   }
 
+  // @LOCALMOD-START
+  // make sure function entry is aligned. We use  XmagicX as our basis
+  // for alignment decisions (c.f. assembler sfi macros)
+  int alignment = MF->getAlignment();
+  if (alignment < 4) alignment = 4;
+  EmitAlignment(alignment);
+  OutStreamer.EmitRawText(StringRef("\t.set XmagicX, .\n"));
+  // @LOCALMOD-END
+ 
   OutStreamer.EmitLabel(CurrentFnSym);
 }
 
@@ -183,6 +257,11 @@
   AFI = MF.getInfo<ARMFunctionInfo>();
   MCP = MF.getConstantPool();
 
+  // @LOCALMOD-START
+  if (FlagSfiBranch) {
+    NaclAlignAllJumpTargetsAndConstantPools(MF);
+  }
+  // @LOCALMOD-END
   return AsmPrinter::runOnMachineFunction(MF);
 }
 
@@ -219,10 +298,10 @@
   case MachineOperand::MO_GlobalAddress: {
     const GlobalValue *GV = MO.getGlobal();
     if ((Modifier && strcmp(Modifier, "lo16") == 0) ||
-        (TF & ARMII::MO_LO16))
+        (TF == ARMII::MO_LO16)) // @LOCALMOD: TEMPORARY FIX
       O << ":lower16:";
     else if ((Modifier && strcmp(Modifier, "hi16") == 0) ||
-             (TF & ARMII::MO_HI16))
+             (TF == ARMII::MO_HI16)) // @LOCALMOD: TEMPORARY FIX
       O << ":upper16:";
     O << *Mang->getSymbol(GV);
 
@@ -325,6 +404,8 @@
   return false;
 }
 
+void EmitSFIHeaders(raw_ostream &O);
+
 void ARMAsmPrinter::EmitStartOfAsmFile(Module &M) {
   if (Subtarget->isTargetDarwin()) {
     Reloc::Model RelocM = TM.getRelocationModel();
@@ -371,9 +452,17 @@
 
     emitAttributes();
   }
+
+  // @LOCALMOD-BEGIN
+  if (/*IsNaCl?*/ true) {
+    std::string str;
+    raw_string_ostream OS(str);
+    EmitSFIHeaders(OS);
+    OutStreamer.EmitRawText(StringRef(OS.str()));
+  }
+  // @LOCALMOD-END
 }
 
-
 void ARMAsmPrinter::EmitEndOfAsmFile(Module &M) {
   if (Subtarget->isTargetDarwin()) {
     // All darwin targets use mach-o.
@@ -641,7 +730,20 @@
       PCRelExpr = MCBinaryExpr::CreateSub(PCRelExpr, DotExpr, OutContext);
     }
     Expr = MCBinaryExpr::CreateSub(Expr, PCRelExpr, OutContext);
+  } else {   // @LOCALMOD-BEGIN
+    // Check mustAddCurrentAddress() when getPCAdjustment() == 0,
+    // and make it actually *Subtract* the current address.
+    // A more appropriate name is probably "relativeToCurrentAddress",
+    // since the assembler can't actually handle "X + .", only "X - .".
+    if (ACPV->mustAddCurrentAddress()) {
+      MCSymbol *DotSym = OutContext.CreateTempSymbol();
+      OutStreamer.EmitLabel(DotSym);
+      const MCExpr *DotExpr = MCSymbolRefExpr::Create(DotSym, OutContext);
+      Expr = MCBinaryExpr::CreateSub(Expr, DotExpr, OutContext);
+    }
   }
+  // @LOCALMOD-END
+
   OutStreamer.EmitValue(Expr, Size);
 }
 
@@ -1076,6 +1178,25 @@
     unsigned CPIdx   = (unsigned)MI->getOperand(1).getIndex();
 
     EmitAlignment(2);
+    // @LOCALMOD-START
+    // NOTE: we also should make sure that the first data item
+    // is not in a code bundle
+    // NOTE: there may be issues with alignment constraints
+    const unsigned size = MI->getOperand(2).getImm();
+    //assert(size == 4 || size == 8 && "Unsupported data item size");
+    if (size == 8) {
+      // we cannot generate a size 8 constant at offset 12 (mod 16)
+      OutStreamer.EmitRawText(StringRef("sfi_nop_if_at_bundle_end\n"));
+    }
+
+    if (FlagSfiData) {
+      SmallString<128> Str;
+      raw_svector_ostream OS(Str);
+      OS << "sfi_illegal_if_at_bundle_begining  @ ========== SFI (" << 
+            size << ")\n";
+      OutStreamer.EmitRawText(OS.str());
+    }
+    // @LOCALMOD-END
     OutStreamer.EmitLabel(GetCPISymbol(LabelId));
 
     const MachineConstantPoolEntry &MCPE = MCP->getConstants()[CPIdx];
@@ -1526,6 +1647,50 @@
   case ARM::UMAALv5:
     EmitPatchedInstruction(MI, ARM::UMAAL);
     return;
+
+  // @LOCALMOD-BEGIN
+  // These are pseudo ops for MOVW / MOVT with operands relative to a PC label.
+  // See the comments on MOVi16PIC in the .td file for more details.
+  case ARM::MOVi16PIC: {
+    MCInst TmpInst;
+    // First, build an instruction w/ the real opcode.
+    TmpInst.setOpcode(ARM::MOVi16);
+
+    unsigned ImmIndex = 1;
+    unsigned PIC_id_index = 2;
+    unsigned PCAdjustment = 8;
+    // NOTE: if getPICLabel was a method of "this", or otherwise in scope for
+    // LowerARMMachineInstrToMCInstPCRel, then we wouldn't need to create
+    // it here (as well as below).
+    MCSymbol *PCLabel = getPICLabel(MAI->getPrivateGlobalPrefix(),
+                                    getFunctionNumber(),
+                                    MI->getOperand(PIC_id_index).getImm(),
+                                    OutContext);
+    LowerARMMachineInstrToMCInstPCRel(MI, TmpInst, *this, ImmIndex,
+                                      PIC_id_index, PCLabel, PCAdjustment);
+    OutStreamer.EmitInstruction(TmpInst);
+    return;
+  }
+  case ARM::MOVTi16PIC: {
+    MCInst TmpInst;
+    // First, build an instruction w/ the real opcode.
+    TmpInst.setOpcode(ARM::MOVTi16);
+
+    unsigned ImmIndex = 2;
+    unsigned PIC_id_index = 3;
+    unsigned PCAdjustment = 8;
+
+    MCSymbol *PCLabel = getPICLabel(MAI->getPrivateGlobalPrefix(),
+                                    getFunctionNumber(),
+                                    MI->getOperand(PIC_id_index).getImm(),
+                                    OutContext);
+
+    LowerARMMachineInstrToMCInstPCRel(MI, TmpInst, *this, ImmIndex,
+                                      PIC_id_index, PCLabel, PCAdjustment);
+    OutStreamer.EmitInstruction(TmpInst);
+    return;
+  }
+  //@LOCALMOD-END
   }
 
   MCInst TmpInst;
diff -r 2b13dadc8fed lib/Target/ARM/ARMAsmPrinter.h
--- a/lib/Target/ARM/ARMAsmPrinter.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMAsmPrinter.h	Thu Jun 09 18:06:40 2011 -0700
@@ -67,11 +67,27 @@
   virtual void EmitInstruction(const MachineInstr *MI);
   bool runOnMachineFunction(MachineFunction &F);
 
-  virtual void EmitConstantPool() {} // we emit constant pools customly!
+  // @LOCALMOD-START
+  // usually this does nothing on ARM as constants pools
+  // are handled with custom code.
+  // For the sfi case we do not use the custom logic and fall back
+  // to the default implementation.
+  virtual void EmitConstantPool() {
+    if (FlagSfiDisableCP) AsmPrinter::EmitConstantPool();
+  }
+  // @LOCALMOD-END
+
   virtual void EmitFunctionEntryLabel();
   void EmitStartOfAsmFile(Module &M);
   void EmitEndOfAsmFile(Module &M);
 
+  // @LOCALMOD-START
+  virtual bool UseReadOnlyJumpTables() const {
+    return true; // should be IsNaCl
+  }
+  virtual unsigned GetTargetLabelAlign(const MachineInstr *MI) const;
+  // @LOCALMOD-END
+
 private:
   // Helpers for EmitStartOfAsmFile() and EmitEndOfAsmFile()
   void emitAttributes();
diff -r 2b13dadc8fed lib/Target/ARM/ARMBaseInstrInfo.cpp
--- a/lib/Target/ARM/ARMBaseInstrInfo.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMBaseInstrInfo.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -517,6 +517,9 @@
   return JT[JTI].MBBs.size();
 }
 
+// @LOCALMOD-START
+// @NOTE: this needs to be fixe to make the constand island estimates better
+// @LOCALMOD-END
 /// GetInstSize - Return the size of the specified MachineInstr.
 ///
 unsigned ARMBaseInstrInfo::GetInstSizeInBytes(const MachineInstr *MI) const {
@@ -629,7 +632,8 @@
   bool GPRSrc  = ARM::GPRRegClass.contains(SrcReg);
 
   if (GPRDest && GPRSrc) {
-    AddDefaultCC(AddDefaultPred(BuildMI(MBB, I, DL, get(ARM::MOVr), DestReg)
+    unsigned Opc = ARM::MOVr;
+    AddDefaultCC(AddDefaultPred(BuildMI(MBB, I, DL, get(Opc), DestReg)
                                   .addReg(SrcReg, getKillRegState(KillSrc))));
     return;
   }
@@ -1343,7 +1347,9 @@
     assert(ARM_AM::getSOImmVal(ThisVal) != -1 && "Bit extraction didn't work?");
 
     // Build the new ADD / SUB.
-    unsigned Opc = isSub ? ARM::SUBri : ARM::ADDri;
+    unsigned Opc;
+    Opc = isSub ? ARM::SUBri : ARM::ADDri;
+
     BuildMI(MBB, MBBI, dl, TII.get(Opc), DestReg)
       .addReg(BaseReg, RegState::Kill).addImm(ThisVal)
       .addImm((unsigned)Pred).addReg(PredReg).addReg(0);
diff -r 2b13dadc8fed lib/Target/ARM/ARMBaseRegisterInfo.cpp
--- a/lib/Target/ARM/ARMBaseRegisterInfo.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMBaseRegisterInfo.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -19,6 +19,7 @@
 #include "ARMInstrInfo.h"
 #include "ARMMachineFunctionInfo.h"
 #include "ARMSubtarget.h"
+#include "ARMTargetMachine.h"  // @LOCALMOD
 #include "llvm/Constants.h"
 #include "llvm/DerivedTypes.h"
 #include "llvm/Function.h"
@@ -60,6 +61,8 @@
     BasePtr(ARM::R6) {
 }
 
+extern cl::opt<bool> ReserveR9; // @LOCALMOD
+
 const unsigned*
 ARMBaseRegisterInfo::getCalleeSavedRegs(const MachineFunction *MF) const {
   static const unsigned CalleeSavedRegs[] = {
@@ -71,6 +74,15 @@
     0
   };
 
+  static const unsigned CalleeSavedRegsNoR9[] = { // @LOCALMOD
+    ARM::LR, ARM::R11, ARM::R10, ARM::R8,
+    ARM::R7, ARM::R6,  ARM::R5,  ARM::R4,
+
+    ARM::D15, ARM::D14, ARM::D13, ARM::D12,
+    ARM::D11, ARM::D10, ARM::D9,  ARM::D8,
+    0
+  };
+
   static const unsigned DarwinCalleeSavedRegs[] = {
     // Darwin ABI deviates from ARM standard ABI. R9 is not a callee-saved
     // register.
@@ -81,7 +93,11 @@
     ARM::D11, ARM::D10, ARM::D9,  ARM::D8,
     0
   };
-  return STI.isTargetDarwin() ? DarwinCalleeSavedRegs : CalleeSavedRegs;
+  if (STI.isTargetDarwin())
+    return DarwinCalleeSavedRegs;
+  if (ReserveR9)
+    return CalleeSavedRegsNoR9; // @LOCALMOD
+  return CalleeSavedRegs;
 }
 
 BitVector ARMBaseRegisterInfo::
@@ -807,6 +823,13 @@
                   unsigned DestReg, unsigned SubIdx, int Val,
                   ARMCC::CondCodes Pred,
                   unsigned PredReg) const {
+  // @LOCALMOD-START
+  // In the sfi case we do not want to use the load const pseudo instr.
+  // Sadly, the ARM backend is not very consistent about using this
+  // pseudo instr. and hence checking this is not sufficient.
+  // But, it should help detect some regressions early.
+  assert(!FlagSfiDisableCP && "unexpected call to emitLoadConstPool");
+  // @LOCALMOD-END
   MachineFunction &MF = *MBB.getParent();
   MachineConstantPool *ConstantPool = MF.getConstantPool();
   const Constant *C =
diff -r 2b13dadc8fed lib/Target/ARM/ARMConstantIslandPass.cpp
--- a/lib/Target/ARM/ARMConstantIslandPass.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMConstantIslandPass.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -18,6 +18,8 @@
 #include "ARMAddressingModes.h"
 #include "ARMMachineFunctionInfo.h"
 #include "ARMInstrInfo.h"
+#include "ARMNaClRewritePass.h"  // @LOCALMOD
+#include "ARMTargetMachine.h"  // @LOCALMOD
 #include "Thumb2InstrInfo.h"
 #include "llvm/CodeGen/MachineConstantPool.h"
 #include "llvm/CodeGen/MachineFunctionPass.h"
@@ -46,6 +48,12 @@
 STATISTIC(NumJTMoved,    "Number of jump table destination blocks moved");
 STATISTIC(NumJTInserted, "Number of jump table intermediate blocks inserted");
 
+// @LOCALMOD-START
+#include "llvm/Support/CommandLine.h"
+cl::opt<bool> FlagSfiCpDisableVerify("sfi-cp-disable-verify");
+cl::opt<bool> FlagSfiCpFudge("sfi-cp-fudge");
+cl::opt<int> FlagSfiCpFudgePercent("sfi-cp-fudge-percent", cl::init(85));
+// @LOCALMOD-END
 
 static cl::opt<bool>
 AdjustJumpTableBlocks("arm-adjust-jump-tables", cl::Hidden, cl::init(true),
@@ -166,6 +174,7 @@
     bool HasInlineAsm;
 
     const ARMInstrInfo *TII;
+    const TargetRegisterInfo *TRI; // @LOCALMOD
     const ARMSubtarget *STI;
     ARMFunctionInfo *AFI;
     bool isThumb;
@@ -182,6 +191,14 @@
     }
 
   private:
+    // @LOCALMOD-BEGIN
+    unsigned GetFudge(const MachineInstr* I,
+                      unsigned  offset,
+                      bool is_start,
+                      bool is_end,
+                      bool is_jump_target) const;
+    // @LOCALMOD-END
+ 
     void DoInitialPlacement(MachineFunction &MF,
                             std::vector<MachineInstr*> &CPEMIs);
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
@@ -228,8 +245,21 @@
 /// verify - check BBOffsets, BBSizes, alignment of islands
 void ARMConstantIslands::verify(MachineFunction &MF) {
   assert(BBOffsets.size() == BBSizes.size());
-  for (unsigned i = 1, e = BBOffsets.size(); i != e; ++i)
-    assert(BBOffsets[i-1]+BBSizes[i-1] == BBOffsets[i]);
+  for (unsigned i = 1, e = BBOffsets.size(); i != e; ++i) 
+    
+    // @LOCALMOD-START
+    // NOTE: this is horrible hack and needs to be cleaned up when
+    //       we revisit constant pools for arm 
+    {
+      if (FlagSfiCpDisableVerify) {
+	if (!(BBOffsets[i-1]+BBSizes[i-1] == BBOffsets[i])) {
+	  errs() << "\nCONSTANT POOL INCONSISTENCY IN SIZES - IGNORED\n\n";
+	}
+      } else {
+	assert(BBOffsets[i-1]+BBSizes[i-1] == BBOffsets[i]);
+      }
+  } 
+  // @LOCALMOD-END
   if (!isThumb)
     return;
 #ifndef NDEBUG
@@ -239,9 +269,19 @@
     if (!MBB->empty() &&
         MBB->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY) {
       unsigned MBBId = MBB->getNumber();
-      assert(HasInlineAsm ||
-             (BBOffsets[MBBId]%4 == 0 && BBSizes[MBBId]%4 == 0) ||
-             (BBOffsets[MBBId]%4 != 0 && BBSizes[MBBId]%4 != 0));
+      // @LOCALMOD-START
+      if (FlagSfiCpDisableVerify) {
+	if (!(HasInlineAsm ||
+	      (BBOffsets[MBBId]%4 == 0 && BBSizes[MBBId]%4 == 0) ||
+	      (BBOffsets[MBBId]%4 != 0 && BBSizes[MBBId]%4 != 0))) {
+	  errs() << "\nCONSTANT POOL INCONSISTENCY IN ALIGNMENT - IGNORED\n\n";
+	}
+      } else {
+	assert(HasInlineAsm ||
+	       (BBOffsets[MBBId]%4 == 0 && BBSizes[MBBId]%4 == 0) ||
+	       (BBOffsets[MBBId]%4 != 0 && BBSizes[MBBId]%4 != 0));
+      }
+      // @LOCALMOD-END
     }
   }
   for (unsigned i = 0, e = CPUsers.size(); i != e; ++i) {
@@ -250,7 +290,16 @@
     unsigned CPEOffset  = GetOffsetOf(U.CPEMI);
     unsigned Disp = UserOffset < CPEOffset ? CPEOffset - UserOffset :
       UserOffset - CPEOffset;
-    assert(Disp <= U.MaxDisp || "Constant pool entry out of range!");
+    // @LOCALMOD-START
+    if (FlagSfiCpDisableVerify) {
+      if(Disp > U.MaxDisp) {
+	errs() << "\nCONSTANT POOL INCONSISTENCY IN DISP - IGNORED\n\n";
+      }
+    } else {
+      // NOTE the original assert used || which is a bug 
+      assert(Disp <= U.MaxDisp && "Constant pool entry out of range!");
+    }
+    // @LOCALMOD-END
   }
 #endif
 }
@@ -270,9 +319,11 @@
 }
 
 bool ARMConstantIslands::runOnMachineFunction(MachineFunction &MF) {
+  if (FlagSfiDisableCP) return false;   // @LOCALMOD
   MachineConstantPool &MCP = *MF.getConstantPool();
 
   TII = (const ARMInstrInfo*)MF.getTarget().getInstrInfo();
+  TRI = MF.getTarget().getRegisterInfo(); // @LOCALMOD
   AFI = MF.getInfo<ARMFunctionInfo>();
   STI = &MF.getTarget().getSubtarget<ARMSubtarget>();
 
@@ -473,6 +524,126 @@
   }
 }
 
+
+//@LOCALMOD-START
+// We try to account for extra sfi space overhead here
+// NOTE: This function needs to be updated whenever changes
+//       to the sfi scheme are made
+// NOTE: this is very likely missing a few cases
+//       we will add those as neeeded and otherwise
+//       rely on artificially reducing the ldr offset range.
+// NOTE: one missing case: jump table targets are 16 bytes aligned
+unsigned ARMConstantIslands::GetFudge(const MachineInstr* I,
+                                      unsigned  offset,
+                                      bool is_start,
+                                      bool is_end,
+                                      bool is_jump_target) const {
+  if (!FlagSfiCpFudge) return 0;
+  const int kBundleSize = 16;
+  unsigned fudge = 0;
+  const int Opc = I->getOpcode();
+
+  if (is_jump_target && is_start) {
+    while ( (offset + fudge) % kBundleSize != 0) fudge += 4;
+  }
+
+  switch(Opc) {
+   case ARM::BL:
+   case ARM::BLX:
+   case ARM::BL_pred:
+   case ARM::BLr9:
+   case ARM::BLXr9:
+   case ARM::BLr9_pred:
+   case ARM::TPsoft:
+    // branches must be in the last slot
+    while ( (offset + fudge) % kBundleSize != 0xc) fudge += 4;
+    break;
+
+   case ARM::CONSTPOOL_ENTRY:
+    if (is_start) {
+      while ( (offset + fudge) % kBundleSize != 0) fudge += 4;
+    }
+
+    if (is_end) {
+      while ( (offset + fudge) % kBundleSize != 0xc) fudge += 4;
+    }
+
+    {
+      const int size = TII->GetInstSizeInBytes(I);
+      assert (size == 4 || size == 8);
+      // we do not want the data to cross bundle boundaries
+      if (size == 8) {
+        if((offset + fudge) % kBundleSize == 0xc) fudge += 4;
+      }
+    }
+    // illegal if at data bundle beginning
+    if ((offset + fudge) % kBundleSize == 0) fudge += 4;
+    break;
+
+   case ARM::STR_PRE:
+   case ARM::STR_POST:
+   case ARM::STRB_PRE:
+   case ARM::STRB_POST:
+   case ARM::STRH:
+   case ARM::STRD:
+    // TODO: there are vfp stores missing
+   case ARM::VSTRS:
+   case ARM::VSTRD:
+
+    // case ARM::STM:// TODO: make this work
+    {
+    const MachineOperand &MO1 = I->getOperand(1);
+    if (MO1.getReg() != ARM::SP) {
+      // cannot be in the last slot
+      if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+      // one mask
+      fudge += 4;
+    }
+    break;
+    }
+  case ARM::BX:
+  case ARM::BXr9_CALL:
+  case ARM::BX_RET:
+    // cannot be in the last slot
+    if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+    // one mask
+    fudge += 4;
+    break;
+  }
+
+  // Check for stack adjustments, which will be sandboxed later
+  // (possibly adding padding and a data mask instrs).
+  if (ARM_SFI::NeedSandboxStackChange(*I, TRI)) {
+    // stack adjusts must not be in the last slot
+    if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+    // add masking
+    fudge += 4;
+  }
+
+  return fudge;
+}
+
+static void UpdateJumpTargetAlignment(MachineFunction &MF) {
+  if (!FlagSfiBranch) return;
+
+  // JUMP TABLE TARGETS
+  MachineJumpTableInfo *jt_info = MF.getJumpTableInfo();
+  const std::vector<MachineJumpTableEntry> &JT = jt_info->getJumpTables();
+  for (unsigned i=0; i < JT.size(); ++i) {
+    std::vector<MachineBasicBlock*> MBBs = JT[i].MBBs;
+
+    //cout << "JUMPTABLE "<< i << " " << MBBs.size() << "\n";
+    for (unsigned j=0; j < MBBs.size(); ++j) {
+      if (MBBs[j]->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY) {
+        continue;
+      }
+      MBBs[j]->setAlignment(16);
+    }
+  }
+}
+
+//@LOCALMOD-END
+
 /// InitialFunctionScan - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
@@ -506,6 +677,15 @@
          I != E; ++I) {
       if (I->isDebugValue())
         continue;
+      //@LOCALMOD-START
+      // TODO: also account for jump_targets more
+      MBBSize += GetFudge(I,
+                          Offset,
+                          I == MBB.begin(),
+                          I == E,
+                          MF.begin() == MBBI || MBBI->getAlignment() == 16);
+      //@LOCALMOD-END
+
       // Add instruction size to MBBSize.
       MBBSize += TII->GetInstSizeInBytes(I);
 
@@ -635,6 +815,14 @@
           unsigned CPI = I->getOperand(op).getIndex();
           MachineInstr *CPEMI = CPEMIs[CPI];
           unsigned MaxOffs = ((1 << Bits)-1) * Scale;
+
+          // @LOCALMOD-BEGIN
+          if (FlagSfiCpFudge) {
+            MaxOffs *= FlagSfiCpFudgePercent;
+            MaxOffs /= 100;
+          }
+          // @LOCALMOD-END
+          
           CPUsers.push_back(CPUser(I, CPEMI, MaxOffs, NegOk, IsSoImm));
 
           // Increment corresponding CPEntry reference count.
@@ -685,6 +873,10 @@
     assert(I != MBB->end() && "Didn't find MI in its own basic block?");
     if (&*I == MI) return Offset;
     Offset += TII->GetInstSizeInBytes(I);
+    // @LOCALMOD-START
+    // TODO: take jump targets into account
+    Offset += GetFudge(I, Offset, I ==  MBB->begin(), I == MBB->end(), 0);
+    // @LOCALMOD-END
   }
 }
 
@@ -807,7 +999,13 @@
   unsigned NewBBSize = 0;
   for (MachineBasicBlock::iterator I = NewBB->begin(), E = NewBB->end();
        I != E; ++I)
+  // @LOCALMOD-START
+  {
+    NewBBSize += GetFudge(I, NewBBSize, false, false, 0);
     NewBBSize += TII->GetInstSizeInBytes(I);
+  }
+  // @LOCALMOD-END
+
   // Set the size of NewBB in BBSizes.  It does not include any padding now.
   BBSizes[NewBBI] = NewBBSize;
 
@@ -932,6 +1130,16 @@
 void ARMConstantIslands::AdjustBBOffsetsAfter(MachineBasicBlock *BB,
                                               int delta) {
   MachineFunction::iterator MBBI = BB; MBBI = llvm::next(MBBI);
+  // @LOCALMOD-START
+  // TODO: explain this
+  if (delta > 0) {
+    BBSizes[BB->getNumber()] += 4;  // @LOCALMOD
+    delta += 4;
+  }
+  // @LOCALMOD-END
+
+
+  
   for(unsigned i = BB->getNumber()+1, e = BB->getParent()->getNumBlockIDs();
       i < e; ++i) {
     BBOffsets[i] += delta;
@@ -1131,7 +1339,8 @@
   MachineBasicBlock *UserMBB = UserMI->getParent();
   unsigned OffsetOfNextBlock = BBOffsets[UserMBB->getNumber()] +
                                BBSizes[UserMBB->getNumber()];
-  assert(OffsetOfNextBlock== BBOffsets[UserMBB->getNumber()+1]);
+  // @LOCALMOD
+  // assert(OffsetOfNextBlock== BBOffsets[UserMBB->getNumber()+1]);
 
   // If the block does not end in an unconditional branch already, and if the
   // end of the block is within range, make new water there.  (The addition
@@ -1175,7 +1384,17 @@
     // The 4 in the following is for the unconditional branch we'll be
     // inserting (allows for long branch on Thumb1).  Alignment of the
     // island is handled inside OffsetIsInRange.
-    unsigned BaseInsertOffset = UserOffset + U.MaxDisp -4;
+
+    // @LOCALMOD-START
+    //unsigned BaseInsertOffset = UserOffset + U.MaxDisp -4;
+     unsigned BaseInsertOffset = UserOffset - 4;
+     if (FlagSfiCpFudge) {
+       BaseInsertOffset += U.MaxDisp * FlagSfiCpFudgePercent / 100;
+     } else {
+       BaseInsertOffset += U.MaxDisp;
+     }
+     // @LOCALMOD-END
+
     // This could point off the end of the block if we've already got
     // constant pool entries following this block; only the last one is
     // in the water list.  Back past any possible branches (allow for a
@@ -1190,9 +1409,16 @@
     unsigned CPUIndex = CPUserIndex+1;
     unsigned NumCPUsers = CPUsers.size();
     MachineInstr *LastIT = 0;
+    MachineBasicBlock::iterator EndBB = UserMBB->end(); //@LOCALMOD
+    // @LOCALMOD: TODO: GetInstSizeInBytes() should be replaced with
+    //                  our estimator
     for (unsigned Offset = UserOffset+TII->GetInstSizeInBytes(UserMI);
-         Offset < BaseInsertOffset;
-         Offset += TII->GetInstSizeInBytes(MI),
+         Offset < BaseInsertOffset && MI != EndBB; //@LOCALMOD
+         // @LOCALMOD-START
+         //Offset += TII->GetInstSizeInBytes(MI),
+         Offset +=  GetFudge(MI, Offset, false, false, false) +
+                           TII->GetInstSizeInBytes(MI),
+        // @LOCALMOD-END 
            MI = llvm::next(MI)) {
       if (CPUIndex < NumCPUsers && CPUsers[CPUIndex].MI == MI) {
         CPUser &U = CPUsers[CPUIndex];
@@ -1422,6 +1648,10 @@
 /// Otherwise, add an intermediate branch instruction to a branch.
 bool
 ARMConstantIslands::FixUpUnconditionalBr(MachineFunction &MF, ImmBranch &Br) {
+  // @LOCALMOD-start
+  assert(0 && "fix up uncond br not implemented");
+  // @LOCALMOD-end
+ 
   MachineInstr *MI = Br.MI;
   MachineBasicBlock *MBB = MI->getParent();
   if (!isThumb1)
@@ -1445,6 +1675,9 @@
 /// conditional branch + an unconditional branch to the destination.
 bool
 ARMConstantIslands::FixUpConditionalBr(MachineFunction &MF, ImmBranch &Br) {
+ // @LOCALMOD-start
+  assert(0 && "fix up cond br not implemented");
+  // @LOCALMOD-end
   MachineInstr *MI = Br.MI;
   MachineBasicBlock *DestBB = MI->getOperand(0).getMBB();
 
diff -r 2b13dadc8fed lib/Target/ARM/ARMConstantPoolValue.cpp
--- a/lib/Target/ARM/ARMConstantPoolValue.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMConstantPoolValue.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -44,6 +44,8 @@
   : MachineConstantPoolValue((const Type*)Type::getInt32Ty(gv->getContext())),
     CVal(gv), S(NULL), LabelId(0), Kind(ARMCP::CPValue), PCAdjust(0),
     Modifier(Modif), AddCurrentAddress(false) {}
+    // @LOCALMOD     ^^^ (should show up in next merge)
+
 
 const GlobalValue *ARMConstantPoolValue::getGV() const {
   return dyn_cast_or_null<GlobalValue>(CVal);
diff -r 2b13dadc8fed lib/Target/ARM/ARMConstantPoolValue.h
--- a/lib/Target/ARM/ARMConstantPoolValue.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMConstantPoolValue.h	Thu Jun 09 18:06:40 2011 -0700
@@ -95,6 +95,9 @@
   bool isExtSymbol() const { return Kind == ARMCP::CPExtSymbol; }
   bool isBlockAddress() { return Kind == ARMCP::CPBlockAddress; }
   bool isLSDA() { return Kind == ARMCP::CPLSDA; }
+  // @LOCALMOD-START
+  bool isValue() const { return Kind == ARMCP::CPValue; }
+  // @LOCALMOD-END
 
   virtual unsigned getRelocationInfo() const { return 2; }
 
diff -r 2b13dadc8fed lib/Target/ARM/ARMExpandPseudoInsts.cpp
--- a/lib/Target/ARM/ARMExpandPseudoInsts.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMExpandPseudoInsts.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -39,6 +39,7 @@
     const TargetRegisterInfo *TRI;
     const ARMSubtarget *STI;
     ARMFunctionInfo *AFI;
+    bool IsRelocPIC; // @LOCALMOD
 
     virtual bool runOnMachineFunction(MachineFunction &Fn);
 
@@ -59,6 +60,16 @@
                     unsigned Opc, bool IsExt, unsigned NumRegs);
     void ExpandMOV32BitImm(MachineBasicBlock &MBB,
                            MachineBasicBlock::iterator &MBBI);
+    // @LOCALMOD-BEGIN
+    void AddPICADD_MOVi16_PICID(MachineInstr &MI,
+                                MachineBasicBlock &MBB,
+                                MachineBasicBlock::iterator &MBBI,
+                                bool NotThumb,
+                                unsigned PredReg, ARMCC::CondCodes Pred,
+                                unsigned DstReg, bool DstIsDead,
+                                MachineInstrBuilder &LO16,
+                                MachineInstrBuilder &HI16);
+    // @LOCALMOD-END
   };
   char ARMExpandPseudo::ID = 0;
 }
@@ -503,6 +514,41 @@
   MI.eraseFromParent();
 }
 
+// @LOCALMOD-BEGIN
+// AddPICADD_MOVi16_PICID - Inserts a PICADD into the given basic block,
+// and adds the PC label ID (of the PICADD) as an operand of the LO16 / HI16
+// MOVs. The ID operand will follow the "Immediate" operand (assumes that
+// operand is already added).
+void ARMExpandPseudo::AddPICADD_MOVi16_PICID(MachineInstr &MI,
+                                       MachineBasicBlock &MBB,
+                                       MachineBasicBlock::iterator &MBBI,
+                                       bool NotThumb,
+                                       unsigned PredReg, ARMCC::CondCodes Pred,
+                                       unsigned DstReg, bool DstIsDead,
+                                       MachineInstrBuilder &LO16,
+                                       MachineInstrBuilder &HI16) {
+  // Throw in a PICADD, and tack on the PC label ID to the MOVT/MOVWs
+  MachineFunction &MF = *MI.getParent()->getParent();
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+
+  // Make a unique ID for this PC by pulling from pool of constPoolIDs
+  unsigned PC_ID = AFI->createPICLabelUId();
+  MachineInstrBuilder PicADD =
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(NotThumb ? ARM::PICADD : ARM::tPICADD))
+      .addReg(DstReg, RegState::Define | getDeadRegState(DstIsDead))
+      .addReg(DstReg)
+      .addImm(PC_ID)
+      .addImm(Pred)
+      .addReg(PredReg);
+  (void)PicADD; // squelch unused warning.
+
+  // Add the PC label ID after what would have been an absolute address.
+  LO16 = LO16.addImm(PC_ID);
+  HI16 = HI16.addImm(PC_ID);
+}
+// @LOCALMOD-END
+
 /// ExpandLaneOp - Translate VLD*LN and VST*LN instructions with Q, QQ or QQQQ
 /// register operands to real instructions with D register operands.
 void ARMExpandPseudo::ExpandLaneOp(MachineBasicBlock::iterator &MBBI) {
@@ -666,7 +712,9 @@
 
   unsigned LO16Opc = 0;
   unsigned HI16Opc = 0;
-  if (Opcode == ARM::t2MOVi32imm || Opcode == ARM::t2MOVCCi32imm) {
+  // @LOCALMOD
+  bool isThumb2 = (Opcode == ARM::t2MOVi32imm || Opcode == ARM::t2MOVCCi32imm);
+  if (isThumb2) {
     LO16Opc = ARM::t2MOVi16;
     HI16Opc = ARM::t2MOVTi16;
   } else {
@@ -674,10 +722,28 @@
     HI16Opc = ARM::MOVTi16;
   }
 
+  // @LOCALMOD-BEGIN
+  // If constant pools are "disabled" (actually, moved to rodata), then
+  // many addresses (e.g., the addresses of what used to be the "pools")
+  // may not be materialized in a pc-relative manner, because MOVT / MOVW
+  // are used to materialize the addresses.
+  // We need to know if it matters that references are pc-relative
+  // (e.g., to be PIC).
+  // See the comments on MOVi16PIC / MOVTi16PIC for more details.
+  const bool ShouldUseMOV16PIC = FlagSfiDisableCP && IsRelocPIC &&
+      (MO.isCPI() || MO.isJTI() || MO.isGlobal()); // TODO check this list.
+  if (ShouldUseMOV16PIC) {
+    if (isThumb2)
+      llvm_unreachable("FIXME: add PIC versions of t2MOVi16");
+    LO16Opc = ARM::MOVi16PIC;
+    HI16Opc = ARM::MOVTi16PIC;
+  }
+  // @LOCALMOD-END
+
   LO16 = BuildMI(MBB, MBBI, MI.getDebugLoc(), TII->get(LO16Opc), DstReg);
   HI16 = BuildMI(MBB, MBBI, MI.getDebugLoc(), TII->get(HI16Opc))
     .addReg(DstReg, RegState::Define | getDeadRegState(DstIsDead))
-    .addReg(DstReg);
+    .addReg(DstReg, RegState::Kill); // @LOCALMOD
 
   if (MO.isImm()) {
     unsigned Imm = MO.getImm();
@@ -685,13 +751,31 @@
     unsigned Hi16 = (Imm >> 16) & 0xffff;
     LO16 = LO16.addImm(Lo16);
     HI16 = HI16.addImm(Hi16);
-  } else {
+  } else if (MO.isGlobal()) { // @LOCALMOD
     const GlobalValue *GV = MO.getGlobal();
     unsigned TF = MO.getTargetFlags();
     LO16 = LO16.addGlobalAddress(GV, MO.getOffset(), TF | ARMII::MO_LO16);
     HI16 = HI16.addGlobalAddress(GV, MO.getOffset(), TF | ARMII::MO_HI16);
+  // @LOCALMOD-START - support for jumptable addresses and CPI
+  } else if (MO.isCPI()) {
+    int i = MO.getIndex();
+    unsigned TF = MO.getTargetFlags();
+    LO16 = LO16.addConstantPoolIndex(i, MO.getOffset(), TF|ARMII::MO_LO16);
+    HI16 = HI16.addConstantPoolIndex(i, MO.getOffset(), TF|ARMII::MO_HI16);
+  } else if (MO.isJTI()){
+    unsigned TF = MO.getTargetFlags();
+    LO16 = LO16.addJumpTableIndex(MO.getIndex(), TF | ARMII::MO_LO16);
+    HI16 = HI16.addJumpTableIndex(MO.getIndex(), TF | ARMII::MO_HI16);
+  } else {
+    assert (0 && "unexpected operand");
+  // @LOCALMOD-END
   }
-
+  // @LOCALMOD-BEGIN
+  if (ShouldUseMOV16PIC) {
+    AddPICADD_MOVi16_PICID(MI, MBB, MBBI, !isThumb2,
+                           PredReg, Pred, DstReg, DstIsDead, LO16, HI16);
+  }
+  // @LOCALMOD-END
   (*LO16).setMemRefs(MI.memoperands_begin(), MI.memoperands_end());
   (*HI16).setMemRefs(MI.memoperands_begin(), MI.memoperands_end());
   LO16.addImm(Pred).addReg(PredReg);
@@ -1203,6 +1287,62 @@
     case ARM::VTBX2Pseudo: ExpandVTBL(MBBI, ARM::VTBX2, true, 2); return true;
     case ARM::VTBX3Pseudo: ExpandVTBL(MBBI, ARM::VTBX3, true, 3); return true;
     case ARM::VTBX4Pseudo: ExpandVTBL(MBBI, ARM::VTBX4, true, 4); return true;
+
+    // @LOCALMOD-BEGIN
+    case ARM::ARMeh_return: {
+      // This pseudo instruction is generated as part of the lowering of
+      // ISD::EH_RETURN (c.f. ARMISelLowering.cpp)
+      // we convert it to a stack increment by OffsetReg and
+      // indirect jump to TargetReg
+      unsigned PredReg = 0;
+      ARMCC::CondCodes Pred = llvm::getInstrPredicate(&MI, PredReg);
+      unsigned OffsetReg = MI.getOperand(0).getReg();
+      unsigned TargetReg = MI.getOperand(1).getReg();
+      BuildMI(MBB, MBBI, MI.getDebugLoc(), TII->get(ARM::ADDrr), ARM::SP)
+          .addReg(OffsetReg)
+          .addReg(ARM::SP)
+          .addImm(Pred)
+          .addReg(PredReg)
+          .addReg(0);
+
+      BuildMI(MBB, MBBI, MI.getDebugLoc(), TII->get(ARM::BX))
+          .addReg(TargetReg);
+      MI.eraseFromParent();
+      break;
+    }
+    case ARM::MOVGOTAddr : {
+      // Expand the pseudo-inst that requests for the GOT address
+      // to be materialized into a register. We use MOVW/MOVT for this.
+      // See ARMISelLowering.cpp for a comment on the strategy.
+      unsigned PredReg = 0;
+      ARMCC::CondCodes Pred = llvm::getInstrPredicate(&MI, PredReg);
+      unsigned DstReg = MI.getOperand(0).getReg();
+      bool DstIsDead = MI.getOperand(0).isDead();
+      MachineInstrBuilder LO16, HI16;
+
+      LO16 = BuildMI(MBB, MBBI, MI.getDebugLoc(),
+                     TII->get(ARM::MOVi16PIC),
+                     DstReg)
+        .addExternalSymbol("_GLOBAL_OFFSET_TABLE_", ARMII::MO_LO16);
+
+      HI16 = BuildMI(MBB, MBBI, MI.getDebugLoc(),
+                     TII->get(ARM::MOVTi16PIC))
+        .addReg(DstReg, RegState::Define | getDeadRegState(DstIsDead))
+        .addReg(DstReg)
+        .addExternalSymbol("_GLOBAL_OFFSET_TABLE_", ARMII::MO_HI16);
+
+      AddPICADD_MOVi16_PICID(MI, MBB, MBBI, true,
+                             PredReg, Pred, DstReg, DstIsDead, LO16, HI16);
+
+      (*LO16).setMemRefs(MI.memoperands_begin(), MI.memoperands_end());
+      (*HI16).setMemRefs(MI.memoperands_begin(), MI.memoperands_end());
+      LO16.addImm(Pred).addReg(PredReg);
+      HI16.addImm(Pred).addReg(PredReg);
+      TransferImpOps(MI, LO16, HI16);
+      MI.eraseFromParent();
+      break;
+    }
+    // @LOCALMOD-END
   }
 
   return false;
@@ -1227,6 +1367,7 @@
   TRI = TM.getRegisterInfo();
   STI = &TM.getSubtarget<ARMSubtarget>();
   AFI = MF.getInfo<ARMFunctionInfo>();
+  IsRelocPIC = MF.getTarget().getRelocationModel() == Reloc::PIC_;
 
   bool Modified = false;
   for (MachineFunction::iterator MFI = MF.begin(), E = MF.end(); MFI != E;
diff -r 2b13dadc8fed lib/Target/ARM/ARMFastISel.cpp
--- a/lib/Target/ARM/ARMFastISel.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMFastISel.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -543,6 +543,11 @@
 }
 
 unsigned ARMFastISel::TargetMaterializeConstant(const Constant *C) {
+  // @LOCALMOD-START
+  // In the sfi case we do not want to use the ARM custom cp handling.
+  // This assert should help detect some regressions early.
+  assert(!FlagSfiDisableCP && "unexpected call to TargetMaterializeConstant");
+  // @LOCALMOD-END
   EVT VT = TLI.getValueType(C->getType(), true);
 
   // Only handle simple types.
diff -r 2b13dadc8fed lib/Target/ARM/ARMFrameLowering.cpp
--- a/lib/Target/ARM/ARMFrameLowering.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMFrameLowering.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -22,6 +22,10 @@
 #include "llvm/CodeGen/MachineRegisterInfo.h"
 #include "llvm/CodeGen/RegisterScavenging.h"
 #include "llvm/Target/TargetOptions.h"
+// @LOCALMOD-START
+#include "llvm/CodeGen/MachineModuleInfo.h"
+#include "llvm/Function.h"
+// @LOCALMOD-END
 
 using namespace llvm;
 
@@ -116,6 +120,16 @@
                            Pred, PredReg, TII);
 }
 
+// @LOCALMOD-START
+void
+ARMFrameLowering::getInitialFrameState(std::vector<MachineMove> &Moves) const {
+  // Initial state of the frame ARM:SP points to cfa
+  MachineLocation dst(MachineLocation::VirtualFP);
+  MachineLocation src(ARM::SP, 0);
+  Moves.push_back(MachineMove(0, dst, src));
+}
+// @LOCALMOD-END
+
 void ARMFrameLowering::emitPrologue(MachineFunction &MF) const {
   MachineBasicBlock &MBB = MF.front();
   MachineBasicBlock::iterator MBBI = MBB.begin();
@@ -139,6 +153,14 @@
   unsigned GPRCS1Size = 0, GPRCS2Size = 0, DPRCSSize = 0;
   int FramePtrSpillFI = 0;
 
+  // @LOCALMOD-START
+  MachineModuleInfo &MMI = MF.getMMI();
+  // This condition was gleaned from x86 / PowerPC / XCore
+  bool needsFrameMoves = MMI.hasDebugInfo() ||
+                         !MF.getFunction()->doesNotThrow() ||
+                         UnwindTablesMandatory;
+  // @LOCALMOD-END
+
   // Allocate the vararg register save area. This is not counted in NumBytes.
   if (VARegSaveSize)
     emitSPUpdate(isARM, MBB, MBBI, dl, TII, -VARegSaveSize);
@@ -186,6 +208,42 @@
   // Move past area 1.
   if (GPRCS1Size > 0) MBBI++;
 
+  // @LOCALMOD-START
+  if (needsFrameMoves && GPRCS1Size > 0) {
+    // we just skipped the initial callee save reg instructions, e.g.
+    // push {r4, r5, r6, lr}
+    // NOTE: this likely is not the right thing to do for darwin as it does not
+    //       treat all callee save regs uniformly
+    MCSymbol *AfterRegSave = MMI.getContext().CreateTempSymbol();
+    BuildMI(MBB, MBBI, dl, TII.get(ARM::PROLOG_LABEL)).addSym(AfterRegSave);
+    // record the fact that the stack has moved
+    MachineLocation dst(MachineLocation::VirtualFP);
+    MachineLocation src(MachineLocation::VirtualFP, -GPRCS1Size);
+    MMI.getFrameMoves().push_back(MachineMove(AfterRegSave, dst, src));
+    // for each callee saved register record where it has been saved
+    int offset = 0;
+    for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
+      unsigned Reg = CSI[i].getReg();
+      switch (Reg) {
+       case ARM::R4:
+       case ARM::R5:
+       case ARM::R6:
+       case ARM::R7:
+       case ARM::R8:
+       case ARM::R9:
+       case ARM::R10:
+       case ARM::R11:
+       case ARM::LR:
+        offset -= 4;
+        MachineLocation dst(MachineLocation::VirtualFP, offset);
+        MachineLocation src(Reg);
+        MMI.getFrameMoves().push_back(MachineMove(AfterRegSave, dst, src));
+        break;
+      }
+    }
+  }
+  // @LOCALMOD-END
+
   // Set FP to point to the stack slot that contains the previous FP.
   // For Darwin, FP is R7, which has now been stored in spill area 1.
   // Otherwise, if this is not Darwin, all the callee-saved registers go
@@ -198,8 +256,29 @@
       BuildMI(MBB, MBBI, dl, TII.get(ADDriOpc), FramePtr)
       .addFrameIndex(FramePtrSpillFI).addImm(0);
     AddDefaultCC(AddDefaultPred(MIB));
+    // @LOCALMOD-START
+    if (needsFrameMoves) {
+      // we just emitted the fp pointer setup instruction, e.g.
+      // add      r11, sp, #8
+      MCSymbol *AfterFramePointerInit = MMI.getContext().CreateTempSymbol();
+      BuildMI(MBB, MBBI, dl,
+              TII.get(ARM::PROLOG_LABEL)).addSym(AfterFramePointerInit);
+      // record the fact that the frame pointer is now tracking the "cfa"
+      // Note, gcc and llvm have a slightly different notion of where the
+      // frame pointer should be pointing. gcc points after the return address
+      // and llvm one word further down (two words = 8).
+      // This should be fine as long as we are consistent.
+      // NOTE: this is related to the offset computed for
+      // ISD::FRAME_TO_ARGS_OFFSET
+      MachineLocation dst(MachineLocation::VirtualFP);
+      MachineLocation src(FramePtr, -8);
+      MMI.getFrameMoves().push_back(MachineMove(AfterFramePointerInit, dst, src));
+    }
+    // @LOCALMOD-END
   }
 
+
+
   // Move past area 2.
   if (GPRCS2Size > 0) MBBI++;
 
@@ -231,6 +310,19 @@
       // an inconsistent state (pointing to the middle of callee-saved area).
       // The interrupt handler can end up clobbering the registers.
       AFI->setShouldRestoreSPFromFP(true);
+
+    // @LOCALMOD-START
+    // we only track sp changes if do not have the fp to figure out where
+    // stack frame lives
+    if (needsFrameMoves && !HasFP) {
+      MCSymbol *AfterStackUpdate = MMI.getContext().CreateTempSymbol();
+      BuildMI(MBB, MBBI, dl,
+              TII.get(ARM::PROLOG_LABEL)).addSym(AfterStackUpdate);
+      MachineLocation dst(MachineLocation::VirtualFP);
+      MachineLocation src(MachineLocation::VirtualFP, - NumBytes - GPRCS1Size);
+      MMI.getFrameMoves().push_back(MachineMove(AfterStackUpdate, dst, src));
+    }
+    // @LOCALMOD-END
   }
 
   if (STI.isTargetELF() && hasFP(MF))
@@ -601,7 +693,8 @@
       unsigned Reg = CSI[i-1].getReg();
       if (!(Func)(Reg, STI.isTargetDarwin())) continue;
 
-      if (Reg == ARM::LR && !isTailCall && !isVarArg && STI.hasV5TOps()) {
+      if (Reg == ARM::LR && !isTailCall && !isVarArg && STI.hasV5TOps() &&
+          false /* @LOCALMOD */) {
         Reg = ARM::PC;
         LdmOpc = AFI->isThumbFunction() ? ARM::t2LDMIA_RET : ARM::LDMIA_RET;
         // Fold the return instruction into the LDM.
diff -r 2b13dadc8fed lib/Target/ARM/ARMFrameLowering.h
--- a/lib/Target/ARM/ARMFrameLowering.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMFrameLowering.h	Thu Jun 09 18:06:40 2011 -0700
@@ -27,7 +27,8 @@
 
 public:
   explicit ARMFrameLowering(const ARMSubtarget &sti)
-    : TargetFrameLowering(StackGrowsDown, sti.getStackAlignment(), 0, 4),
+    : TargetFrameLowering(StackGrowsDown, sti.getStackAlignment(), 0, 4,
+      4), // @LOCALMOD
       STI(sti) {
   }
 
@@ -54,6 +55,7 @@
   int ResolveFrameIndexReference(const MachineFunction &MF, int FI,
                                  unsigned &FrameReg, int SPAdj) const;
   int getFrameIndexOffset(const MachineFunction &MF, int FI) const;
+  void getInitialFrameState(std::vector<MachineMove> &Moves) const; // @LOCALMOD
 
   void processFunctionBeforeCalleeSavedScan(MachineFunction &MF,
                                             RegScavenger *RS) const;
diff -r 2b13dadc8fed lib/Target/ARM/ARMISelDAGToDAG.cpp
--- a/lib/Target/ARM/ARMISelDAGToDAG.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMISelDAGToDAG.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -35,8 +35,16 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/raw_ostream.h"
 
+// @LOCALMOD-START
+#include "llvm/Support/CommandLine.h"
+namespace llvm {
+  extern cl::opt<bool> FlagSfiStore;
+}
+// @LOCALMOD-END
+
 using namespace llvm;
 
+
 static cl::opt<bool>
 DisableShifterOp("disable-shifter-op", cl::Hidden,
   cl::desc("Disable isel of shifter-op"),
@@ -97,21 +105,24 @@
   bool SelectAddrModeImm12(SDValue N, SDValue &Base, SDValue &OffImm);
   bool SelectLdStSOReg(SDValue N, SDValue &Base, SDValue &Offset, SDValue &Opc);
 
-  AddrMode2Type SelectAddrMode2Worker(SDValue N, SDValue &Base,
+  AddrMode2Type SelectAddrMode2Worker(SDNode *Op, SDValue N, SDValue &Base,
                                       SDValue &Offset, SDValue &Opc);
-  bool SelectAddrMode2Base(SDValue N, SDValue &Base, SDValue &Offset,
+  bool SelectAddrMode2Base(SDNode *Op,
+                           SDValue N, SDValue &Base, SDValue &Offset,
                            SDValue &Opc) {
-    return SelectAddrMode2Worker(N, Base, Offset, Opc) == AM2_BASE;
+    return SelectAddrMode2Worker(Op, N, Base, Offset, Opc) == AM2_BASE;
   }
 
-  bool SelectAddrMode2ShOp(SDValue N, SDValue &Base, SDValue &Offset,
+  bool SelectAddrMode2ShOp(SDNode *Op,
+                           SDValue N, SDValue &Base, SDValue &Offset,
                            SDValue &Opc) {
-    return SelectAddrMode2Worker(N, Base, Offset, Opc) == AM2_SHOP;
+    return SelectAddrMode2Worker(Op, N, Base, Offset, Opc) == AM2_SHOP;
   }
 
-  bool SelectAddrMode2(SDValue N, SDValue &Base, SDValue &Offset,
+  bool SelectAddrMode2(SDNode *Op, 
+                       SDValue N, SDValue &Base, SDValue &Offset,
                        SDValue &Opc) {
-    SelectAddrMode2Worker(N, Base, Offset, Opc);
+    SelectAddrMode2Worker(Op, N, Base, Offset, Opc);
 //    return SelectAddrMode2ShOp(N, Base, Offset, Opc);
     // This always matches one way or another.
     return true;
@@ -119,7 +130,7 @@
 
   bool SelectAddrMode2Offset(SDNode *Op, SDValue N,
                              SDValue &Offset, SDValue &Opc);
-  bool SelectAddrMode3(SDValue N, SDValue &Base,
+  bool SelectAddrMode3(SDNode *Op, SDValue N, SDValue &Base,
                        SDValue &Offset, SDValue &Opc);
   bool SelectAddrMode3Offset(SDNode *Op, SDValue N,
                              SDValue &Offset, SDValue &Opc);
@@ -421,6 +432,23 @@
   return true;
 }
 
+// @LOCALMOD-START
+static bool ShouldOperandBeUnwrappedForUseAsBaseAddress(
+  SDValue& N, const ARMSubtarget* Subtarget) {
+  assert (N.getOpcode() == ARMISD::Wrapper);
+  // Never use this transformation if constant island pools are disallowed 
+  if (FlagSfiDisableCP) return false;
+
+  // always apply this when we do not have movt/movw available
+  // (if we do have movt/movw we be able to get rid of the
+  // constant pool entry altogether)
+  if (!Subtarget->useMovt()) return true;
+  // explain why we do not want to use this for TargetGlobalAddress
+  if (N.getOperand(0).getOpcode() != ISD::TargetGlobalAddress) return true;
+  return false;
+}
+// @LOCALMOD-END
+
 bool ARMDAGToDAGISel::SelectAddrModeImm12(SDValue N,
                                           SDValue &Base,
                                           SDValue &OffImm) {
@@ -473,6 +501,9 @@
 
 bool ARMDAGToDAGISel::SelectLdStSOReg(SDValue N, SDValue &Base, SDValue &Offset,
                                       SDValue &Opc) {
+  // @LOCALMOD-BEGIN
+  return false;   // Can't handle two registers
+  // @LOCALMOD-END
   if (N.getOpcode() == ISD::MUL &&
       (!Subtarget->isCortexA9() || N.hasOneUse())) {
     if (ConstantSDNode *RHS = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
@@ -574,10 +605,21 @@
 
 //-----
 
-AddrMode2Type ARMDAGToDAGISel::SelectAddrMode2Worker(SDValue N,
+AddrMode2Type ARMDAGToDAGISel::SelectAddrMode2Worker(SDNode *Op,
+                                                     SDValue N,
                                                      SDValue &Base,
                                                      SDValue &Offset,
+// @LOCALMOD-START
+// Note: In the code below we do not want "Offfset" to be real register to
+// not violate ARM sandboxing.
+// @LOCALMOD-END
                                                      SDValue &Opc) {
+  // @LOCALMOD-START
+  // avoid two reg addressing mode for stores
+  const bool is_store = (Op->getOpcode() == ISD::STORE);
+  if (!FlagSfiStore || !is_store ) {
+  // @LOCALMOD-END
+
   if (N.getOpcode() == ISD::MUL &&
       (!Subtarget->isCortexA9() || N.hasOneUse())) {
     if (ConstantSDNode *RHS = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
@@ -601,6 +643,7 @@
       }
     }
   }
+  } // @LOCALMOD
 
   if (N.getOpcode() != ISD::ADD && N.getOpcode() != ISD::SUB &&
       // ISD::OR that is equivalent to an ADD.
@@ -610,8 +653,8 @@
       int FI = cast<FrameIndexSDNode>(N)->getIndex();
       Base = CurDAG->getTargetFrameIndex(FI, TLI.getPointerTy());
     } else if (N.getOpcode() == ARMISD::Wrapper &&
-               !(Subtarget->useMovt() &&
-                 N.getOperand(0).getOpcode() == ISD::TargetGlobalAddress)) {
+               // @LOCALMOD
+               ShouldOperandBeUnwrappedForUseAsBaseAddress(N, Subtarget)) {
       Base = N.getOperand(0);
     }
     Offset = CurDAG->getRegister(0, MVT::i32);
@@ -654,6 +697,24 @@
                                     MVT::i32);
     return AM2_BASE;
   }
+  
+  // @LOCALMOD-START
+  // keep store addressing modes simple
+  if (FlagSfiStore && is_store) {
+    Base = N;
+    if (N.getOpcode() == ISD::FrameIndex) {
+      int FI = cast<FrameIndexSDNode>(N)->getIndex();
+      Base = CurDAG->getTargetFrameIndex(FI, TLI.getPointerTy());
+    } else if (N.getOpcode() == ARMISD::Wrapper) {
+      Base = N.getOperand(0);
+    }
+    Offset = CurDAG->getRegister(0, MVT::i32);
+    Opc = CurDAG->getTargetConstant(ARM_AM::getAM2Opc(ARM_AM::add, 0,
+                                                      ARM_AM::no_shift),
+                                    MVT::i32);
+    return AM2_BASE;
+  }
+  // @LOCALMOD-END
 
   // Otherwise this is R +/- [possibly shifted] R.
   ARM_AM::AddrOpc AddSub = N.getOpcode() != ISD::SUB ? ARM_AM::add:ARM_AM::sub;
@@ -727,13 +788,19 @@
     return true;
   }
 
+  const bool is_store = (Opcode == ISD::STORE); // @LOCALMOD
+
+
   Offset = N;
   ARM_AM::ShiftOpc ShOpcVal = ARM_AM::getShiftOpcForNode(N);
   unsigned ShAmt = 0;
   if (ShOpcVal != ARM_AM::no_shift) {
     // Check to see if the RHS of the shift is a constant, if not, we can't fold
     // it.
-    if (ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+
+    //if (ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+    ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1));
+    if ((!FlagSfiStore || !is_store) && Sh ) { // @LOCALMOD
       ShAmt = Sh->getZExtValue();
       if (isShifterOpProfitable(N, ShOpcVal, ShAmt))
         Offset = N.getOperand(0);
@@ -752,16 +819,22 @@
 }
 
 
-bool ARMDAGToDAGISel::SelectAddrMode3(SDValue N,
+bool ARMDAGToDAGISel::SelectAddrMode3(SDNode *Op, SDValue N,
                                       SDValue &Base, SDValue &Offset,
                                       SDValue &Opc) {
+  // @LOCALMOD-START
+  const bool is_store = (Op->getOpcode() == ISD::STORE);
+  if (!FlagSfiStore ||!is_store) {
+  // @LOCALMOD-END
   if (N.getOpcode() == ISD::SUB) {
+
     // X - C  is canonicalize to X + -C, no need to handle it here.
     Base = N.getOperand(0);
     Offset = N.getOperand(1);
     Opc = CurDAG->getTargetConstant(ARM_AM::getAM3Opc(ARM_AM::sub, 0),MVT::i32);
     return true;
   }
+  } // @LOCALMOD-END
 
   if (!CurDAG->isBaseWithConstantOffset(N)) {
     Base = N;
@@ -794,6 +867,15 @@
     return true;
   }
 
+  // @LOCALMOD-START
+  if (FlagSfiStore && is_store) {
+    Base = N;
+    Offset = CurDAG->getRegister(0, MVT::i32);
+    Opc = CurDAG->getTargetConstant(ARM_AM::getAM3Opc(ARM_AM::add, 0),MVT::i32);
+    return true;
+  }
+  // @LOCALMOD-END
+
   Base = N.getOperand(0);
   Offset = N.getOperand(1);
   Opc = CurDAG->getTargetConstant(ARM_AM::getAM3Opc(ARM_AM::add, 0), MVT::i32);
@@ -828,8 +910,8 @@
       int FI = cast<FrameIndexSDNode>(N)->getIndex();
       Base = CurDAG->getTargetFrameIndex(FI, TLI.getPointerTy());
     } else if (N.getOpcode() == ARMISD::Wrapper &&
-               !(Subtarget->useMovt() &&
-                 N.getOperand(0).getOpcode() == ISD::TargetGlobalAddress)) {
+               // @LOCALMOD
+               ShouldOperandBeUnwrappedForUseAsBaseAddress(N, Subtarget)) {
       Base = N.getOperand(0);
     }
     Offset = CurDAG->getTargetConstant(ARM_AM::getAM5Opc(ARM_AM::add, 0),
@@ -2210,6 +2292,8 @@
                  !ARM_AM::isSOImmTwoPartVal(Val));     // two instrs.
     }
 
+    if (FlagSfiDisableCP) UseCP = false; // @LOCALMOD
+
     if (UseCP) {
       SDValue CPIdx =
         CurDAG->getTargetConstantPool(ConstantInt::get(
diff -r 2b13dadc8fed lib/Target/ARM/ARMISelLowering.cpp
--- a/lib/Target/ARM/ARMISelLowering.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMISelLowering.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -51,6 +51,15 @@
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
 #include <sstream>
+
+// @LOCALMOD-START
+#include "llvm/Support/CommandLine.h"
+namespace llvm {
+  extern cl::opt<bool> FlagSfiStore;
+  extern cl::opt<bool> FlagSfiDisableCP;
+}
+// @LOCALMOD-END
+
 using namespace llvm;
 
 STATISTIC(NumTailCalls, "Number of tail calls");
@@ -72,6 +81,12 @@
   cl::desc("Enable / disable ARM interworking (for debugging only)"),
   cl::init(true));
 
+// @LOCALMOD-START for debugging TLS models
+static cl::opt<bool> ARMStaticTLS("arm_static_tls",
+                                  cl::desc("Force a static TLS model for ARM"),
+                                  cl::init(false));
+// @LOCALMOD-END
+
 void ARMTargetLowering::addTypeForNEON(EVT VT, EVT PromotedLdStVT,
                                        EVT PromotedBitwiseVT) {
   if (VT != PromotedLdStVT) {
@@ -547,9 +562,14 @@
   setOperationAction(ISD::GLOBAL_OFFSET_TABLE, MVT::i32, Custom);
   setOperationAction(ISD::GlobalTLSAddress, MVT::i32, Custom);
   setOperationAction(ISD::BlockAddress, MVT::i32, Custom);
-
+  // @LOCALMOD-START
+  if (!Subtarget->useInlineJumpTables())
+    setOperationAction(ISD::JumpTable,     MVT::i32,   Custom);
+  // @LOCALMOD-END
+  
   setOperationAction(ISD::TRAP, MVT::Other, Legal);
 
+
   // Use the default implementation.
   setOperationAction(ISD::VASTART,            MVT::Other, Custom);
   setOperationAction(ISD::VAARG,              MVT::Other, Expand);
@@ -558,10 +578,18 @@
   setOperationAction(ISD::STACKSAVE,          MVT::Other, Expand);
   setOperationAction(ISD::STACKRESTORE,       MVT::Other, Expand);
   setOperationAction(ISD::EHSELECTION,        MVT::i32,   Expand);
-  setOperationAction(ISD::EXCEPTIONADDR,      MVT::i32,   Expand);
-  setExceptionPointerRegister(ARM::R0);
-  setExceptionSelectorRegister(ARM::R1);
-
+  // @LOCALMOD-START
+  setOperationAction(ISD::EXCEPTIONADDR, MVT::i32, Expand);
+  // we use the first caller saved regs here
+  // c.f.: llvm-gcc/llvm-gcc-4.2/gcc/unwind-dw2.c::uw_install_context
+  // NOTE: these are related to the _Unwind_PNaClSetResult{0,1} functions
+  setExceptionPointerRegister(ARM::R4);
+  setExceptionSelectorRegister(ARM::R5);
+
+  setOperationAction(ISD::FRAME_TO_ARGS_OFFSET, MVT::i32, Custom);
+
+  setOperationAction(ISD::EH_RETURN, MVT::Other, Custom);
+  // @LOCALMOD-END
   setOperationAction(ISD::DYNAMIC_STACKALLOC, MVT::i32, Expand);
   // ARMv6 Thumb1 (except for CPUs that support dmb / dsb) and earlier use
   // the default expansion.
@@ -648,8 +676,12 @@
   setOperationAction(ISD::BR_CC,     MVT::i32,   Custom);
   setOperationAction(ISD::BR_CC,     MVT::f32,   Custom);
   setOperationAction(ISD::BR_CC,     MVT::f64,   Custom);
-  setOperationAction(ISD::BR_JT,     MVT::Other, Custom);
-
+  // @LOCALMOD-START
+  //setOperationAction(ISD::BR_JT,     MVT::Other, Custom);
+  setOperationAction(ISD::BR_JT,     MVT::Other,
+                     Subtarget->useInlineJumpTables() ? Custom : Expand);
+  // @LOCALMOD-END
+  
   // We don't support sin/cos/fmod/copysign/pow
   setOperationAction(ISD::FSIN,      MVT::f64, Expand);
   setOperationAction(ISD::FSIN,      MVT::f32, Expand);
@@ -762,6 +794,10 @@
   case ARMISD::WrapperDYN:    return "ARMISD::WrapperDYN";
   case ARMISD::WrapperPIC:    return "ARMISD::WrapperPIC";
   case ARMISD::WrapperJT:     return "ARMISD::WrapperJT";
+  // @LOCALMOD-START
+  case ARMISD::WrapperJT2:    return "ARMISD::WrapperJT2"; 
+  case ARMISD::EH_RETURN:     return "ARMISD::EH_RETURN"; 
+  // @LOCALMOD-END
   case ARMISD::CALL:          return "ARMISD::CALL";
   case ARMISD::CALL_PRED:     return "ARMISD::CALL_PRED";
   case ARMISD::CALL_NOLINK:   return "ARMISD::CALL_NOLINK";
@@ -1835,7 +1871,14 @@
 }
 
 unsigned ARMTargetLowering::getJumpTableEncoding() const {
-  return MachineJumpTableInfo::EK_Inline;
+  // @LOCALMOD-BEGIN
+  if (Subtarget->useInlineJumpTables()) { 
+    return MachineJumpTableInfo::EK_Inline;
+  } else {
+    // TODO: Find a better way to call the super-class.
+    return TargetLowering::getJumpTableEncoding();
+  }
+  // @LOCALMOD-END
 }
 
 SDValue ARMTargetLowering::LowerBlockAddress(SDValue Op,
@@ -1868,28 +1911,85 @@
   return DAG.getNode(ARMISD::PIC_ADD, DL, PtrVT, Result, PICLabel);
 }
 
+// @LOCALMOD-START
+// more conventional jumptable implementation
+SDValue ARMTargetLowering::LowerJumpTable(SDValue Op, SelectionDAG &DAG) const {
+  assert(!Subtarget->useInlineJumpTables() &&
+         "inline jump tables not custom lowered");
+  const DebugLoc dl = Op.getDebugLoc();
+  EVT PTy = getPointerTy();
+  JumpTableSDNode *JT = cast<JumpTableSDNode>(Op);
+  SDValue JTI = DAG.getTargetJumpTable(JT->getIndex(), PTy);
+  return DAG.getNode(ARMISD::WrapperJT2, dl, MVT::i32, JTI);
+}
+// @LOCALMOD-END
+
 // Lower ISD::GlobalTLSAddress using the "general dynamic" model
 SDValue
 ARMTargetLowering::LowerToTLSGeneralDynamicModel(GlobalAddressSDNode *GA,
                                                  SelectionDAG &DAG) const {
   DebugLoc dl = GA->getDebugLoc();
   EVT PtrVT = getPointerTy();
-  unsigned char PCAdj = Subtarget->isThumb() ? 4 : 8;
-  MachineFunction &MF = DAG.getMachineFunction();
-  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  // @LOCALMOD-BEGIN
+  SDValue Chain;
+  SDValue Argument;
+
+  if (FlagSfiDisableCP) {
+    // With constant pools "disabled" (moved to rodata), this constant pool
+    // entry is no longer in text, and simultaneous PC relativeness
+    // and CP Addr relativeness is no longer expressible.
+    // So, instead of having:
+    //
+    // .LCPI12_0:
+    //   .long var(tlsgd)-((.LPC12_0+8) - .)
+    // ...
+    //    ldr r2, .LCPI12_0
+    // .LPC12_0:
+    //    add r0, pc, r2
+    //
+    // we have:
+    //
+    // .LCPI12_0:
+    //   .long var(tlsgd)
+    // ...
+    //    // get addr of .LCPI12_0 into r2
+    //    ldr r0, [r2]
+    //    add r0, r2, r0
+    // (1) No longer subtracting pc, so no longer adding that back
+    // (2) Not adding "." in the CP entry, so adding it via instructions.
+    //
+    unsigned char PCAdj = 0;
+    MachineFunction &MF = DAG.getMachineFunction();
+    ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+    unsigned ARMPCLabelIndex = AFI->createPICLabelUId();
+    ARMConstantPoolValue *CPV =
+        new ARMConstantPoolValue(GA->getGlobal(), ARMPCLabelIndex,
+                                 ARMCP::CPValue, PCAdj, ARMCP::TLSGD, false);
+    SDValue CPAddr = DAG.getTargetConstantPool(CPV, PtrVT, 4);
+    CPAddr = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, CPAddr);
+    Argument = DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), CPAddr,
+                                   MachinePointerInfo::getConstantPool(),
+                                   false, false, 0);
+    Chain = Argument.getValue(1);
+    Argument = DAG.getNode(ISD::ADD, dl, PtrVT, Argument, CPAddr);
+  } else { // sort of @LOCALMOD-END
+    unsigned char PCAdj = Subtarget->isThumb() ? 4 : 8;
+    MachineFunction &MF = DAG.getMachineFunction();
+    ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   unsigned ARMPCLabelIndex = AFI->createPICLabelUId();
-  ARMConstantPoolValue *CPV =
-    new ARMConstantPoolValue(GA->getGlobal(), ARMPCLabelIndex,
-                             ARMCP::CPValue, PCAdj, ARMCP::TLSGD, true);
-  SDValue Argument = DAG.getTargetConstantPool(CPV, PtrVT, 4);
-  Argument = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, Argument);
-  Argument = DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), Argument,
-                         MachinePointerInfo::getConstantPool(),
-                         false, false, 0);
-  SDValue Chain = Argument.getValue(1);
-
-  SDValue PICLabel = DAG.getConstant(ARMPCLabelIndex, MVT::i32);
-  Argument = DAG.getNode(ARMISD::PIC_ADD, dl, PtrVT, Argument, PICLabel);
+    ARMConstantPoolValue *CPV =
+        new ARMConstantPoolValue(GA->getGlobal(), ARMPCLabelIndex,
+                                 ARMCP::CPValue, PCAdj, ARMCP::TLSGD, true);
+    Argument = DAG.getTargetConstantPool(CPV, PtrVT, 4); // @ LOCALMOD
+    Argument = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, Argument);
+    Argument = DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), Argument,
+                           MachinePointerInfo::getConstantPool(),
+                           false, false, 0);
+    Chain = Argument.getValue(1); // @LOCALMOD
+
+    SDValue PICLabel = DAG.getConstant(ARMPCLabelIndex, MVT::i32);
+    Argument = DAG.getNode(ARMISD::PIC_ADD, dl, PtrVT, Argument, PICLabel);
+  } // @LOCALMOD-END
 
   // call __tls_get_addr.
   ArgListTy Args;
@@ -1910,7 +2010,8 @@
 // "local exec" model.
 SDValue
 ARMTargetLowering::LowerToTLSExecModels(GlobalAddressSDNode *GA,
-                                        SelectionDAG &DAG) const {
+                                        SelectionDAG &DAG,
+                                        bool InitialExec) const { // @LOCALMOD
   const GlobalValue *GV = GA->getGlobal();
   DebugLoc dl = GA->getDebugLoc();
   SDValue Offset;
@@ -1919,28 +2020,52 @@
   // Get the Thread Pointer
   SDValue ThreadPointer = DAG.getNode(ARMISD::THREAD_POINTER, dl, PtrVT);
 
-  if (GV->isDeclaration()) {
+  if (InitialExec) {  // @LOCALMOD
     MachineFunction &MF = DAG.getMachineFunction();
     ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
     unsigned ARMPCLabelIndex = AFI->createPICLabelUId();
-    // Initial exec model.
-    unsigned char PCAdj = Subtarget->isThumb() ? 4 : 8;
-    ARMConstantPoolValue *CPV =
-      new ARMConstantPoolValue(GA->getGlobal(), ARMPCLabelIndex,
-                               ARMCP::CPValue, PCAdj, ARMCP::GOTTPOFF, true);
-    Offset = DAG.getTargetConstantPool(CPV, PtrVT, 4);
-    Offset = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, Offset);
-    Offset = DAG.getLoad(PtrVT, dl, Chain, Offset,
-                         MachinePointerInfo::getConstantPool(),
-                         false, false, 0);
-    Chain = Offset.getValue(1);
-
-    SDValue PICLabel = DAG.getConstant(ARMPCLabelIndex, MVT::i32);
-    Offset = DAG.getNode(ARMISD::PIC_ADD, dl, PtrVT, Offset, PICLabel);
-
-    Offset = DAG.getLoad(PtrVT, dl, Chain, Offset,
-                         MachinePointerInfo::getConstantPool(),
-                         false, false, 0);
+
+    // @LOCALMOD-BEGIN
+    if (FlagSfiDisableCP) {
+      // Similar to change to LowerToTLSGeneralDynamicModel, and
+      // for the same reason.
+      unsigned char PCAdj = 0;
+      ARMConstantPoolValue *CPV =
+          new ARMConstantPoolValue(GA->getGlobal(), ARMPCLabelIndex,
+                                   ARMCP::CPValue, PCAdj, ARMCP::GOTTPOFF,
+                                   false);
+      SDValue CPAddr = DAG.getTargetConstantPool(CPV, PtrVT, 4);
+      CPAddr = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, CPAddr);
+      Offset = DAG.getLoad(PtrVT, dl, Chain, CPAddr,
+                           MachinePointerInfo::getConstantPool(),
+                           false, false, 0);
+      Chain = Offset.getValue(1);
+
+      Offset = DAG.getNode(ISD::ADD, dl, PtrVT, Offset, CPAddr);
+
+      Offset = DAG.getLoad(PtrVT, dl, Chain, Offset,
+                           MachinePointerInfo::getConstantPool(),
+                           false, false, 0);
+    } else { // sort of @LOCALMOD-END (indentation)
+      // Initial exec model.
+      unsigned char PCAdj = Subtarget->isThumb() ? 4 : 8;
+      ARMConstantPoolValue *CPV =
+          new ARMConstantPoolValue(GA->getGlobal(), ARMPCLabelIndex,
+                                   ARMCP::CPValue, PCAdj, ARMCP::GOTTPOFF, true);
+      Offset = DAG.getTargetConstantPool(CPV, PtrVT, 4);
+      Offset = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, Offset);
+      Offset = DAG.getLoad(PtrVT, dl, Chain, Offset,
+                           MachinePointerInfo::getConstantPool(),
+                           false, false, 0);
+      Chain = Offset.getValue(1);
+
+      SDValue PICLabel = DAG.getConstant(ARMPCLabelIndex, MVT::i32);
+      Offset = DAG.getNode(ARMISD::PIC_ADD, dl, PtrVT, Offset, PICLabel);
+
+      Offset = DAG.getLoad(PtrVT, dl, Chain, Offset,
+                           MachinePointerInfo::getConstantPool(),
+                           false, false, 0);
+    } // @LOCALMOD-END
   } else {
     // local exec model
     ARMConstantPoolValue *CPV = new ARMConstantPoolValue(GV, ARMCP::TPOFF);
@@ -1964,10 +2089,20 @@
   GlobalAddressSDNode *GA = cast<GlobalAddressSDNode>(Op);
   // If the relocation model is PIC, use the "General Dynamic" TLS Model,
   // otherwise use the "Local Exec" TLS Model
-  if (getTargetMachine().getRelocationModel() == Reloc::PIC_)
-    return LowerToTLSGeneralDynamicModel(GA, DAG);
-  else
-    return LowerToTLSExecModels(GA, DAG);
+  // @LOCALMOD-BEGIN NaCl is testing with Initial Exec for now.
+  // This supports DSOs that are known at startup, but not those
+  // that may or may not be loaded through dlopen.
+  // Must wait for ARM Glibc port.
+  if (getTargetMachine().getRelocationModel() == Reloc::PIC_) {
+    if (ARMStaticTLS)
+      return LowerToTLSExecModels(GA, DAG, true);
+    else
+      return LowerToTLSGeneralDynamicModel(GA, DAG);
+  } else {
+    const GlobalValue *GV = GA->getGlobal();
+    return LowerToTLSExecModels(GA, DAG, GV->isDeclaration());
+  } //@LOCALMOD-END
+
 }
 
 SDValue ARMTargetLowering::LowerGlobalAddressELF(SDValue Op,
@@ -2078,17 +2213,55 @@
   unsigned ARMPCLabelIndex = AFI->createPICLabelUId();
   EVT PtrVT = getPointerTy();
   DebugLoc dl = Op.getDebugLoc();
-  unsigned PCAdj = Subtarget->isThumb() ? 4 : 8;
-  ARMConstantPoolValue *CPV = new ARMConstantPoolValue(*DAG.getContext(),
-                                                       "_GLOBAL_OFFSET_TABLE_",
-                                                       ARMPCLabelIndex, PCAdj);
-  SDValue CPAddr = DAG.getTargetConstantPool(CPV, PtrVT, 4);
-  CPAddr = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, CPAddr);
-  SDValue Result = DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), CPAddr,
-                               MachinePointerInfo::getConstantPool(),
-                               false, false, 0);
-  SDValue PICLabel = DAG.getConstant(ARMPCLabelIndex, MVT::i32);
-  return DAG.getNode(ARMISD::PIC_ADD, dl, PtrVT, Result, PICLabel);
+
+  // @LOCALMOD-BEGIN
+  if (FlagSfiDisableCP) {
+    // With constant pools "disabled" (moved to rodata), the constant pool
+    // entry is no longer in text, and the PC relativeness is
+    // no longer expressible.
+    //
+    // Instead of having:
+    //
+    // .LCPI12_0:
+    //   .long _GLOBAL_OFFSET_TABLE_-(.LPC12_0+8)
+    // ...
+    //    ldr r2, .LCPI12_0
+    // .LPC12_0:
+    //    add r0, pc, r2
+    //
+    // Things to try:
+    // (1) get the address of the GOT through a pc-relative MOVW / MOVT.
+    //
+    //    movw r0, :lower16:_GLOBAL_OFFSET_TABLE_ - (.LPC12_0 + 8)
+    //    movt r0, :upper16:_GLOBAL_OFFSET_TABLE_ - (.LPC12_0 + 8)
+    // .LPC12_0:
+    //    add r0, pc, r0
+    //
+    // (2) Make the constant pool entry relative to its own location
+    //
+    // .LCPI12_0:
+    //   .long _GLOBAL_OFFSET_TABLE_-.
+    // ...
+    //    // get address of LCPI12_0 into r0 (possibly 3 instructions for PIC)
+    //    ldr r1, [r0]
+    //    add r1, r0, r1
+    //
+    // We will try (1) for now, since (2) takes about 3 more instructions
+    // (and one of them is a load).
+    return DAG.getNode(ARMISD::WrapperGOT, dl, MVT::i32);
+  } else { // Sort of LOCALMOD-END (indentation only
+    unsigned PCAdj = Subtarget->isThumb() ? 4 : 8;
+    ARMConstantPoolValue *CPV = new ARMConstantPoolValue(*DAG.getContext(),
+                                                         "_GLOBAL_OFFSET_TABLE_",
+                                                         ARMPCLabelIndex, PCAdj);
+    SDValue CPAddr = DAG.getTargetConstantPool(CPV, PtrVT, 4);
+    CPAddr = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, CPAddr);
+    SDValue Result = DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), CPAddr,
+                                 MachinePointerInfo::getConstantPool(),
+                                 false, false, 0);
+    SDValue PICLabel = DAG.getConstant(ARMPCLabelIndex, MVT::i32);
+    return DAG.getNode(ARMISD::PIC_ADD, dl, PtrVT, Result, PICLabel);
+  } // @LOCALMOD-END
 }
 
 SDValue
@@ -4465,12 +4638,33 @@
   return N0;
 }
 
+// @LOCALMOD-START
+// An EH_RETURN is the result of lowering llvm.eh.return.i32 which in turn is
+// generated from __builtin_eh_return (offset, handler)
+// The effect of this is to adjust the stack pointer by "offset"
+// and then branch to "handler".
+SDValue ARMTargetLowering::LowerEH_RETURN(SDValue Op, SelectionDAG &DAG)
+  const {
+  SDValue Chain     = Op.getOperand(0);
+  SDValue Offset    = Op.getOperand(1);
+  SDValue Handler   = Op.getOperand(2);
+  DebugLoc dl       = Op.getDebugLoc();
+
+  return DAG.getNode(ARMISD::EH_RETURN, dl,
+                     MVT::Other,
+                     Chain,
+                     Offset,
+                     Handler);
+}
+// @LOCALMOD-END
+
 SDValue ARMTargetLowering::LowerOperation(SDValue Op, SelectionDAG &DAG) const {
   switch (Op.getOpcode()) {
   default: llvm_unreachable("Don't know how to custom lower this!");
   case ISD::ConstantPool:  return LowerConstantPool(Op, DAG);
   case ISD::BlockAddress:  return LowerBlockAddress(Op, DAG);
-  case ISD::GlobalAddress:
+  case ISD::JumpTable:    return LowerJumpTable(Op, DAG); // @LOCALMOD
+   case ISD::GlobalAddress:
     return Subtarget->isTargetDarwin() ? LowerGlobalAddressDarwin(Op, DAG) :
       LowerGlobalAddressELF(Op, DAG);
   case ISD::GlobalTLSAddress:   return LowerGlobalTLSAddress(Op, DAG);
@@ -4488,6 +4682,17 @@
   case ISD::FCOPYSIGN:     return LowerFCOPYSIGN(Op, DAG);
   case ISD::RETURNADDR:    return LowerRETURNADDR(Op, DAG);
   case ISD::FRAMEADDR:     return LowerFRAMEADDR(Op, DAG);
+  // @LOCALMOD-START
+  // The exact semantics of this ISD are not completely clear.
+  // LLVM seems to always point the fp after the push ra and the old fp, i.e.
+  // two register slots after the beginning of the stack frame.
+  // It is not clear what happens when there is no frame pointer but
+  // but llvm unlike gcc seems to always force one when this node is
+  // encountered.
+  case ISD::FRAME_TO_ARGS_OFFSET: return DAG.getIntPtrConstant(2*4);
+  case ISD::EH_RETURN:            return LowerEH_RETURN(Op, DAG);
+  // @LOCALMOD-END
+   
   case ISD::GLOBAL_OFFSET_TABLE: return LowerGLOBAL_OFFSET_TABLE(Op, DAG);
   case ISD::EH_SJLJ_SETJMP: return LowerEH_SJLJ_SETJMP(Op, DAG);
   case ISD::EH_SJLJ_LONGJMP: return LowerEH_SJLJ_LONGJMP(Op, DAG);
@@ -6434,6 +6639,13 @@
   if (Subtarget->isThumb1Only())
     return false;
 
+  // @LOCAMOD-START
+  // NOTE: THIS IS A LITTLE DRASTIC
+  if (FlagSfiStore && N->getOpcode() == ISD::STORE) {
+    return false;
+  }
+  // @LOCAMOD-END
+
   EVT VT;
   SDValue Ptr;
   bool isSEXTLoad = false;
@@ -6472,7 +6684,12 @@
                                                    SelectionDAG &DAG) const {
   if (Subtarget->isThumb1Only())
     return false;
-
+   // @LOCALMOD-START
+  // THIS IS A LITTLE DRASTIC
+  if (FlagSfiStore && N->getOpcode() == ISD::STORE) {
+    return false;
+  }
+  // @LOCALMOD-END
   EVT VT;
   SDValue Ptr;
   bool isSEXTLoad = false;
diff -r 2b13dadc8fed lib/Target/ARM/ARMISelLowering.h
--- a/lib/Target/ARM/ARMISelLowering.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMISelLowering.h	Thu Jun 09 18:06:40 2011 -0700
@@ -39,7 +39,11 @@
       WrapperPIC,   // WrapperPIC - A wrapper node for TargetGlobalAddress in
                     // PIC mode.
       WrapperJT,    // WrapperJT - A wrapper node for TargetJumpTable
-
+      // @LOCALMOD-START
+      WrapperJT2,   // like WrapperJT but without the UID
+      WrapperGOT,   // A Wrapper node for GOT addresses
+      EH_RETURN,    // For LowerEH_RETURN
+      // @LOCALMOD-START
       CALL,         // Function call.
       CALL_PRED,    // Function call that's predicable.
       CALL_NOLINK,  // Function call with branch not branch-and-link.
@@ -394,8 +398,14 @@
     SDValue LowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const;
     SDValue LowerToTLSGeneralDynamicModel(GlobalAddressSDNode *GA,
                                             SelectionDAG &DAG) const;
+    // @LOCALMOD-START
     SDValue LowerToTLSExecModels(GlobalAddressSDNode *GA,
-                                   SelectionDAG &DAG) const;
+                                 SelectionDAG &DAG,
+                                 bool InitialExec) const;
+    SDValue LowerJumpTable(SDValue Op, SelectionDAG &DAG) const;
+    SDValue LowerEH_RETURN(SDValue Op, SelectionDAG &DAG) const;
+    // @LOCALMOD-END
+
     SDValue LowerGLOBAL_OFFSET_TABLE(SDValue Op, SelectionDAG &DAG) const;
     SDValue LowerBR_JT(SDValue Op, SelectionDAG &DAG) const;
     SDValue LowerSELECT(SDValue Op, SelectionDAG &DAG) const;
diff -r 2b13dadc8fed lib/Target/ARM/ARMInstrInfo.td
--- a/lib/Target/ARM/ARMInstrInfo.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMInstrInfo.td	Thu Jun 09 18:06:40 2011 -0700
@@ -72,6 +72,14 @@
 def ARMWrapperDYN    : SDNode<"ARMISD::WrapperDYN",  SDTIntUnaryOp>;
 def ARMWrapperPIC    : SDNode<"ARMISD::WrapperPIC",  SDTIntUnaryOp>;
 def ARMWrapperJT     : SDNode<"ARMISD::WrapperJT",   SDTIntBinOp>;
+// @LOCALMOD-START
+// support non-inline jumptables
+// we do not use the extre uid immediate that comes with ARMWrapperJT
+// TODO(robertm): figure out what it is used for
+def ARMWrapperJT2    : SDNode<"ARMISD::WrapperJT2",  SDTIntUnaryOp>;
+// Support for MOVW/MOVT'ing the GOT address directly into a register.
+def ARMWrapperGOT       : SDNode<"ARMISD::WrapperGOT",  SDTPtrLeaf>;
+// @LOCALMOD-END
 
 def ARMcallseq_start : SDNode<"ISD::CALLSEQ_START", SDT_ARMCallSeqStart,
                               [SDNPHasChain, SDNPOutGlue]>;
@@ -181,6 +189,11 @@
 def DontUseMovt      : Predicate<"!Subtarget->useMovt()">;
 def UseFPVMLx        : Predicate<"Subtarget->useFPVMLx()">;
 
+// @LOCALMOD-BEGIN
+def UseConstPool : Predicate<"Subtarget->useConstPool()">;
+def DontUseConstPool : Predicate<"!Subtarget->useConstPool()">;
+// @LOCALMOD-END
+
 //===----------------------------------------------------------------------===//
 // ARM Flag Definitions.
 
@@ -498,7 +511,8 @@
 //           := reg +/- reg shop imm
 //
 def addrmode2 : Operand<i32>,
-                ComplexPattern<i32, 3, "SelectAddrMode2", []> {
+                ComplexPattern<i32, 3, "SelectAddrMode2", [],
+                               [SDNPWantRoot]> { // @LOCALMOD
   let EncoderMethod = "getAddrMode2OpValue";
   let PrintMethod = "printAddrMode2Operand";
   let MIOperandInfo = (ops GPR:$base, GPR:$offsreg, i32imm:$offsimm);
@@ -516,7 +530,8 @@
 // addrmode3 := reg +/- imm8
 //
 def addrmode3 : Operand<i32>,
-                ComplexPattern<i32, 3, "SelectAddrMode3", []> {
+                ComplexPattern<i32, 3, "SelectAddrMode3", [],
+                               [SDNPWantRoot]> { // @LOCALMOD
   let EncoderMethod = "getAddrMode3OpValue";
   let PrintMethod = "printAddrMode3Operand";
   let MIOperandInfo = (ops GPR:$base, GPR:$offsreg, i32imm:$offsimm);
@@ -1009,6 +1024,37 @@
 // Instructions
 //===----------------------------------------------------------------------===//
 
+// @LOCALMOD-START
+def SFI_GUARD_STORE :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a, pred:$p), NoItinerary, []>;
+
+let Defs = [CPSR] in
+def SFI_GUARD_STORE_TST :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a), NoItinerary, []>;
+
+def SFI_GUARD_INDIRECT_CALL :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a, pred:$p), NoItinerary, []>;
+
+def SFI_GUARD_INDIRECT_JMP :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a, pred:$p), NoItinerary, []>;
+
+def SFI_GUARD_CALL :
+PseudoInst<(outs), (ins pred:$p), NoItinerary, []>;
+
+// NOTE: the BX_RET instruction hardcodes lr as well
+def SFI_GUARD_RETURN :
+PseudoInst<(outs), (ins pred:$p), NoItinerary, []>;
+
+def SFI_NOP_IF_AT_BUNDLE_END :
+PseudoInst<(outs), (ins), NoItinerary, []>;
+
+// Note: intention is that $src and $dst are the same register.
+def SFI_DATA_MASK :
+PseudoInst<(outs GPR:$dst), (ins GPR:$src, pred:$p), NoItinerary, []>;
+
+// @LOCALMOD-END
+
+
 //===----------------------------------------------------------------------===//
 //  Miscellaneous Instructions.
 //
@@ -1304,9 +1350,43 @@
                     Requires<[IsARM, NoV4T]>;
 }
 
+// @LOCALMOD-START
+// Exception handling related Node and Instructions.
+// The conversion sequence is:
+// ISD::EH_RETURN -> ARMISD::EH_RETURN ->  
+// ARMeh_return -> (stack change + indirect branch)
+// 
+// Note: the ARMeh_return takes the place of regular return instruction
+// but takes two arguments.
+// At one point we saw a problem: where the registers consumed by 
+// the ARMeh_return where invalidated by the epilog code which
+// is injected immediately in front of it.
+// This problem is currently irreproducible but should it ever come back
+// the symptoms are easily recognized: all toolchain eh_* tests will fail.
+// The fix in that case would be to either hardcode result registers
+// (r0, r1) into  ARMISD::EH_RETURN or fix whatever bug may lurk in the epilog
+// code emission.
+def SDT_ARMEHRET : SDTypeProfile<0, 2, [SDTCisInt<0>, SDTCisPtrTy<1>]>;
+
+def ARMehret : SDNode<"ARMISD::EH_RETURN", SDT_ARMEHRET,
+                      [SDNPHasChain, SDNPOptInGlue]>;
+
+
+let isTerminator = 1, isReturn = 1, isBarrier = 1,
+  Defs = [SP],
+  Uses = [SP] in {
+    def ARMeh_return : PseudoInst<(outs), 
+                               (ins GPR:$spadj, GPR:$dst),
+                               IIC_Br,
+                               [(ARMehret GPR:$spadj, GPR:$dst)]>,
+                    Requires<[IsARM]>; 
+}
+// @LOCALMOD-END
+
 // All calls clobber the non-callee saved registers. SP is marked as
 // a use to prevent stack-pointer assignments that appear immediately
 // before calls from potentially appearing dead.
+
 let isCall = 1,
   // On non-Darwin platforms R9 is callee-saved.
   Defs = [R0,  R1,  R2,  R3,  R12, LR,
@@ -2012,6 +2092,68 @@
 
 } // Constraints
 
+// @LOCALMOD-BEGIN
+// PIC / PC-relative versions of MOVi16/MOVTi16, which have an extra 
+// operand representing the ID of the PICADD instruction that corrects
+// for relativity. This is used to materialize addresses into
+// a register in a PC-relative manner.
+// 
+// E.g. Rather than have an absolute address in $imm, and transferred to 
+// a register with:
+//    movw $Rd, :lower16:$imm
+//    movt $Rd, :upper16:$imm
+//
+// we will instead have a relative offset:
+//    movw $Rd, :lower16:$imm - ($pic_add_id + 8)
+//    ...
+//    movt $Rd, :upper16:$imm - ($pic_add_id + 8)
+//    ...
+// $pic_add_id:
+//    add  $Rd, pc, $Rd
+//
+// One way these pseudo instructions (and the corresponding PICADD) 
+// come about is during expansion of the MOVi32imm pseudo instruction
+// (see ARMExpandPseudo::ExpandMBB).
+// These pseudo instructions become real instructions when they are
+// finally lowered to MCInsts (e.g., at ARMAsmPrinter::EmitInstruction),
+// and the extra pclabel ID becomes part of the appropriate operand.
+//
+// NOTE: aside from adding the pclabel operand, all other operands should
+// be the same as the non-PIC versions to simplify conversion to the 
+// non-pseudo instructions.
+let isReMaterializable = 1, isAsCheapAsAMove = 1, isMoveImm = 1 in
+def MOVi16PIC : PseudoInst<(outs GPR:$Rd), (ins i32imm_hilo16:$imm,
+                                                pclabel:$pic_add_id,
+                                                pred:$p),
+                           IIC_iMOVi,
+                           []>,
+                 Requires<[IsARM, HasV6T2]>, UnaryDP;
+
+let Constraints = "$src = $Rd" in
+def MOVTi16PIC : PseudoInst<(outs GPR:$Rd), (ins GPR:$src,
+                                                 i32imm_hilo16:$imm,
+                                                 pclabel:$pic_add_id,
+                                                 pred:$p),
+                            IIC_iMOVi,
+                            []>,
+                 UnaryDP, Requires<[IsARM, HasV6T2]>;
+// @LOCALMOD-END
+
+// @LOCALMOD-BEGIN
+// Pseudo-instruction that will be expanded into MOVW / MOVT (PIC versions) w/
+// GOT as the operand. 
+// The alternative is to create a constant pool entry with the (relative)
+// GOT address and load from the constant pool. This is currently used
+// when constant islands are turned off, since MOVW / MOVT will be faster.
+let isReMaterializable = 1, isMoveImm = 1 in
+def MOVGOTAddr : PseudoInst<(outs GPR:$dst), (ins),
+                           IIC_iMOVix2, // will expand to two MOVi's
+                           []>,
+                           Requires<[IsARM, UseMovt]>;
+
+def : ARMPat<(ARMWrapperGOT), (MOVGOTAddr)>;
+// @LOCALMOD-END
+
 def : ARMPat<(or GPR:$src, 0xffff0000), (MOVTi16 GPR:$src, 0xffff)>,
       Requires<[IsARM, HasV6T2]>;
 
@@ -3482,9 +3624,20 @@
 // ConstantPool, GlobalAddress, and JumpTable
 def : ARMPat<(ARMWrapper  tglobaladdr :$dst), (LEApcrel tglobaladdr :$dst)>,
             Requires<[IsARM, DontUseMovt]>;
-def : ARMPat<(ARMWrapper  tconstpool  :$dst), (LEApcrel tconstpool  :$dst)>;
+// @LOCALMOD-START
+def : ARMPat<(ARMWrapper  tconstpool  :$dst), (LEApcrel tconstpool  :$dst)>,
+            Requires<[IsARM, DontUseMovt]>;
+// @LOCALMOD-END
 def : ARMPat<(ARMWrapper  tglobaladdr :$dst), (MOVi32imm tglobaladdr :$dst)>,
             Requires<[IsARM, UseMovt]>;
+// @LOCALMOD-START
+def : ARMPat<(ARMWrapper  tconstpool :$dst), (MOVi32imm tconstpool :$dst)>,
+            Requires<[IsARM, UseMovt, DontUseConstPool]>;
+def : ARMPat<(ARMWrapper  tconstpool :$dst), (LEApcrel tconstpool :$dst)>,
+            Requires<[IsARM, UseMovt, UseConstPool]>;
+def : ARMPat<(ARMWrapperJT2  tjumptable :$dst), (MOVi32imm tjumptable :$dst)>,
+            Requires<[IsARM, UseMovt]>;
+// @LOCALMOD-END
 def : ARMPat<(ARMWrapperJT tjumptable:$dst, imm:$id),
              (LEApcrelJT tjumptable:$dst, imm:$id)>;
 
diff -r 2b13dadc8fed lib/Target/ARM/ARMLoadStoreOptimizer.cpp
--- a/lib/Target/ARM/ARMLoadStoreOptimizer.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMLoadStoreOptimizer.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -39,6 +39,9 @@
 #include "llvm/ADT/Statistic.h"
 using namespace llvm;
 
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
+extern cl::opt<bool> FlagSfiStore; // @LOCALMOD
+
 STATISTIC(NumLDMGened , "Number of ldm instructions generated");
 STATISTIC(NumSTMGened , "Number of stm instructions generated");
 STATISTIC(NumVLDMGened, "Number of vldm instructions generated");
@@ -672,6 +675,7 @@
 /// ldmia rn, <ra, rb, rc>
 /// =>
 /// ldmdb rn!, <ra, rb, rc>
+/// @LOCALMOD This is especially useful for rn == sp
 bool ARMLoadStoreOpt::MergeBaseUpdateLSMultiple(MachineBasicBlock &MBB,
                                                MachineBasicBlock::iterator MBBI,
                                                bool &Advance,
@@ -1331,7 +1335,12 @@
 ///   mov pc, lr
 /// =>
 ///   ldmfd sp!, {..., pc}
+// @LOCALMOD for sfi we do not want this to happen
 bool ARMLoadStoreOpt::MergeReturnIntoLDM(MachineBasicBlock &MBB) {
+// @LOCALMOD-START
+  return false;
+  // @LOCALMOD-END
+
   if (MBB.empty()) return false;
 
   MachineBasicBlock::iterator MBBI = MBB.getLastNonDebugInstr();
diff -r 2b13dadc8fed lib/Target/ARM/ARMMCAsmInfo.cpp
--- a/lib/Target/ARM/ARMMCAsmInfo.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMMCAsmInfo.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -65,4 +65,9 @@
   DwarfRequiresFrameSection = false;
 
   SupportsDebugInformation = true;
+
+  // @LOCALMOD-START
+  // Exceptions handling
+  ExceptionsType = ExceptionHandling::DwarfTable;
+  // @LOCALMOD-END
 }
diff -r 2b13dadc8fed lib/Target/ARM/ARMMCInstLower.cpp
--- a/lib/Target/ARM/ARMMCInstLower.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMMCInstLower.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -60,21 +60,20 @@
 
 }
 
-void llvm::LowerARMMachineInstrToMCInst(const MachineInstr *MI, MCInst &OutMI,
-                                        ARMAsmPrinter &AP) {
-  OutMI.setOpcode(MI->getOpcode());
-
-  for (unsigned i = 0, e = MI->getNumOperands(); i != e; ++i) {
-    const MachineOperand &MO = MI->getOperand(i);
-
-    MCOperand MCOp;
-    switch (MO.getType()) {
+// @LOCALMOD-BEGIN essentially the loop body of LowerARMMachineOperandToMCInst.
+// Returns true if the Operand was really converted to an MCOperand.
+static bool LowerARMMachineOperandToMCOperand(const MachineOperand &MO,
+                                               const MachineInstr *MI,
+                                               ARMAsmPrinter &AP,
+                                               MCOperand &OutOp) {
+  MCOperand MCOp;
+  switch (MO.getType()) {
     default:
       MI->dump();
       assert(0 && "unknown operand type");
     case MachineOperand::MO_Register:
       // Ignore all non-CPSR implicit register operands.
-      if (MO.isImplicit() && MO.getReg() != ARM::CPSR) continue;
+      if (MO.isImplicit() && MO.getReg() != ARM::CPSR) return false;
       assert(!MO.getSubReg() && "Subregs should be eliminated!");
       MCOp = MCOperand::CreateReg(MO.getReg());
       break;
@@ -83,7 +82,7 @@
       break;
     case MachineOperand::MO_MachineBasicBlock:
       MCOp = MCOperand::CreateExpr(MCSymbolRefExpr::Create(
-                       MO.getMBB()->getSymbol(), AP.OutContext));
+          MO.getMBB()->getSymbol(), AP.OutContext));
       break;
     case MachineOperand::MO_GlobalAddress:
       MCOp = GetSymbolRef(MO, AP.Mang->getSymbol(MO.getGlobal()), AP);
@@ -108,8 +107,80 @@
       MCOp = MCOperand::CreateFPImm(Val.convertToDouble());
       break;
     }
+  }
+  OutOp = MCOp;
+  return true;
+
+}
+// @LOCALMOD-END
+
+
+void llvm::LowerARMMachineInstrToMCInst(const MachineInstr *MI, MCInst &OutMI,
+                                        ARMAsmPrinter &AP) {
+  OutMI.setOpcode(MI->getOpcode());
+
+  for (unsigned i = 0, e = MI->getNumOperands(); i != e; ++i) {
+    const MachineOperand &MO = MI->getOperand(i);
+
+    MCOperand MCOp;
+    // @LOCALMOD-BEGIN (code moved to LowerARMMachineOperandToMCOperand)
+    if (LowerARMMachineOperandToMCOperand(MO, MI, AP, MCOp)) {
+      OutMI.addOperand(MCOp);
     }
-
-    OutMI.addOperand(MCOp);
+    // @LOCALMOD-END
   }
 }
+
+// @LOCALMOD-BEGIN
+// Unlike LowerARMMachineInstrToMCInst, the opcode has already been set.
+// Otherwise, this is like LowerARMMachineInstrToMCInst, but with special
+// handling where the "immediate" is PC Relative
+// (used for MOVi16PIC / MOVTi16PIC, etc. -- see .td file)
+void llvm::LowerARMMachineInstrToMCInstPCRel(const MachineInstr *MI,
+                                             MCInst &OutMI,
+                                             ARMAsmPrinter &AP,
+                                             unsigned ImmIndex,
+                                             unsigned PCIndex,
+                                             MCSymbol *PCLabel,
+                                             unsigned PCAdjustment) {
+
+  for (unsigned i = 0, e = MI->getNumOperands(); i != e; ++i) {
+    if (i == ImmIndex) {
+      MCContext &Ctx = AP.OutContext;
+      const MCExpr *PCRelExpr = MCSymbolRefExpr::Create(PCLabel, Ctx);
+      if (PCAdjustment) {
+        const MCExpr *AdjExpr = MCConstantExpr::Create(PCAdjustment, Ctx);
+        PCRelExpr = MCBinaryExpr::CreateAdd(PCRelExpr, AdjExpr, Ctx);
+      }
+
+      // Get the usual symbol operand, then subtract the PCRelExpr.
+      const MachineOperand &MOImm = MI->getOperand(ImmIndex);
+      MCOperand SymOp;
+      bool DidLower = LowerARMMachineOperandToMCOperand(MOImm, MI, AP, SymOp);
+      assert (DidLower && "Immediate-like operand should have been lowered");
+
+      const MCExpr *Expr = SymOp.getExpr();
+      ARMMCExpr::VariantKind TargetKind = ARMMCExpr::VK_ARM_None;
+      /* Unwrap and rewrap the ARMMCExpr */
+      if (Expr->getKind() == MCExpr::Target) {
+        const ARMMCExpr *TargetExpr = cast<ARMMCExpr>(Expr);
+        TargetKind = TargetExpr->getKind();
+        Expr = TargetExpr->getSubExpr();
+      }
+      Expr = MCBinaryExpr::CreateSub(Expr, PCRelExpr, Ctx);
+      if (TargetKind != ARMMCExpr::VK_ARM_None) {
+        Expr = ARMMCExpr::Create(TargetKind, Expr, Ctx);
+      }
+      MCOperand MCOp = MCOperand::CreateExpr(Expr);
+      OutMI.addOperand(MCOp);
+    } else if (i == PCIndex) {  // dummy index already handled as PCLabel
+      continue;
+    } else {
+      MCOperand MCOp;
+      if (LowerARMMachineOperandToMCOperand(MI->getOperand(i), MI, AP, MCOp)) {
+        OutMI.addOperand(MCOp);
+      }
+    }
+  }
+}
+// @LOCALMOD-END
diff -r 2b13dadc8fed lib/Target/ARM/ARMNaClHeaders.cpp
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/lib/Target/ARM/ARMNaClHeaders.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,183 @@
+//===-- ARMNaClHeaders.cpp - Print SFI headers to an ARM .s file -----------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the initial header string needed
+// for the Native Client target in ARM assembly.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Support/raw_ostream.h"
+#include "ARMNaClRewritePass.h"
+#include <string>
+
+using namespace llvm;
+
+void EmitSFIHeaders(raw_ostream &O) {
+  O << " @ ========================================\n";
+  O << "@ Branch: " << FlagSfiBranch << "\n";
+  O << "@ Stack: " << FlagSfiStack << "\n";
+  O << "@ Store: " << FlagSfiStore << "\n";
+  O << "@ Data: " << FlagSfiData << "\n";
+
+  O << " @ ========================================\n";
+  // NOTE: this macro does bundle alignment as follows
+  //       if current bundle pos is X emit pX data items of value "val"
+  // NOTE: that pos will be one of: 0,4,8,12
+  //
+  O <<
+    "\t.macro sfi_long_based_on_pos p0 p1 p2 p3 val\n"
+    "\t.set pos, (. - XmagicX) % 16\n"
+    "\t.fill  (((\\p3<<12)|(\\p2<<8)|(\\p1<<4)|\\p0)>>pos) & 15, 4, \\val\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_illegal_if_at_bundle_begining\n"
+    "\tsfi_long_based_on_pos 1 0 0 0 0xe1277777\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nop_if_at_bundle_end\n"
+    "\tsfi_long_based_on_pos 0 0 0 1 0xe1a00000\n"
+    "\t.endm\n"
+      "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot3\n"
+    "\tsfi_long_based_on_pos 3 2 1 0 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot2\n"
+    "\tsfi_long_based_on_pos 2 1 0 3 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot1\n"
+    "\tsfi_long_based_on_pos 1 0 3 2 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O << " @ ========================================\n";
+  if (FlagSfiZeroMask) {
+    O <<
+      "\t.macro sfi_data_mask reg cond\n"
+      "\tbic\\cond \\reg, \\reg, #0\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_code_mask reg cond=\n"
+      "\tbic\\cond \\reg, \\reg, #0\n"
+      "\t.endm\n"
+      "\n\n";
+
+  } else {
+    O <<
+      "\t.macro sfi_data_mask reg cond\n"
+      "\tbic\\cond \\reg, \\reg, #0xc0000000\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_data_tst reg\n"
+      "\ttst \\reg, #0xc0000000\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_code_mask reg cond=\n"
+      "\tbic\\cond \\reg, \\reg, #0xc000000f\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+  O << " @ ========================================\n";
+  if (FlagSfiBranch) {
+    O <<
+      "\t.macro sfi_call_preamble cond=\n"
+      "\tsfi_nops_to_force_slot3\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_return_preamble reg cond=\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_code_mask \\reg \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+    
+    // This is used just before "bx rx"
+    O <<
+      "\t.macro sfi_indirect_jump_preamble link cond=\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_code_mask \\link \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    // This is use just before "blx rx"
+    O <<
+      "\t.macro sfi_indirect_call_preamble link cond=\n"
+      "\tsfi_nops_to_force_slot2\n"
+      "\tsfi_code_mask \\link \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+  }
+
+  if (FlagSfiStore) {
+    O << " @ ========================================\n";
+
+    O <<
+      "\t.macro sfi_store_preamble reg cond\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_data_mask \\reg, \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_cstore_preamble reg\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_data_tst \\reg\n"
+      "\t.endm\n"
+      "\n\n";
+  } else {
+    O <<
+      "\t.macro sfi_store_preamble reg cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_cstore_preamble reg cond\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+  const char* kPreds[] = {
+    "eq",
+    "ne",
+    "lt",
+    "le",
+    "ls",
+    "ge",
+    "gt",
+    "hs",
+    "hi",
+    "lo",
+    "mi",
+    "pl",
+    NULL,
+  };
+
+  O << " @ ========================================\n";
+  O << "\t.text\n";
+}
diff -r 2b13dadc8fed lib/Target/ARM/ARMNaClRewritePass.cpp
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/lib/Target/ARM/ARMNaClRewritePass.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,645 @@
+//===-- ARMNaClRewritePass.cpp - Native Client Rewrite Pass  ------*- C++ -*-=//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Native Client Rewrite Pass
+// This final pass inserts the sandboxing instructions needed to run inside
+// the Native Client sandbox. Native Client requires certain software fault
+// isolation (SFI) constructions to be put in place, to prevent escape from
+// the sandbox. Native Client refuses to execute binaries without the correct
+// SFI sequences.
+// 
+// Potentially dangerous operations which are protected include:
+// * Stores
+// * Branches
+// * Changes to SP
+//
+//===----------------------------------------------------------------------===//
+
+#define DEBUG_TYPE "arm-sfi"
+#include "ARM.h"
+#include "ARMBaseInstrInfo.h"
+#include "ARMNaClRewritePass.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/Function.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Support/CommandLine.h"
+#include <set>
+#include <stdio.h>
+
+using namespace llvm;
+
+namespace llvm {
+
+cl::opt<bool>
+FlagSfiZeroMask("sfi-zero-mask");
+
+cl::opt<bool>
+FlagSfiData("sfi-data", cl::desc("use illegal at data bundle beginning"));
+
+cl::opt<bool>
+FlagSfiStore("sfi-store", cl::desc("enable sandboxing for stores"));
+
+cl::opt<bool> 
+FlagSfiStack("sfi-stack", cl::desc("enable sandboxing for stack changes"));
+
+cl::opt<bool>
+FlagSfiBranch("sfi-branch", cl::desc("enable sandboxing for branches"));
+
+}
+
+namespace {
+  class ARMNaClRewritePass : public MachineFunctionPass {
+  public:
+    static char ID;
+    ARMNaClRewritePass() : MachineFunctionPass(ID) {}
+
+    const ARMBaseInstrInfo *TII;
+    const TargetRegisterInfo *TRI;
+    virtual void getAnalysisUsage(AnalysisUsage &AU) const;
+    virtual bool runOnMachineFunction(MachineFunction &Fn);
+
+    virtual const char *getPassName() const {
+      return "ARM Native Client Rewrite Pass";
+    }
+
+  private:
+
+    bool SandboxStoresInBlock(MachineBasicBlock &MBB);
+    void SandboxStore(MachineBasicBlock &MBB,
+                      MachineBasicBlock::iterator MBBI,
+                      MachineInstr &MI,
+                      int AddrIdx,
+                      bool CPSRLive);
+    bool TryPredicating(MachineInstr &MI, ARMCC::CondCodes);
+
+    bool SandboxBranchesInBlock(MachineBasicBlock &MBB);
+    bool SandboxStackChangesInBlock(MachineBasicBlock &MBB);
+
+    void SandboxStackChange(MachineBasicBlock &MBB,
+                              MachineBasicBlock::iterator MBBI);
+    void LightweightVerify(MachineFunction &MF);
+  };
+  char ARMNaClRewritePass::ID = 0;
+}
+
+static bool IsReturn(const MachineInstr &MI) {
+  return (MI.getOpcode() == ARM::BX_RET);
+}
+
+static bool IsIndirectJump(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+   default: return false;
+
+   case ARM::BX:
+   case ARM::TAILJMPr:
+   case ARM::TAILJMPrND:
+    return true;
+  }
+}
+
+static bool IsIndirectCall(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+    case ARM::BLX:
+      return true;
+    case ARM::BLXr9:
+      assert(0 && "Internal error. Native Client should not be using r9.");
+  }
+  return false;
+}
+
+static bool IsDirectCall(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+   case ARM::BL:
+   case ARM::BL_pred:
+   case ARM::TPsoft:
+     return true;
+   case ARM::BLr9:
+   case ARM::BLr9_pred:
+    assert(0 && "Internal error. Native Client should not be using r9.");
+  }
+  return false;
+}
+
+static bool IsCPSRLiveOut(const MachineBasicBlock &MBB) {
+  // CPSR is live-out if any successor lists it as live-in.
+  for (MachineBasicBlock::const_succ_iterator SI = MBB.succ_begin(),
+                                              E = MBB.succ_end();
+       SI != E;
+       ++SI) {
+    const MachineBasicBlock *Succ = *SI;
+    if (Succ->isLiveIn(ARM::CPSR)) return true;
+  }
+  return false;
+}
+
+static void DumpInstructionVerbose(const MachineInstr &MI) {
+  dbgs() << MI;
+  dbgs() << MI.getNumOperands() << " operands:" << "\n";
+  for (unsigned i = 0; i < MI.getNumOperands(); ++i) {
+    const MachineOperand& op = MI.getOperand(i);
+    dbgs() << "  " << i << "(" << op.getType() << "):" << op << "\n";
+  }
+  dbgs() << "\n";
+}
+
+static void DumpBasicBlockVerbose(const MachineBasicBlock &MBB) {
+  dbgs() << "\n<<<<< DUMP BASIC BLOCK START\n";
+  for (MachineBasicBlock::const_iterator MBBI = MBB.begin(), MBBE = MBB.end();
+       MBBI != MBBE;
+       ++MBBI) {
+    DumpInstructionVerbose(*MBBI);
+  }
+  dbgs() << "<<<<< DUMP BASIC BLOCK END\n\n";
+}
+
+static void DumpBasicBlockVerboseCond(const MachineBasicBlock &MBB, bool b) {
+  if (b) {
+    DumpBasicBlockVerbose(MBB);
+  }
+}
+
+/**********************************************************************/
+/* Exported functions */
+
+namespace ARM_SFI {
+
+bool IsStackChange(const MachineInstr &MI, const TargetRegisterInfo *TRI) {
+  return MI.modifiesRegister(ARM::SP, TRI);
+}
+
+bool NextInstrMasksSP(const MachineInstr &MI) {
+  MachineBasicBlock::const_iterator It = &MI;
+  const MachineBasicBlock *MBB = MI.getParent();
+
+  MachineBasicBlock::const_iterator next = ++It;
+  if (next == MBB->end()) {
+    return false;
+  }
+
+  const MachineInstr &next_instr = *next;
+  unsigned opcode = next_instr.getOpcode();
+  return (opcode == ARM::SFI_DATA_MASK) &&
+      (next_instr.getOperand(0).getReg() == ARM::SP);
+}
+
+bool IsSandboxedStackChange(const MachineInstr &MI) {
+  unsigned opcode = MI.getOpcode();
+  switch (opcode) {
+    default: break;
+
+    // These just bump SP by a little (and access the stack),
+    // so that is okay due to guard pages.
+    case ARM::STMIA_UPD:
+    case ARM::STMDA_UPD:
+    case ARM::STMDB_UPD:
+    case ARM::STMIB_UPD:
+
+    case ARM::VSTMDIA_UPD:
+    case ARM::VSTMDDB_UPD:
+    case ARM::VSTMSIA_UPD:
+    case ARM::VSTMSDB_UPD:
+      return true;
+
+    // Similar, unless it is a load into SP...
+    case ARM::LDMIA_UPD:
+    case ARM::LDMDA_UPD:
+    case ARM::LDMDB_UPD:
+    case ARM::LDMIB_UPD:
+
+    case ARM::VLDMDIA_UPD:
+    case ARM::VLDMDDB_UPD:
+    case ARM::VLDMSIA_UPD:
+    case ARM::VLDMSDB_UPD: {
+      bool dest_SP = false;
+      // Dest regs start at operand index 4.
+      for (unsigned i = 4; i < MI.getNumOperands(); ++i) {
+        const MachineOperand &DestReg = MI.getOperand(i);
+        dest_SP = dest_SP || (DestReg.getReg() == ARM::SP);
+      }
+      if (dest_SP) {
+        break;
+      }
+      return true;
+    }
+
+    // Some localmods *should* prevent selecting a reg offset
+    // (see SelectAddrMode2 in ARMISelDAGToDAG.cpp).
+    // Otherwise, the store is already a potential violation.
+    case ARM::STR_PRE:
+    case ARM::STRH_PRE:
+    case ARM::STRB_PRE:
+      return true;
+
+    // Similar, unless it is a load into SP...
+    case ARM::LDR_PRE:
+    case ARM::LDRH_PRE:
+    case ARM::LDRB_PRE:
+    case ARM::LDRSH_PRE:
+    case ARM::LDRSB_PRE: {
+      const MachineOperand &DestReg = MI.getOperand(0);
+      if (DestReg.getReg() == ARM::SP) {
+        break;
+      }
+      return true;
+    }
+
+    // Here, if SP is the base / write-back reg, we need to check if
+    // a reg is used as offset (otherwise it is not a small nudge).
+    case ARM::STR_POST:
+    case ARM::STRH_POST:
+    case ARM::STRB_POST: {
+      const MachineOperand &WBReg = MI.getOperand(0);
+      const MachineOperand &OffReg = MI.getOperand(3);
+      if (WBReg.getReg() == ARM::SP && OffReg.getReg() != 0) {
+        break;
+      }
+      return true;
+    }
+
+    // Similar, but also check that DestReg is not SP.
+    case ARM::LDR_POST:
+    case ARM::LDRH_POST:
+    case ARM::LDRB_POST:
+    case ARM::LDRSH_POST:
+    case ARM::LDRSB_POST: {
+      const MachineOperand &DestReg = MI.getOperand(0);
+      if (DestReg.getReg() == ARM::SP) {
+        break;
+      }
+      const MachineOperand &WBReg = MI.getOperand(1);
+      const MachineOperand &OffReg = MI.getOperand(3);
+      if (WBReg.getReg() == ARM::SP && OffReg.getReg() != 0) {
+        break;
+      }
+      return true;
+    }
+  }
+
+  return (NextInstrMasksSP(MI));
+}
+
+bool NeedSandboxStackChange(const MachineInstr &MI,
+                               const TargetRegisterInfo *TRI) {
+  return (IsStackChange(MI, TRI) && !IsSandboxedStackChange(MI));
+}
+
+} // namespace ARM_SFI
+
+/**********************************************************************/
+
+void ARMNaClRewritePass::getAnalysisUsage(AnalysisUsage &AU) const {
+  // Slight (possibly unnecessary) efficiency tweak:
+  // Promise not to modify the CFG.
+  AU.setPreservesCFG();
+  MachineFunctionPass::getAnalysisUsage(AU);
+}
+
+/*
+ * A primitive validator to catch problems at compile time.
+ * E.g., it could be used along with bugpoint to reduce a bitcode file.
+ */
+void ARMNaClRewritePass::LightweightVerify(MachineFunction &MF) {
+
+  for (MachineFunction::iterator MFI = MF.begin(), MFE = MF.end();
+       MFI != MFE;
+       ++MFI) {
+    MachineBasicBlock &MBB = *MFI;
+    for (MachineBasicBlock::iterator MBBI = MBB.begin(), MBBE = MBB.end();
+         MBBI != MBBE;
+         ++MBBI) {
+      MachineInstr &MI = *MBBI;
+
+      if (ARM_SFI::NeedSandboxStackChange(MI, TRI)) {
+        dbgs() << "LightWeightVerify for function: "
+               << MF.getFunction()->getName() << "  (BAD STACK CHANGE)\n";
+        DumpInstructionVerbose(MI);
+        DumpBasicBlockVerbose(MBB);
+        //        assert(false && "LightweightVerify Failed");
+      }
+    }
+  }
+}
+
+void ARMNaClRewritePass::SandboxStackChange(MachineBasicBlock &MBB,
+                                       MachineBasicBlock::iterator MBBI) {
+  // (1) Ensure there is room in the bundle for a data mask instruction
+  // (nop'ing to the next bundle if needed).
+  // (2) Do a data mask on SP after the instruction that updated SP.
+  MachineInstr &MI = *MBBI;
+
+  // Use same predicate as current instruction.
+  ARMCC::CondCodes Pred = TII->getPredicate(&MI);
+
+  BuildMI(MBB, MBBI, MI.getDebugLoc(),
+          TII->get(ARM::SFI_NOP_IF_AT_BUNDLE_END));
+
+  // Get to next instr (one + to get the original, and one more + to get past)
+  MachineBasicBlock::iterator MBBINext = (MBBI++);
+  MachineBasicBlock::iterator MBBINext2 = (MBBI++);
+
+  BuildMI(MBB, MBBINext2, MI.getDebugLoc(),
+          TII->get(ARM::SFI_DATA_MASK))
+      .addReg(ARM::SP)         // modify SP (as dst)
+      .addReg(ARM::SP)         // start with SP (as src)
+      .addImm((int64_t) Pred)  // predicate condition
+      .addReg(ARM::CPSR);      // predicate source register (CPSR)
+
+  return;
+}
+
+bool ARMNaClRewritePass::SandboxStackChangesInBlock(MachineBasicBlock &MBB) {
+  bool Modified = false;
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+       MBBI != E;
+       ++MBBI) {
+    MachineInstr &MI = *MBBI;
+    if (ARM_SFI::NeedSandboxStackChange(MI, TRI)) {
+      SandboxStackChange(MBB, MBBI);
+      Modified |= true;
+    }
+  }
+  return Modified;
+}
+
+bool ARMNaClRewritePass::SandboxBranchesInBlock(MachineBasicBlock &MBB) {
+  bool Modified = false;
+
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+       MBBI != E;
+       ++MBBI) {
+    MachineInstr &MI = *MBBI;
+
+    if (IsReturn(MI)) {
+      ARMCC::CondCodes Pred = TII->getPredicate(&MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_RETURN))
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+      Modified = true;
+    }
+
+    if (IsIndirectJump(MI)) {
+      MachineOperand &Addr = MI.getOperand(0);
+      ARMCC::CondCodes Pred = TII->getPredicate(&MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_INDIRECT_JMP))
+        .addOperand(Addr)        // rD
+        .addReg(0)               // apparently unused source register?
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+      Modified = true;
+    }
+
+    if (IsDirectCall(MI)) {
+      ARMCC::CondCodes Pred = TII->getPredicate(&MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_CALL))
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+      Modified = true;
+    }
+
+    if (IsIndirectCall(MI)) {
+      MachineOperand &Addr = MI.getOperand(0);
+      ARMCC::CondCodes Pred = TII->getPredicate(&MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_INDIRECT_CALL))
+        .addOperand(Addr)        // rD
+        .addReg(0)               // apparently unused source register?
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+        Modified = true;
+    }
+  }
+
+  return Modified;
+}
+
+bool ARMNaClRewritePass::TryPredicating(MachineInstr &MI, ARMCC::CondCodes Pred) {
+  // Can't predicate if it's already predicated.
+  // TODO(cbiffle): actually we can, if the conditions match.
+  if (TII->isPredicated(&MI)) return false;
+
+  /*
+   * ARM predicate operands use two actual MachineOperands: an immediate
+   * holding the predicate condition, and a register referencing the flags.
+   */
+  SmallVector<MachineOperand, 2> PredOperands;
+  PredOperands.push_back(MachineOperand::CreateImm((int64_t) Pred));
+  PredOperands.push_back(MachineOperand::CreateReg(ARM::CPSR, false));
+
+  // This attempts to rewrite, but some instructions can't be predicated.
+  return TII->PredicateInstruction(&MI, PredOperands);
+}
+
+/*
+ * Sandboxes a store instruction by inserting an appropriate mask or check
+ * operation before it.
+ */
+void ARMNaClRewritePass::SandboxStore(MachineBasicBlock &MBB,
+                                      MachineBasicBlock::iterator MBBI,
+                                      MachineInstr &MI,
+                                      int AddrIdx,
+                                      bool CPSRLive) {
+  MachineOperand &Addr = MI.getOperand(AddrIdx);
+
+  if (!CPSRLive && TryPredicating(MI, ARMCC::EQ)) {
+    /*
+     * For unconditional stores where CPSR is not in use, we can use a faster
+     * sandboxing sequence by predicating the store -- assuming we *can*
+     * predicate the store.
+     */
+
+    // Instruction can be predicated -- use the new sandbox.
+    BuildMI(MBB, MBBI, MI.getDebugLoc(),
+            TII->get(ARM::SFI_GUARD_STORE_TST))
+      .addOperand(Addr)   // rD
+      .addReg(0);         // apparently unused source register?
+  } else {
+    // Use the older BIC sandbox, which is universal, but incurs a stall.
+    ARMCC::CondCodes Pred = TII->getPredicate(&MI);
+    BuildMI(MBB, MBBI, MI.getDebugLoc(),
+            TII->get(ARM::SFI_GUARD_STORE))
+      .addOperand(Addr)        // rD
+      .addReg(0)               // apparently unused source register?
+      .addImm((int64_t) Pred)  // predicate condition
+      .addReg(ARM::CPSR);      // predicate source register (CPSR)
+
+    /*
+     * This pseudo-instruction is intended to generate something resembling the
+     * following, but with alignment enforced.
+     * TODO(cbiffle): move alignment into this function, use the code below.
+     *
+     *  // bic<cc> Addr, Addr, #0xC0000000
+     *  BuildMI(MBB, MBBI, MI.getDebugLoc(),
+     *          TII->get(ARM::BICri))
+     *    .addOperand(Addr)        // rD
+     *    .addOperand(Addr)        // rN
+     *    .addImm(0xC0000000)      // imm
+     *    .addImm((int64_t) Pred)  // predicate condition
+     *    .addReg(ARM::CPSR)       // predicate source register (CPSR)
+     *    .addReg(0);              // flag output register (0 == no flags)
+     */
+  }
+}
+
+static bool IsDangerousStore(const MachineInstr &MI, int *AddrIdx) {
+  unsigned Opcode = MI.getOpcode();
+  switch (Opcode) {
+  default: return false;
+
+  // Instructions with base address register in position 0...
+  case ARM::STMIA:
+  case ARM::STMDA:
+  case ARM::STMDB:
+  case ARM::STMIB:
+
+  case ARM::VSTMDIA:
+  case ARM::VSTMDDB:
+  case ARM::VSTMSIA:
+  case ARM::VSTMSDB:
+    *AddrIdx = 0;
+    break;
+
+  // Instructions with base address register in position 1...
+  case ARM::STMIA_UPD: // same reg at position 0 and position 1
+  case ARM::STMDA_UPD:
+  case ARM::STMDB_UPD:
+  case ARM::STMIB_UPD:
+
+  case ARM::STRH:
+  case ARM::STRi12:
+  case ARM::STRrs:
+  case ARM::STRBi12:
+  case ARM::STRBrs:
+  case ARM::VSTMDIA_UPD:
+  case ARM::VSTMDDB_UPD:
+  case ARM::VSTMSIA_UPD:
+  case ARM::VSTMSDB_UPD:
+  case ARM::VSTRS:
+  case ARM::VSTRD:
+    *AddrIdx = 1;
+    break;
+
+  // Instructions with base address register in position 2...
+  case ARM::STR_PRE:
+  case ARM::STR_POST:
+  case ARM::STRB_PRE:
+  case ARM::STRB_POST:
+  case ARM::STRH_PRE:
+  case ARM::STRH_POST:
+  case ARM::STRD:
+  case ARM::STREX:
+  case ARM::STREXB:
+  case ARM::STREXH:
+    *AddrIdx = 2;
+    break;
+  }
+
+  if (MI.getOperand(*AddrIdx).getReg() == ARM::SP) {
+    // The contents of SP do not require masking.
+    return false;
+  }
+
+  return true;
+}
+
+bool ARMNaClRewritePass::SandboxStoresInBlock(MachineBasicBlock &MBB) {
+  /*
+   * This is a simple local reverse-dataflow analysis to determine where CPSR
+   * is live.  We cannot use the conditional store sequence anywhere that CPSR
+   * is live, or we'd affect correctness.  The existing liveness analysis passes
+   * barf when applied pre-emit, after allocation, so we must do it ourselves.
+   */
+
+  // LOCALMOD(pdox): Short-circuit this function. Assume CPSR is always live,
+  //                 until we figure out why the assert is tripping.
+  bool Modified2 = false;
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+       MBBI != E;
+       ++MBBI) {
+    MachineInstr &MI = *MBBI;
+    int AddrIdx;
+
+    if (IsDangerousStore(MI, &AddrIdx)) {
+      bool CPSRLive = true;
+      SandboxStore(MBB, MBBI, MI, AddrIdx, CPSRLive);
+      Modified2 = true;
+    }
+  }
+  return Modified2;
+  // END LOCALMOD(pdox)
+
+  bool CPSRLive = IsCPSRLiveOut(MBB);
+
+  // Given that, record which instructions should not be altered to trash CPSR:
+  std::set<const MachineInstr *> InstrsWhereCPSRLives;
+  for (MachineBasicBlock::const_reverse_iterator MBBI = MBB.rbegin(),
+                                                 E = MBB.rend();
+       MBBI != E;
+       ++MBBI) {
+    const MachineInstr &MI = *MBBI;
+    // Check for kills first.
+    if (MI.modifiesRegister(ARM::CPSR, TRI)) CPSRLive = false;
+    // Then check for uses.
+    if (MI.readsRegister(ARM::CPSR)) CPSRLive = true;
+
+    if (CPSRLive) InstrsWhereCPSRLives.insert(&MI);
+  }
+
+  // Sanity check:
+  assert(CPSRLive == MBB.isLiveIn(ARM::CPSR)
+         && "CPSR Liveness analysis does not match cached live-in result.");
+
+  // Now: find and sandbox stores.
+  bool Modified = false;
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+       MBBI != E;
+       ++MBBI) {
+    MachineInstr &MI = *MBBI;
+    int AddrIdx;
+
+    if (IsDangerousStore(MI, &AddrIdx)) {
+      bool CPSRLive =
+        (InstrsWhereCPSRLives.find(&MI) != InstrsWhereCPSRLives.end());
+      SandboxStore(MBB, MBBI, MI, AddrIdx, CPSRLive);
+      Modified = true;
+    }
+  }
+
+  return Modified;
+}
+
+/**********************************************************************/
+
+bool ARMNaClRewritePass::runOnMachineFunction(MachineFunction &MF) {
+  TII = static_cast<const ARMBaseInstrInfo*>(MF.getTarget().getInstrInfo());
+  TRI = MF.getTarget().getRegisterInfo();
+
+  bool Modified = false;
+  for (MachineFunction::iterator MFI = MF.begin(), E = MF.end();
+       MFI != E;
+       ++MFI) {
+    MachineBasicBlock &MBB = *MFI;
+
+    if (FlagSfiStore)  Modified |= SandboxStoresInBlock(MBB);
+    if (FlagSfiBranch) Modified |= SandboxBranchesInBlock(MBB);
+    if (FlagSfiStack)  Modified |= SandboxStackChangesInBlock(MBB);
+  }
+  DEBUG(LightweightVerify(MF));
+  return Modified;
+}
+
+/// createARMNaClRewritePass - returns an instance of the NaClRewritePass.
+FunctionPass *llvm::createARMNaClRewritePass() {
+  return new ARMNaClRewritePass();
+}
+
diff -r 2b13dadc8fed lib/Target/ARM/ARMNaClRewritePass.h
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/lib/Target/ARM/ARMNaClRewritePass.h	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,35 @@
+//===-- ARMNaClRewritePass.h - NaCl Sandboxing Pass    ------- --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef TARGET_ARMNACLREWRITEPASS_H
+#define TARGET_ARMNACLREWRITEPASS_H
+
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/Support/CommandLine.h"
+
+namespace llvm {
+  extern cl::opt<bool> FlagSfiZeroMask;
+  extern cl::opt<bool> FlagSfiData;
+  extern cl::opt<bool> FlagSfiStore;
+  extern cl::opt<bool> FlagSfiStack;
+  extern cl::opt<bool> FlagSfiBranch;
+}
+
+namespace ARM_SFI {
+
+bool IsStackChange(const llvm::MachineInstr &MI,
+                   const llvm::TargetRegisterInfo *TRI);
+bool IsSandboxedStackChange(const llvm::MachineInstr &MI);
+bool NeedSandboxStackChange(const llvm::MachineInstr &MI,
+                               const llvm::TargetRegisterInfo *TRI);
+
+} // namespace ARM_SFI
+
+#endif
diff -r 2b13dadc8fed lib/Target/ARM/ARMSubtarget.cpp
--- a/lib/Target/ARM/ARMSubtarget.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMSubtarget.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -20,13 +20,22 @@
 #include "llvm/ADT/SmallVector.h"
 using namespace llvm;
 
-static cl::opt<bool>
+cl::opt<bool> // @LOCALMOD
 ReserveR9("arm-reserve-r9", cl::Hidden,
           cl::desc("Reserve R9, making it unavailable as GPR"));
 
 static cl::opt<bool>
 DarwinUseMOVT("arm-darwin-use-movt", cl::init(true), cl::Hidden);
 
+// @LOCALMOD-START
+// TODO: * JITing has not been tested at all
+//       * Thumb mode operation is also not clear: it seems jump tables
+//         for thumb are broken independent of this option
+static cl::opt<bool>
+NoInlineJumpTables("no-inline-jumptables",
+                  cl::desc("Do not place jump tables inline in the code"));
+// @LOCALMOD-END
+                     
 static cl::opt<bool>
 StrictAlign("arm-strict-align", cl::Hidden,
             cl::desc("Disallow all unaligned memory accesses"));
@@ -44,6 +53,7 @@
   , NoARM(false)
   , PostRAScheduler(false)
   , IsR9Reserved(ReserveR9)
+  , UseInlineJumpTables(!NoInlineJumpTables) // @LOCALMOD
   , UseMovt(false)
   , HasFP16(false)
   , HasD16(false)
@@ -153,6 +163,11 @@
     UseMovt = DarwinUseMOVT && hasV6T2Ops();
   }
 
+  // @LOCALMOD-BEGIN
+  // This should be true if isTargetNaCl()
+  UseMovt = true;
+  // @LOCALMOD-END
+
   if (!isThumb() || hasThumb2())
     PostRAScheduler = true;
 
diff -r 2b13dadc8fed lib/Target/ARM/ARMSubtarget.h
--- a/lib/Target/ARM/ARMSubtarget.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMSubtarget.h	Thu Jun 09 18:06:40 2011 -0700
@@ -20,6 +20,14 @@
 #include "llvm/ADT/Triple.h"
 #include <string>
 
+// @LOCALMOD-BEGIN
+#include "llvm/Support/CommandLine.h"
+namespace llvm {
+  extern cl::opt<bool> FlagSfiDisableCP;
+}
+// @LOCALMOD-END
+
+
 namespace llvm {
 class GlobalValue;
 
@@ -132,6 +140,12 @@
   /// Selected instruction itineraries (one entry per itinerary class.)
   InstrItineraryData InstrItins;
 
+  // @LOCALMOD-START
+  /// UseInlineJumpTables - True if jump tables should be in-line in the code.
+  bool UseInlineJumpTables;
+  // @LOCALMOD-END
+
+
  public:
   enum {
     isELF, isDarwin
@@ -205,6 +219,9 @@
 
   bool useMovt() const { return UseMovt && hasV6T2Ops(); }
 
+  // @LOCALMOD
+  bool useConstPool() const { return !FlagSfiDisableCP; }
+
   bool allowsUnalignedMem() const { return AllowsUnalignedMem; }
 
   const std::string & getCPUString() const { return CPUString; }
@@ -228,6 +245,8 @@
   /// GVIsIndirectSymbol - true if the GV will be accessed via an indirect
   /// symbol.
   bool GVIsIndirectSymbol(const GlobalValue *GV, Reloc::Model RelocM) const;
+
+  bool useInlineJumpTables() const {return UseInlineJumpTables;} // @LOCALMOD
 };
 } // End llvm namespace
 
diff -r 2b13dadc8fed lib/Target/ARM/ARMTargetMachine.cpp
--- a/lib/Target/ARM/ARMTargetMachine.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMTargetMachine.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -24,6 +24,13 @@
 
 static cl::opt<bool>ExpandMLx("expand-fp-mlx", cl::init(false), cl::Hidden);
 
+// @LOCALMOD-START
+namespace llvm {
+cl::opt<bool> FlagSfiDisableCP("sfi-disable-cp",
+                               cl::desc("disable arm constant island pools"));
+}
+// @LOCALMOD-END
+
 static MCAsmInfo *createMCAsmInfo(const Target &T, StringRef TT) {
   Triple TheTriple(TT);
   switch (TheTriple.getOS()) {
@@ -185,7 +192,22 @@
   if (Subtarget.isThumb2() && !Subtarget.prefers32BitThumb())
     PM.add(createThumb2SizeReductionPass());
 
+  // @LOCALMOD-START
+  // Note with FlagSfiDisableCP we effectively disable the
+  // ARMConstantIslandPass and rely on movt/movw to eliminate the need
+  // for constant islands
+  if (FlagSfiDisableCP) {
+    assert(Subtarget.useMovt());
+  }
+  // @LOCALMOD-END
+
   PM.add(createARMConstantIslandPass());
+  
+  // @LOCALMOD-START
+  // This pass does all the heavy sfi lifting. 
+  PM.add(createARMNaClRewritePass());
+  // @LOCALMOD-END
+ 
   return true;
 }
 
diff -r 2b13dadc8fed lib/Target/ARM/ARMTargetMachine.h
--- a/lib/Target/ARM/ARMTargetMachine.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/ARMTargetMachine.h	Thu Jun 09 18:06:40 2011 -0700
@@ -29,6 +29,13 @@
 #include "llvm/MC/MCStreamer.h"
 #include "llvm/ADT/OwningPtr.h"
 
+// @LOCALMOD-START
+#include "llvm/Support/CommandLine.h"
+namespace llvm {
+   extern cl::opt<bool> FlagSfiDisableCP;
+}
+// @LOCALMOD-END
+
 namespace llvm {
 
 class ARMBaseTargetMachine : public LLVMTargetMachine {
diff -r 2b13dadc8fed lib/Target/ARM/CMakeLists.txt
--- a/lib/Target/ARM/CMakeLists.txt	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/CMakeLists.txt	Thu Jun 09 18:06:40 2011 -0700
@@ -39,6 +39,8 @@
   ARMLoadStoreOptimizer.cpp
   ARMMCAsmInfo.cpp
   ARMMCInstLower.cpp
+  ARMNaClHeaders.cpp
+  ARMNaClRewritePass.cpp
   ARMRegisterInfo.cpp
   ARMSelectionDAGInfo.cpp
   ARMSubtarget.cpp
diff -r 2b13dadc8fed lib/Target/ARM/InstPrinter/ARMInstPrinter.cpp
--- a/lib/Target/ARM/InstPrinter/ARMInstPrinter.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/ARM/InstPrinter/ARMInstPrinter.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -106,6 +106,69 @@
     return;
   }
 
+  // @LOCALMOD-BEGIN
+  // TODO(pdox): Kill this code once we switch to MC object emission
+  const char *SFIInst = NULL;
+  unsigned SFIEmitDest = ~0;
+  unsigned SFIEmitPred = ~0;
+  switch (Opcode) {
+  case ARM::SFI_NOP_IF_AT_BUNDLE_END :
+    SFIInst = "sfi_nop_if_at_bundle_end";
+    SFIEmitDest = ~0;
+    SFIEmitPred = ~0;
+    break;
+  case ARM::SFI_GUARD_STORE        :
+    SFIInst = "sfi_store_preamble";
+    SFIEmitDest = 0;
+    SFIEmitPred = 2;
+    break;
+  case ARM::SFI_GUARD_INDIRECT_CALL:
+    SFIInst = "sfi_indirect_call_preamble";
+    SFIEmitDest = 0;
+    SFIEmitPred = 2;
+    break;
+  case ARM::SFI_GUARD_INDIRECT_JMP :
+    SFIInst = "sfi_indirect_jump_preamble";
+    SFIEmitDest = 0;
+    SFIEmitPred = 2;
+    break;
+  case ARM::SFI_DATA_MASK          :
+    SFIInst = "sfi_data_mask";
+    SFIEmitDest = 0;
+    SFIEmitPred = 2;
+    break;
+  case ARM::SFI_GUARD_STORE_TST:
+    SFIInst = "sfi_cstore_preamble";
+    SFIEmitDest = 0;
+    SFIEmitPred = ~0;
+    break;
+  case ARM::SFI_GUARD_CALL     :
+    SFIInst = "sfi_call_preamble";
+    SFIEmitDest = ~0;
+    SFIEmitPred = 0;
+    break;
+  case ARM::SFI_GUARD_RETURN   :
+    SFIInst = "sfi_return_preamble lr,";
+    SFIEmitDest = ~0;
+    SFIEmitPred = 0;
+    break;
+  }
+  if (SFIInst) {
+    O << '\t' << SFIInst;
+    if (SFIEmitDest != ~0) {
+      O << ' ';
+      printOperand(MI, SFIEmitDest, O);
+    }
+    if (SFIEmitDest != ~0 && SFIEmitPred != ~0) {
+      O << ',';
+    }
+    if (SFIEmitPred != ~0) {
+      O << ' ';
+      printPredicateOperand(MI, SFIEmitPred, O);
+    }
+    O << '\n';
+    return;
+  }
   printInstruction(MI, O);
 }
 
diff -r 2b13dadc8fed lib/Target/X86/CMakeLists.txt
--- a/lib/Target/X86/CMakeLists.txt	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/CMakeLists.txt	Thu Jun 09 18:06:40 2011 -0700
@@ -28,11 +28,13 @@
   X86ISelDAGToDAG.cpp
   X86ISelLowering.cpp
   X86InstrInfo.cpp
+  X86InstrNaCl.cpp
   X86JITInfo.cpp
   X86MachObjectWriter.cpp
   X86MCAsmInfo.cpp
   X86MCCodeEmitter.cpp 
   X86MCInstLower.cpp
+  X86NaClRewritePass.cpp
   X86RegisterInfo.cpp
   X86SelectionDAGInfo.cpp
   X86Subtarget.cpp
diff -r 2b13dadc8fed lib/Target/X86/X86.h
--- a/lib/Target/X86/X86.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86.h	Thu Jun 09 18:06:40 2011 -0700
@@ -52,6 +52,9 @@
 /// crossings.
 FunctionPass *createSSEDomainFixPass();
 
+// @LOCALMOD - Creates a pass to make instructions follow NaCl SFI rules.
+FunctionPass* createX86NaClRewritePass();
+
 /// createX86CodeEmitterPass - Return a pass that emits the collected X86 code
 /// to the specified MCE object.
 FunctionPass *createX86JITCodeEmitterPass(X86TargetMachine &TM,
diff -r 2b13dadc8fed lib/Target/X86/X86AsmBackend.cpp
--- a/lib/Target/X86/X86AsmBackend.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86AsmBackend.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -10,6 +10,7 @@
 #include "llvm/Target/TargetAsmBackend.h"
 #include "X86.h"
 #include "X86FixupKinds.h"
+#include "X86InstrNaCl.h" // @LOCALMOD
 #include "llvm/ADT/Twine.h"
 #include "llvm/MC/MCAssembler.h"
 #include "llvm/MC/MCELFObjectWriter.h"
@@ -299,6 +300,20 @@
     const MCSectionELF &ES = static_cast<const MCSectionELF&>(Section);
     return ES.getFlags() & ELF::SHF_MERGE;
   }
+
+  // @LOCALMOD-BEGIN
+  unsigned getBundleSize() const {
+    return OSType == Triple::NativeClient ? 32 : 0;
+  }
+
+  bool CustomExpandInst(const MCInst &Inst, MCStreamer &Out) const {
+    if (OSType == Triple::NativeClient) {
+      return CustomExpandInstNaCl(Inst, Out);
+    }
+    return false;
+  }
+  // @LOCALMOD-END
+
 };
 
 class ELFX86_32AsmBackend : public ELFX86AsmBackend {
diff -r 2b13dadc8fed lib/Target/X86/X86AsmPrinter.cpp
--- a/lib/Target/X86/X86AsmPrinter.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86AsmPrinter.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -73,6 +73,34 @@
   return false;
 }
 
+// @LOCALMOD-BEGIN
+void X86AsmPrinter::EmitFunctionBodyEnd() {
+  if (Subtarget->isTargetNaCl())
+    EmitAlignment(5);
+}
+
+bool X86AsmPrinter::UseReadOnlyJumpTables() const {
+  return Subtarget->isTargetNaCl();
+}
+
+unsigned X86AsmPrinter::GetTargetLabelAlign(const MachineInstr *MI) const {
+  if (Subtarget->isTargetNaCl()) {
+    switch (MI->getOpcode()) {
+      default: return 0;
+      // These labels may indicate an indirect entry point that is
+      // externally reachable and hence must be bundle aligned.
+      // Note: these labels appear to be always at basic block beginnings
+      // so it may be possible to simply set the MBB alignment.
+      // However, it is unclear whether this always holds.
+      case TargetOpcode::EH_LABEL:
+      case TargetOpcode::GC_LABEL:
+        return 5;
+    }
+  }
+  return 0;
+}
+// @LOCALMOD-END
+
 /// printSymbolOperand - Print a raw symbol reference operand.  This handles
 /// jump tables, constant pools, global address and external symbols, all of
 /// which print to a label with various suffixes for relocation types etc.
diff -r 2b13dadc8fed lib/Target/X86/X86AsmPrinter.h
--- a/lib/Target/X86/X86AsmPrinter.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86AsmPrinter.h	Thu Jun 09 18:06:40 2011 -0700
@@ -50,7 +50,11 @@
   virtual void EmitEndOfAsmFile(Module &M);
   
   virtual void EmitInstruction(const MachineInstr *MI);
-  
+
+  virtual void EmitFunctionBodyEnd(); // @LOCALMOD
+  virtual bool UseReadOnlyJumpTables() const; // @LOCALMOD
+  virtual unsigned GetTargetLabelAlign(const MachineInstr *MI) const;//@LOCALMOD
+
   void printSymbolOperand(const MachineOperand &MO, raw_ostream &O);
 
   // These methods are used by the tablegen'erated instruction printer.
diff -r 2b13dadc8fed lib/Target/X86/X86FrameLowering.cpp
--- a/lib/Target/X86/X86FrameLowering.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86FrameLowering.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -107,6 +107,8 @@
   case X86::TCRETURNmi:
   case X86::TCRETURNdi64:
   case X86::TCRETURNri64:
+  case X86::NACL_CG_TCRETURNdi64: // @LOCALMOD
+  case X86::NACL_CG_TCRETURNri64: // @LOCALMOD
   case X86::TCRETURNmi64:
   case X86::EH_RETURN:
   case X86::EH_RETURN64: {
@@ -291,7 +293,7 @@
   bool HasFP = hasFP(MF);
 
   // Calculate amount of bytes used for return address storing.
-  int stackGrowth = -TD->getPointerSize();
+  int stackGrowth = -TM.getFrameLowering()->getStackSlotSize(); // @LOCALMOD
 
   // FIXME: This is dirty hack. The code itself is pretty mess right now.
   // It should be rewritten from scratch and generalized sometimes.
@@ -429,7 +431,7 @@
   std::vector<MachineMove> &Moves = MMI.getFrameMoves();
   const TargetData *TD = MF.getTarget().getTargetData();
   uint64_t NumBytes = 0;
-  int stackGrowth = -TD->getPointerSize();
+  int stackGrowth = -TM.getFrameLowering()->getStackSlotSize(); // @LOCALMOD
 
   if (HasFP) {
     // Calculate required stack adjustment.
@@ -661,6 +663,8 @@
   case X86::TCRETURNdi64:
   case X86::TCRETURNri64:
   case X86::TCRETURNmi64:
+  case X86::NACL_CG_TCRETURNdi64: // @LOCALMOD
+  case X86::NACL_CG_TCRETURNri64: // @LOCALMOD
   case X86::EH_RETURN:
   case X86::EH_RETURN64:
     break;  // These are ok
@@ -760,6 +764,8 @@
   } else if (RetOpcode == X86::TCRETURNri || RetOpcode == X86::TCRETURNdi ||
              RetOpcode == X86::TCRETURNmi ||
              RetOpcode == X86::TCRETURNri64 || RetOpcode == X86::TCRETURNdi64 ||
+             RetOpcode == X86::NACL_CG_TCRETURNri64 || // @LOCALMOD
+             RetOpcode == X86::NACL_CG_TCRETURNdi64 || // @LOCALMOD
              RetOpcode == X86::TCRETURNmi64) {
     bool isMem = RetOpcode == X86::TCRETURNmi || RetOpcode == X86::TCRETURNmi64;
     // Tail call return: adjust the stack pointer and jump to callee.
@@ -785,10 +791,22 @@
     }
 
     // Jump to label or value in register.
-    if (RetOpcode == X86::TCRETURNdi || RetOpcode == X86::TCRETURNdi64) {
+    if (RetOpcode == X86::TCRETURNdi || RetOpcode == X86::TCRETURNdi64 ||
+        RetOpcode == X86::NACL_CG_TCRETURNdi64) { // @LOCALMOD
+      // @LOCALMOD-BEGIN
+      unsigned TailJmpOpc;
+      switch (RetOpcode) {
+      case X86::TCRETURNdi  : TailJmpOpc = X86::TAILJMPd; break;
+      case X86::TCRETURNdi64: TailJmpOpc = X86::TAILJMPd64; break;
+      case X86::NACL_CG_TCRETURNdi64:
+        TailJmpOpc = X86::NACL_CG_TAILJMPd64;
+        break;
+      default: llvm_unreachable("Unexpected return opcode");
+      }
+      // @LOCALMOD-END
       MachineInstrBuilder MIB =
-        BuildMI(MBB, MBBI, DL, TII.get((RetOpcode == X86::TCRETURNdi)
-                                       ? X86::TAILJMPd : X86::TAILJMPd64));
+        BuildMI(MBB, MBBI, DL, TII.get(TailJmpOpc)); // @LOCALMOD
+
       if (JumpTarget.isGlobal())
         MIB.addGlobalAddress(JumpTarget.getGlobal(), JumpTarget.getOffset(),
                              JumpTarget.getTargetFlags());
@@ -806,6 +824,11 @@
     } else if (RetOpcode == X86::TCRETURNri64) {
       BuildMI(MBB, MBBI, DL, TII.get(X86::TAILJMPr64)).
         addReg(JumpTarget.getReg(), RegState::Kill);
+// @LOCALMOD-BEGIN
+    } else if (RetOpcode == X86::NACL_CG_TCRETURNri64) {
+      BuildMI(MBB, MBBI, DL, TII.get(X86::NACL_CG_TAILJMPr64)).
+        addReg(JumpTarget.getReg(), RegState::Kill);
+// @LOCALMOD-END
     } else {
       BuildMI(MBB, MBBI, DL, TII.get(X86::TAILJMPr)).
         addReg(JumpTarget.getReg(), RegState::Kill);
diff -r 2b13dadc8fed lib/Target/X86/X86FrameLowering.h
--- a/lib/Target/X86/X86FrameLowering.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86FrameLowering.h	Thu Jun 09 18:06:40 2011 -0700
@@ -28,7 +28,8 @@
   explicit X86FrameLowering(const X86TargetMachine &tm, const X86Subtarget &sti)
     : TargetFrameLowering(StackGrowsDown,
                           sti.getStackAlignment(),
-                          (sti.is64Bit() ? -8 : -4)),
+                          (sti.is64Bit() ? -8 : -4),   // @LOCALMOD
+                          1, (sti.is64Bit() ? 8 : 4)), // @LOCALMOD
       TM(tm), STI(sti) {
   }
 
@@ -63,3 +64,4 @@
 } // End llvm namespace
 
 #endif
+y
diff -r 2b13dadc8fed lib/Target/X86/X86ISelDAGToDAG.cpp
--- a/lib/Target/X86/X86ISelDAGToDAG.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86ISelDAGToDAG.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -210,6 +210,10 @@
                              SDValue &Index, SDValue &Disp,
                              SDValue &Segment,
                              SDValue &NodeWithChain);
+    // @LOCALMOD-BEGIN
+    void LegalizeIndexForNaCl(SDValue N, X86ISelAddressMode &AM);
+    // @LOCALMOD-END
+
     
     bool TryFoldLoad(SDNode *P, SDValue N,
                      SDValue &Base, SDValue &Scale,
@@ -286,6 +290,15 @@
     const X86InstrInfo *getInstrInfo() {
       return getTargetMachine().getInstrInfo();
     }
+
+    // @LOCALMOD-START
+    bool selectingMemOp;
+    bool RestrictUseOfBaseReg() {
+      return selectingMemOp && Subtarget->isTargetNaCl64();
+    }
+    // @LOCALMOD-END
+
+
   };
 }
 
@@ -429,6 +442,7 @@
     SDNode *N = I++;  // Preincrement iterator to avoid invalidation issues.
 
     if (OptLevel != CodeGenOpt::None &&
+        !Subtarget->isTargetNaCl() &&   // @LOCALMOD: We can't fold load/call
         (N->getOpcode() == X86ISD::CALL ||
          N->getOpcode() == X86ISD::TC_RETURN)) {
       /// Also try moving call address load from outside callseq_start to just
@@ -548,7 +562,15 @@
 
 bool X86DAGToDAGISel::MatchLoadInAddress(LoadSDNode *N, X86ISelAddressMode &AM){
   SDValue Address = N->getOperand(1);
-  
+
+  // @LOCALMOD-START
+  // Disable this tls access optimization in Native Client, since
+  // gs:0 (or fs:0 on X86-64) does not exactly contain its own address.
+  if (Subtarget->isTargetNaCl()) {
+    return true;
+  }
+  // @LOCALMOD-END
+    
   // load gs:0 -> GS segment register.
   // load fs:0 -> FS segment register.
   //
@@ -660,6 +682,8 @@
   if (MatchAddressRecursively(N, AM, 0))
     return true;
 
+
+  if (!RestrictUseOfBaseReg()) {   // @LOCALMOD
   // Post-processing: Convert lea(,%reg,2) to lea(%reg,%reg), which has
   // a smaller encoding and avoids a scaled-index.
   if (AM.Scale == 2 &&
@@ -668,7 +692,8 @@
     AM.Base_Reg = AM.IndexReg;
     AM.Scale = 1;
   }
-
+  } // @LOCALMOD
+  
   // Post-processing: Convert foo to foo(%rip), even in non-PIC mode,
   // because it has a smaller encoding.
   // TODO: Which other code models can use this?
@@ -796,6 +821,8 @@
     // FALL THROUGH
   case ISD::MUL:
   case X86ISD::MUL_IMM:
+    // @LOCALMOD
+    if (!RestrictUseOfBaseReg()) {
     // X*[3,5,9] -> X+X*[2,4,8]
     if (AM.BaseType == X86ISelAddressMode::RegBase &&
         AM.Base_Reg.getNode() == 0 &&
@@ -833,6 +860,7 @@
           return false;
         }
     }
+    } // @LOCALMOD
     break;
 
   case ISD::SUB: {
@@ -927,6 +955,7 @@
       return false;
     AM = Backup;
 
+    if (!RestrictUseOfBaseReg()) { // @LOCALMOD
     // If we couldn't fold both operands into the address at the same time,
     // see if we can just put each operand into a register and fold at least
     // the add.
@@ -939,6 +968,7 @@
       AM.Scale = 1;
       return false;
     }
+    } // @LOCALMOD
     N = Handle.getValue();
     break;
   }
@@ -1093,7 +1123,15 @@
 /// MatchAddressBase - Helper for MatchAddress. Add the specified node to the
 /// specified addressing mode without any further recursion.
 bool X86DAGToDAGISel::MatchAddressBase(SDValue N, X86ISelAddressMode &AM) {
-  // Is the base register already occupied?
+  if (RestrictUseOfBaseReg()) { // @LOCALMOD
+    if (AM.IndexReg.getNode() == 0) {
+      AM.IndexReg = N;
+      AM.Scale = 1;
+      return false;
+    }
+    return true;
+  } // @LOCALMOD
+// Is the base register already occupied?
   if (AM.BaseType != X86ISelAddressMode::RegBase || AM.Base_Reg.getNode()) {
     // If so, check to see if the scale index register is set.
     if (AM.IndexReg.getNode() == 0) {
@@ -1123,6 +1161,8 @@
                                  SDValue &Scale, SDValue &Index,
                                  SDValue &Disp, SDValue &Segment) {
   X86ISelAddressMode AM;
+  // @LOCALMOD
+  selectingMemOp = true;
   
   if (Parent &&
       // This list of opcodes are all the nodes that have an "addr:$ptr" operand
@@ -1143,6 +1183,15 @@
     return false;
 
   EVT VT = N.getValueType();
+
+  // @LOCALMOD-START
+  if (Subtarget->isTargetNaCl64()) {
+    // NaCl needs to zero the top 32-bits of the index, so we can't
+    // allow the index register to be negative.
+    LegalizeIndexForNaCl(N, AM);
+  }
+  // @LOCALMOD-END
+  
   if (AM.BaseType == X86ISelAddressMode::RegBase) {
     if (!AM.Base_Reg.getNode())
       AM.Base_Reg = CurDAG->getRegister(0, VT);
@@ -1214,6 +1263,8 @@
   SDValue Copy = AM.Segment;
   SDValue T = CurDAG->getRegister(0, MVT::i32);
   AM.Segment = T;
+  // @LOCALMOD
+  selectingMemOp = false;
   if (MatchAddress(N, AM))
     return false;
   assert (T == AM.Segment);
@@ -1277,7 +1328,8 @@
   AM.Base_Reg = CurDAG->getRegister(0, N.getValueType());
   AM.SymbolFlags = GA->getTargetFlags();
 
-  if (N.getValueType() == MVT::i32) {
+  if (N.getValueType() == MVT::i32 && 
+      !Subtarget->isTargetNaCl64()) {   // @LOCALMOD
     AM.Scale = 1;
     AM.IndexReg = CurDAG->getRegister(X86::EBX, MVT::i32);
   } else {
@@ -1302,6 +1354,111 @@
                     N.getOperand(1), Base, Scale, Index, Disp, Segment);
 }
 
+// @LOCALMOD-BEGIN
+// LegalizeIndexForNaCl - NaCl specific addressing fix
+//
+//   Because NaCl needs to zero the top 32-bits of the index, we can't
+//   allow the index register to be negative. However, if we are using a base
+//   frame index, global address or the constant pool, and AM.Disp > 0, then
+//   negative values of "index" may be expected to legally occur.
+//   To avoid this, we fold the displacement (and scale) back into the 
+//   index. This results in a LEA before the current instruction.
+//   Unfortunately, this may add a requirement for an additional register.
+//
+//   For example, this sandboxed code is broken if %eax is negative:
+//
+//     movl %eax,%eax
+//     incl -30(%rbp,%rax,4)
+//
+//   Instead, we now generate:
+//     leal -30(%rbp,%rax,4), %tmp
+//     movl %tmp,%tmp
+//     incl (%r15,%tmp,1)
+//
+//  TODO(espindola): This might not be complete since the matcher can select
+//  any dag node to go in the index. This is also not how the rest of the
+//  matcher logic works, if the matcher selects something, it must be
+//  valid and not depend on further patching. A more desirable fix is
+//  probably to update the matching code to avoid assigning a register
+//  to a value that we cannot prove is positive.
+void X86DAGToDAGISel::LegalizeIndexForNaCl(SDValue N, X86ISelAddressMode &AM) {
+
+
+  if (AM.isRIPRelative())
+    return;
+
+  // MatchAddress wants to use the base register when there's only
+  // one register and no scale. We need to use the index register instead.
+  if (AM.BaseType == X86ISelAddressMode::RegBase &&
+      AM.Base_Reg.getNode() &&
+      !AM.IndexReg.getNode()) {
+    AM.IndexReg = AM.Base_Reg;
+    AM.setBaseReg(SDValue());
+  }
+
+  // Case 1: Prevent negative indexes
+  bool NeedsFixing1 =
+       (AM.BaseType == X86ISelAddressMode::FrameIndexBase || AM.GV || AM.CP) &&
+       AM.IndexReg.getNode() && 
+       AM.Disp > 0;
+
+  // Case 2: Both index and base registers are being used
+  bool NeedsFixing2 =
+       (AM.BaseType == X86ISelAddressMode::RegBase) &&
+       AM.Base_Reg.getNode() &&
+       AM.IndexReg.getNode();
+
+  if (!NeedsFixing1 && !NeedsFixing2) 
+    return;
+
+  DebugLoc dl = N->getDebugLoc();
+  static const unsigned LogTable[] = { ~0, 0, 1, ~0, 2, ~0, ~0, ~0, 3 };
+  assert(AM.Scale < sizeof(LogTable)/sizeof(LogTable[0]));
+  unsigned ScaleLog = LogTable[AM.Scale];
+  assert(ScaleLog <= 3);
+  SmallVector<SDNode*, 8> NewNodes;
+  
+  SDValue NewIndex = AM.IndexReg;
+  if (ScaleLog > 0) {
+    SDValue ShlCount = CurDAG->getConstant(ScaleLog, MVT::i8);
+    NewNodes.push_back(ShlCount.getNode());
+    SDValue ShlNode = CurDAG->getNode(ISD::SHL, dl, N.getValueType(),
+                                      NewIndex, ShlCount);
+    NewNodes.push_back(ShlNode.getNode());
+    NewIndex = ShlNode;
+  }
+  if (AM.Disp > 0) {
+    SDValue DispNode = CurDAG->getConstant(AM.Disp, N.getValueType());
+    NewNodes.push_back(DispNode.getNode());
+
+    SDValue AddNode = CurDAG->getNode(ISD::ADD, dl, N.getValueType(), 
+                                  NewIndex, DispNode);
+    NewNodes.push_back(AddNode.getNode());
+    NewIndex = AddNode;
+  }
+
+  if (NeedsFixing2) {
+    SDValue AddBase = CurDAG->getNode(ISD::ADD, dl, N.getValueType(),
+                                      NewIndex, AM.Base_Reg); 
+    NewNodes.push_back(AddBase.getNode());
+    NewIndex = AddBase;
+    AM.setBaseReg(SDValue());
+  }
+  AM.Disp = 0;
+  AM.Scale = 1;
+  AM.IndexReg = NewIndex;
+
+  // Insert the new nodes into the topological ordering.
+  for (unsigned i=0; i < NewNodes.size(); i++) {
+    if (NewNodes[i]->getNodeId() == -1 ||
+        NewNodes[i]->getNodeId() > N.getNode()->getNodeId()) {
+      CurDAG->RepositionNode(N.getNode(), NewNodes[i]);
+      NewNodes[i]->setNodeId(N.getNode()->getNodeId());
+    }
+  }
+}
+// @LOCALMOD-END
+
 /// getGlobalBaseReg - Return an SDNode that returns the value of
 /// the global base register. Output instructions required to
 /// initialize the global base register, if necessary.
diff -r 2b13dadc8fed lib/Target/X86/X86ISelLowering.cpp
--- a/lib/Target/X86/X86ISelLowering.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86ISelLowering.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -216,7 +216,9 @@
   Subtarget = &TM.getSubtarget<X86Subtarget>();
   X86ScalarSSEf64 = Subtarget->hasXMMInt();
   X86ScalarSSEf32 = Subtarget->hasXMM();
-  X86StackPtr = Subtarget->is64Bit() ? X86::RSP : X86::ESP;
+  // @LOCALMOD-START
+  X86StackPtr = Subtarget->has64BitPointers() ? X86::RSP : X86::ESP;
+  // @LOCALMOD-END
 
   RegInfo = TM.getRegisterInfo();
   TD = getTargetData();
@@ -522,7 +524,7 @@
   setOperationAction(ISD::EHSELECTION,   MVT::i64, Expand);
   setOperationAction(ISD::EXCEPTIONADDR, MVT::i32, Expand);
   setOperationAction(ISD::EHSELECTION,   MVT::i32, Expand);
-  if (Subtarget->is64Bit()) {
+  if (Subtarget->has64BitPointers()) {
     setExceptionPointerRegister(X86::RAX);
     setExceptionSelectorRegister(X86::RDX);
   } else {
@@ -539,7 +541,7 @@
   // VASTART needs to be custom lowered to use the VarArgsFrameIndex
   setOperationAction(ISD::VASTART           , MVT::Other, Custom);
   setOperationAction(ISD::VAEND             , MVT::Other, Expand);
-  if (Subtarget->is64Bit()) {
+  if (Subtarget->is64Bit() && !Subtarget->isTargetWin64()) {
     setOperationAction(ISD::VAARG           , MVT::Other, Custom);
     setOperationAction(ISD::VACOPY          , MVT::Other, Custom);
   } else {
@@ -1430,7 +1432,16 @@
            "SRetReturnReg should have been set in LowerFormalArguments().");
     SDValue Val = DAG.getCopyFromReg(Chain, dl, Reg, getPointerTy());
 
-    Chain = DAG.getCopyToReg(Chain, dl, X86::RAX, Val, Flag);
+    // @LOCALMOD-START
+    if (Subtarget->isTargetNaCl()) {
+      // NaCl 64 uses 32-bit pointers, so there might be some zero-ext needed.
+      SDValue Zext = DAG.getZExtOrTrunc(Val, dl, MVT::i64);
+      Chain = DAG.getCopyToReg(Chain, dl, X86::RAX, Zext, Flag);
+    } else {
+      Chain = DAG.getCopyToReg(Chain, dl, X86::RAX, Val, Flag);
+    }
+    // @LOCALMOD-END
+
     Flag = Chain.getValue(1);
 
     // RAX now acts like a return value.
@@ -1664,6 +1675,13 @@
       Fn->getName() == "main")
     FuncInfo->setForceFramePointer(true);
 
+
+  // @LOCALMOD-START
+  if (Subtarget->isTargetNaCl64()) {
+    FuncInfo->setForceFramePointer(true);
+  }
+  // @LOCALMOD-END
+  
   MachineFrameInfo *MFI = MF.getFrameInfo();
   bool Is64Bit = Subtarget->is64Bit();
   bool IsWin64 = Subtarget->isTargetWin64();
@@ -1756,7 +1774,9 @@
     X86MachineFunctionInfo *FuncInfo = MF.getInfo<X86MachineFunctionInfo>();
     unsigned Reg = FuncInfo->getSRetReturnReg();
     if (!Reg) {
-      Reg = MF.getRegInfo().createVirtualRegister(getRegClassFor(MVT::i64));
+      // @LOCALMOD
+      Reg = MF.getRegInfo().createVirtualRegister(
+          getRegClassFor(getPointerTy()));
       FuncInfo->setSRetReturnReg(Reg);
     }
     SDValue Copy = DAG.getCopyToReg(DAG.getEntryNode(), dl, Reg, InVals[0]);
@@ -2420,7 +2440,8 @@
   unsigned StackAlignment = TFI.getStackAlignment();
   uint64_t AlignMask = StackAlignment - 1;
   int64_t Offset = StackSize;
-  uint64_t SlotSize = TD->getPointerSize();
+  // @LOCALMOD
+  uint64_t SlotSize = Subtarget->is64Bit() ? 8 : 4;
   if ( (Offset & AlignMask) <= (StackAlignment - SlotSize) ) {
     // Number smaller than 12 so just add the difference.
     Offset += ((StackAlignment - SlotSize) - (Offset & AlignMask));
@@ -2782,13 +2803,16 @@
 
   if (ReturnAddrIndex == 0) {
     // Set up a frame object for the return address.
-    uint64_t SlotSize = TD->getPointerSize();
+    // @LOCALMOD-START
+    uint64_t SlotSize = Subtarget->is64Bit() ? 8 : 4;
     ReturnAddrIndex = MF.getFrameInfo()->CreateFixedObject(SlotSize, -SlotSize,
                                                            false);
     FuncInfo->setRAIndex(ReturnAddrIndex);
   }
 
-  return DAG.getFrameIndex(ReturnAddrIndex, getPointerTy());
+  return DAG.getFrameIndex(ReturnAddrIndex,
+                           Subtarget->is64Bit() ? MVT::i64 : MVT::i32);
+  // @LOCALMOD-END
 }
 
 
@@ -6420,6 +6444,15 @@
                     X86::RAX, X86II::MO_TLSGD);
 }
 
+// @LOCALMOD-START
+static SDValue
+LowerToTLSNaClModel64(GlobalAddressSDNode *GA, SelectionDAG &DAG,
+                                const EVT PtrVT) {
+  return GetTLSADDR(DAG, DAG.getEntryNode(), GA, NULL, PtrVT,
+                    X86::EAX, X86II::MO_TPOFF);
+}
+// @LOCALMOD-END
+
 // Lower ISD::GlobalTLSAddress using the "initial exec" (for no-pic) or
 // "local exec" model.
 static SDValue LowerToTLSExecModel(GlobalAddressSDNode *GA, SelectionDAG &DAG,
@@ -6484,6 +6517,11 @@
     TLSModel::Model model
       = getTLSModel(GV, getTargetMachine().getRelocationModel());
 
+    // @LOCAMOD-START
+    if (Subtarget->isTargetNaCl64())
+      return LowerToTLSNaClModel64(GA, DAG, getPointerTy());
+    // @LOCALMOD-END
+
     switch (model) {
       case TLSModel::GeneralDynamic:
       case TLSModel::LocalDynamic: // not implemented
@@ -7297,6 +7335,10 @@
 /// if it's possible.
 SDValue X86TargetLowering::LowerToBT(SDValue And, ISD::CondCode CC,
                                      DebugLoc dl, SelectionDAG &DAG) const {
+   // @LOCALMOD: NaCl validator rejects BT, BTS, and BTC.
+  if (Subtarget->isTargetNaCl())
+    return SDValue();
+  
   SDValue Op0 = And.getOperand(0);
   SDValue Op1 = And.getOperand(1);
   if (Op0.getOpcode() == ISD::TRUNCATE)
@@ -7869,7 +7911,8 @@
 
   SDValue Flag;
 
-  EVT SPTy = Subtarget->is64Bit() ? MVT::i64 : MVT::i32;
+  // LOCALMOD
+  EVT SPTy = getPointerTy();
 
   Chain = DAG.getCopyToReg(Chain, dl, X86::EAX, Size, Flag);
   Flag = Chain.getValue(1);
@@ -7928,7 +7971,7 @@
   FIN = DAG.getNode(ISD::ADD, DL, getPointerTy(),
                     FIN, DAG.getIntPtrConstant(4));
   SDValue OVFIN = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),
-                                    getPointerTy());
+                                    MVT::i64);
   Store = DAG.getStore(Op.getOperand(0), DL, OVFIN, FIN,
                        MachinePointerInfo(SV, 8),
                        false, false, 0);
@@ -7938,7 +7981,7 @@
   FIN = DAG.getNode(ISD::ADD, DL, getPointerTy(),
                     FIN, DAG.getIntPtrConstant(8));
   SDValue RSFIN = DAG.getFrameIndex(FuncInfo->getRegSaveFrameIndex(),
-                                    getPointerTy());
+                                    MVT::i64);
   Store = DAG.getStore(Op.getOperand(0), DL, RSFIN, FIN,
                        MachinePointerInfo(SV, 16), false, false, 0);
   MemOps.push_back(Store);
@@ -7950,7 +7993,8 @@
   assert(Subtarget->is64Bit() &&
          "LowerVAARG only handles 64-bit va_arg!");
   assert((Subtarget->isTargetLinux() ||
-          Subtarget->isTargetDarwin()) &&
+          Subtarget->isTargetDarwin() ||
+          Subtarget->isTargetNaCl()) && // @LOCALMOD
           "Unhandled target in LowerVAARG");
   assert(Op.getNode()->getNumOperands() == 4);
   SDValue Chain = Op.getOperand(0);
@@ -8338,7 +8382,7 @@
   EVT VT = Op.getValueType();
   DebugLoc dl = Op.getDebugLoc();  // FIXME probably not meaningful
   unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue();
-  unsigned FrameReg = Subtarget->is64Bit() ? X86::RBP : X86::EBP;
+  unsigned FrameReg = Subtarget->has64BitPointers() ? X86::RBP : X86::EBP;
   SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), dl, FrameReg, VT);
   while (Depth--)
     FrameAddr = DAG.getLoad(VT, dl, DAG.getEntryNode(), FrameAddr,
@@ -8349,7 +8393,10 @@
 
 SDValue X86TargetLowering::LowerFRAME_TO_ARGS_OFFSET(SDValue Op,
                                                      SelectionDAG &DAG) const {
-  return DAG.getIntPtrConstant(2*TD->getPointerSize());
+  // @LOCALMOD-START
+  int SlotSize = Subtarget->is64Bit() ? 8 : 4;
+  return DAG.getIntPtrConstant(2*SlotSize);
+  // @LOCALMOD-END
 }
 
 SDValue X86TargetLowering::LowerEH_RETURN(SDValue Op, SelectionDAG &DAG) const {
@@ -8358,14 +8405,17 @@
   SDValue Offset    = Op.getOperand(1);
   SDValue Handler   = Op.getOperand(2);
   DebugLoc dl       = Op.getDebugLoc();
+  // @LOCALMOD-START
+  bool Has64BitPtrs = Subtarget->has64BitPointers();
 
   SDValue Frame = DAG.getCopyFromReg(DAG.getEntryNode(), dl,
-                                     Subtarget->is64Bit() ? X86::RBP : X86::EBP,
+                                     Has64BitPtrs ? X86::RBP : X86::EBP,
                                      getPointerTy());
-  unsigned StoreAddrReg = (Subtarget->is64Bit() ? X86::RCX : X86::ECX);
-
+  unsigned StoreAddrReg = (Has64BitPtrs ? X86::RCX : X86::ECX);
+  int SlotSize = Subtarget->is64Bit() ? 8 : 4;
   SDValue StoreAddr = DAG.getNode(ISD::ADD, dl, getPointerTy(), Frame,
-                                  DAG.getIntPtrConstant(TD->getPointerSize()));
+                                  DAG.getIntPtrConstant(SlotSize));
+  // @LOCALMOD-END
   StoreAddr = DAG.getNode(ISD::ADD, dl, getPointerTy(), StoreAddr, Offset);
   Chain = DAG.getStore(Chain, dl, Handler, StoreAddr, MachinePointerInfo(),
                        false, false, 0);
@@ -9994,6 +10044,15 @@
   MachineBasicBlock *overflowMBB;
   MachineBasicBlock *offsetMBB;
   MachineBasicBlock *endMBB;
+  
+  // @LOCALMOD-BEGIN
+  unsigned RealDestReg = 0;
+  if (Subtarget->isTargetNaCl()) {
+    // Pretend pointers are 64-bit until the end
+    RealDestReg = DestReg;
+    DestReg = MRI.createVirtualRegister(AddrRegClass);
+  }
+  // @LOCALMOD-END
 
   unsigned OffsetDestReg = 0;    // Argument address computed by offsetMBB
   unsigned OverflowDestReg = 0;  // Argument address computed by overflowMBB
@@ -10179,6 +10238,19 @@
       .addReg(OffsetDestReg).addMBB(offsetMBB)
       .addReg(OverflowDestReg).addMBB(overflowMBB);
   }
+  
+  // @LOCALMOD-BEGIN
+  if (Subtarget->isTargetNaCl()) {
+    // Truncate the address to 32-bits
+    MachineBasicBlock::iterator I = endMBB->begin();
+    ++I;
+    MachineInstr *CopyMI =
+      BuildMI(*endMBB, I, DL,
+              TII->get(TargetOpcode::COPY), RealDestReg)
+        .addReg(DestReg);
+    CopyMI->getOperand(1).setSubReg(X86::sub_32bit);
+  }
+  // @LOCALMOD-END
 
   // Erase the pseudo instruction
   MI->eraseFromParent();
@@ -10726,6 +10798,7 @@
     return EmitVAStartSaveXMMRegsWithCustomInserter(MI, BB);
 
   case X86::VAARG_64:
+  case X86::NACL_CG_VAARG_64:
     return EmitVAARG64WithCustomInserter(MI, BB);
   }
 }
diff -r 2b13dadc8fed lib/Target/X86/X86InstrArithmetic.td
--- a/lib/Target/X86/X86InstrArithmetic.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86InstrArithmetic.td	Thu Jun 09 18:06:40 2011 -0700
@@ -30,8 +30,9 @@
                   "lea{l}\t{$src|$dst}, {$dst|$src}",
                   [(set GR32:$dst, lea32addr:$src)]>, Requires<[In64BitMode]>;
 
+// @LOCALMOD (lea64mem)
 let isReMaterializable = 1 in
-def LEA64r   : RI<0x8D, MRMSrcMem, (outs GR64:$dst), (ins i64mem:$src),
+def LEA64r   : RI<0x8D, MRMSrcMem, (outs GR64:$dst), (ins lea64mem:$src),
                   "lea{q}\t{$src|$dst}, {$dst|$src}",
                   [(set GR64:$dst, lea64addr:$src)]>;
 
diff -r 2b13dadc8fed lib/Target/X86/X86InstrCompiler.td
--- a/lib/Target/X86/X86InstrCompiler.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86InstrCompiler.td	Thu Jun 09 18:06:40 2011 -0700
@@ -92,8 +92,8 @@
                  "#VAARG_64 $dst, $ap, $size, $mode, $align",
                  [(set GR64:$dst,
                     (X86vaarg64 addr:$ap, imm:$size, imm:$mode, imm:$align)),
-                  (implicit EFLAGS)]>;
-
+                  (implicit EFLAGS)]>,
+                 Requires<[NotNaCl]>;
 // Dynamic stack allocation yields a _chkstk or _alloca call for all Windows
 // targets.  These calls are needed to probe the stack when allocating more than
 // 4k bytes in one go. Touching the stack at 4K increments is necessary to
@@ -263,7 +263,7 @@
 def TLS_addr32 : I<0, Pseudo, (outs), (ins i32mem:$sym),
                   "# TLS_addr32",
                   [(X86tlsaddr tls32addr:$sym)]>,
-                  Requires<[In32BitMode]>;
+                  Requires<[In32BitMode, NotNaCl]>;
 
 // All calls clobber the non-callee saved registers. RSP is marked as
 // a use to prevent stack-pointer assignments that appear immediately
@@ -837,9 +837,9 @@
 // Direct PC relative function call for small code model. 32-bit displacement
 // sign extended to 64-bit.
 def : Pat<(X86call (i64 tglobaladdr:$dst)),
-          (CALL64pcrel32 tglobaladdr:$dst)>, Requires<[NotWin64]>;
+          (CALL64pcrel32 tglobaladdr:$dst)>, Requires<[NotWin64, NotNaCl]>;
 def : Pat<(X86call (i64 texternalsym:$dst)),
-          (CALL64pcrel32 texternalsym:$dst)>, Requires<[NotWin64]>;
+          (CALL64pcrel32 texternalsym:$dst)>, Requires<[NotWin64, NotNaCl]>;
 
 def : Pat<(X86call (i64 tglobaladdr:$dst)),
           (WINCALL64pcrel32 tglobaladdr:$dst)>, Requires<[IsWin64]>;
@@ -856,7 +856,7 @@
 // callee-saved register.
 def : Pat<(X86tcret (load addr:$dst), imm:$off),
           (TCRETURNmi addr:$dst, imm:$off)>,
-          Requires<[In32BitMode, IsNotPIC]>;
+	  Requires<[In32BitMode, IsNotPIC, NotNaCl]>;
 
 def : Pat<(X86tcret (i32 tglobaladdr:$dst), imm:$off),
           (TCRETURNdi texternalsym:$dst, imm:$off)>,
@@ -868,27 +868,27 @@
 
 def : Pat<(X86tcret ptr_rc_tailcall:$dst, imm:$off),
           (TCRETURNri64 ptr_rc_tailcall:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+	  Requires<[In64BitMode, NotNaCl]>;
 
 def : Pat<(X86tcret (load addr:$dst), imm:$off),
           (TCRETURNmi64 addr:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+	  Requires<[In64BitMode, NotNaCl]>;
 
 def : Pat<(X86tcret (i64 tglobaladdr:$dst), imm:$off),
           (TCRETURNdi64 tglobaladdr:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+	  Requires<[In64BitMode, NotNaCl]>;
 
 def : Pat<(X86tcret (i64 texternalsym:$dst), imm:$off),
           (TCRETURNdi64 texternalsym:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+	  Requires<[In64BitMode, NotNaCl]>;
 
 // Normal calls, with various flavors of addresses.
 def : Pat<(X86call (i32 tglobaladdr:$dst)),
-          (CALLpcrel32 tglobaladdr:$dst)>;
+          (CALLpcrel32 tglobaladdr:$dst)>, Requires<[NotNaCl]>;
 def : Pat<(X86call (i32 texternalsym:$dst)),
-          (CALLpcrel32 texternalsym:$dst)>;
+          (CALLpcrel32 texternalsym:$dst)>, Requires<[NotNaCl]>;
 def : Pat<(X86call (i32 imm:$dst)),
-          (CALLpcrel32 imm:$dst)>, Requires<[CallImmAddr]>;
+          (CALLpcrel32 imm:$dst)>, Requires<[CallImmAddr, NotNaCl]>;
 
 // Comparisons.
 
@@ -1302,19 +1302,19 @@
           (MOV8mr_NOREX
             addr:$dst,
             (EXTRACT_SUBREG (i64 (COPY_TO_REGCLASS GR64:$src, GR64_ABCD)),
-                            sub_8bit_hi))>;
+                            sub_8bit_hi))>, Requires<[NotNaCl]>;
 def : Pat<(store (i8 (trunc_su (srl_su GR32:$src, (i8 8)))), addr:$dst),
           (MOV8mr_NOREX
             addr:$dst,
             (EXTRACT_SUBREG (i32 (COPY_TO_REGCLASS GR32:$src, GR32_ABCD)),
                             sub_8bit_hi))>,
-      Requires<[In64BitMode]>;
+      Requires<[In64BitMode, NotNaCl]>;
 def : Pat<(store (i8 (trunc_su (srl_su GR16:$src, (i8 8)))), addr:$dst),
           (MOV8mr_NOREX
             addr:$dst,
             (EXTRACT_SUBREG (i16 (COPY_TO_REGCLASS GR16:$src, GR16_ABCD)),
                             sub_8bit_hi))>,
-      Requires<[In64BitMode]>;
+      Requires<[In64BitMode, NotNaCl]>;
 
 
 // (shl x, 1) ==> (add x, x)
diff -r 2b13dadc8fed lib/Target/X86/X86InstrControl.td
--- a/lib/Target/X86/X86InstrControl.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86InstrControl.td	Thu Jun 09 18:06:40 2011 -0700
@@ -101,7 +101,7 @@
   def JMP32r     : I<0xFF, MRM4r, (outs), (ins GR32:$dst), "jmp{l}\t{*}$dst",
                      [(brind GR32:$dst)]>, Requires<[In32BitMode]>;
   def JMP32m     : I<0xFF, MRM4m, (outs), (ins i32mem:$dst), "jmp{l}\t{*}$dst",
-                     [(brind (loadi32 addr:$dst))]>, Requires<[In32BitMode]>;
+                     [(brind (loadi32 addr:$dst))]>, Requires<[In32BitMode,NotNaCl]>;
 
   def JMP64r     : I<0xFF, MRM4r, (outs), (ins GR64:$dst), "jmp{q}\t{*}$dst",
                      [(brind GR64:$dst)]>, Requires<[In64BitMode]>;
@@ -115,7 +115,7 @@
                           (ins i32imm:$off, i16imm:$seg),
                           "ljmp{l}\t{$seg, $off|$off, $seg}", []>;
   def FARJMP64   : RI<0xFF, MRM5m, (outs), (ins opaque80mem:$dst),
-                      "ljmp{q}\t{*}$dst", []>;
+                      "ljmp{q}\t{*}$dst", Requires<[NotNaCl]>;
 
   def FARJMP16m  : I<0xFF, MRM5m, (outs), (ins opaque32mem:$dst),
                      "ljmp{w}\t{*}$dst", []>, OpSize;
@@ -145,13 +145,14 @@
       Uses = [ESP] in {
     def CALLpcrel32 : Ii32PCRel<0xE8, RawFrm,
                            (outs), (ins i32imm_pcrel:$dst,variable_ops),
-                           "call{l}\t$dst", []>, Requires<[In32BitMode]>;
+                           "call{l}\t$dst", []>,
+                      Requires<[In32BitMode, NotNaCl]>;
     def CALL32r     : I<0xFF, MRM2r, (outs), (ins GR32:$dst, variable_ops),
                         "call{l}\t{*}$dst", [(X86call GR32:$dst)]>,
-                         Requires<[In32BitMode]>;
+                         Requires<[In32BitMode, NotNaCl]>;
     def CALL32m     : I<0xFF, MRM2m, (outs), (ins i32mem:$dst, variable_ops),
                         "call{l}\t{*}$dst", [(X86call (loadi32 addr:$dst))]>,
-                        Requires<[In32BitMode]>;
+                        Requires<[In32BitMode, NotNaCl]>;
 
     def FARCALL16i  : Iseg16<0x9A, RawFrmImm16, (outs),
                              (ins i16imm:$off, i16imm:$seg),
@@ -169,7 +170,8 @@
     let isAsmParserOnly = 1 in
       def CALLpcrel16 : Ii16PCRel<0xE8, RawFrm,
                        (outs), (ins i16imm_pcrel:$dst, variable_ops),
-                       "callw\t$dst", []>, OpSize;
+                       "callw\t$dst", []>, OpSize,
+                       Requires<[NotNaCl]>; // @LOCALMOD
   }
 
 
@@ -200,7 +202,7 @@
                    "", []>;  // FIXME: Remove encoding when JIT is dead.
   let mayLoad = 1 in
   def TAILJMPm : I<0xFF, MRM4m, (outs), (ins i32mem_TC:$dst, variable_ops),
-                   "jmp{l}\t{*}$dst  # TAILCALL", []>;
+                   "jmp{l}\t{*}$dst  # TAILCALL", []>, Requires<[NotNaCl]>;
 }
 
 
@@ -225,16 +227,16 @@
     def CALL64pcrel32 : Ii32PCRel<0xE8, RawFrm,
                           (outs), (ins i64i32imm_pcrel:$dst, variable_ops),
                           "call{q}\t$dst", []>,
-                        Requires<[In64BitMode, NotWin64]>;
+                        Requires<[In64BitMode, NotWin64, NotNaCl]>;
     def CALL64r       : I<0xFF, MRM2r, (outs), (ins GR64:$dst, variable_ops),
                           "call{q}\t{*}$dst", [(X86call GR64:$dst)]>,
-                        Requires<[In64BitMode, NotWin64]>;
+                        Requires<[In64BitMode, NotWin64, NotNaCl]>;
     def CALL64m       : I<0xFF, MRM2m, (outs), (ins i64mem:$dst, variable_ops),
                           "call{q}\t{*}$dst", [(X86call (loadi64 addr:$dst))]>,
-                        Requires<[In64BitMode, NotWin64]>;
+                        Requires<[In64BitMode, NotWin64, NotNaCl]>;
 
     def FARCALL64   : RI<0xFF, MRM3m, (outs), (ins opaque80mem:$dst),
-                         "lcall{q}\t{*}$dst", []>;
+                         "lcall{q}\t{*}$dst", []>, Requires<[NotNaCl]>;
   }
 
   // FIXME: We need to teach codegen about single list of call-clobbered
@@ -290,5 +292,6 @@
 
   let mayLoad = 1 in
   def TAILJMPm64 : I<0xFF, MRM4m, (outs), (ins i64mem_TC:$dst, variable_ops),
-                     "jmp{q}\t{*}$dst  # TAILCALL", []>;
+                     "jmp{q}\t{*}$dst  # TAILCALL", []>,
+                   Requires<[NotNaCl]>;
 }
diff -r 2b13dadc8fed lib/Target/X86/X86InstrFormats.td
--- a/lib/Target/X86/X86InstrFormats.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86InstrFormats.td	Thu Jun 09 18:06:40 2011 -0700
@@ -41,6 +41,7 @@
 def MRM_F9 : Format<42>;
 def RawFrmImm8 : Format<43>;
 def RawFrmImm16 : Format<44>;
+def CustomFrm : Format<62>; // @LOCALMOD 
 
 // ImmType - This specifies the immediate type used by an instruction. This is
 // part of the ad-hoc solution used to emit machine instruction encodings by our
diff -r 2b13dadc8fed lib/Target/X86/X86InstrInfo.cpp
--- a/lib/Target/X86/X86InstrInfo.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86InstrInfo.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -245,86 +245,93 @@
     MemOp2RegOpTable[MemOp] = std::make_pair(RegOp, AuxInfo);
   }
 
+  // NaCl needs indirect calls to go through a reg to align the target,
+  // so, skip Reg2Mem in some cases.
+  bool isNaCl = TM.getSubtarget<X86Subtarget>().isTargetNaCl();
+
   // If the third value is 1, then it's folding either a load or a store.
-  static const unsigned OpTbl0[][4] = {
-    { X86::BT16ri8,     X86::BT16mi8, 1, 0 },
-    { X86::BT32ri8,     X86::BT32mi8, 1, 0 },
-    { X86::BT64ri8,     X86::BT64mi8, 1, 0 },
-    { X86::CALL32r,     X86::CALL32m, 1, 0 },
-    { X86::CALL64r,     X86::CALL64m, 1, 0 },
-    { X86::WINCALL64r,  X86::WINCALL64m, 1, 0 },
-    { X86::CMP16ri,     X86::CMP16mi, 1, 0 },
-    { X86::CMP16ri8,    X86::CMP16mi8, 1, 0 },
-    { X86::CMP16rr,     X86::CMP16mr, 1, 0 },
-    { X86::CMP32ri,     X86::CMP32mi, 1, 0 },
-    { X86::CMP32ri8,    X86::CMP32mi8, 1, 0 },
-    { X86::CMP32rr,     X86::CMP32mr, 1, 0 },
-    { X86::CMP64ri32,   X86::CMP64mi32, 1, 0 },
-    { X86::CMP64ri8,    X86::CMP64mi8, 1, 0 },
-    { X86::CMP64rr,     X86::CMP64mr, 1, 0 },
-    { X86::CMP8ri,      X86::CMP8mi, 1, 0 },
-    { X86::CMP8rr,      X86::CMP8mr, 1, 0 },
-    { X86::DIV16r,      X86::DIV16m, 1, 0 },
-    { X86::DIV32r,      X86::DIV32m, 1, 0 },
-    { X86::DIV64r,      X86::DIV64m, 1, 0 },
-    { X86::DIV8r,       X86::DIV8m, 1, 0 },
-    { X86::EXTRACTPSrr, X86::EXTRACTPSmr, 0, 16 },
-    { X86::FsMOVAPDrr,  X86::MOVSDmr | TB_NOT_REVERSABLE , 0, 0 },
-    { X86::FsMOVAPSrr,  X86::MOVSSmr | TB_NOT_REVERSABLE , 0, 0 },
-    { X86::IDIV16r,     X86::IDIV16m, 1, 0 },
-    { X86::IDIV32r,     X86::IDIV32m, 1, 0 },
-    { X86::IDIV64r,     X86::IDIV64m, 1, 0 },
-    { X86::IDIV8r,      X86::IDIV8m, 1, 0 },
-    { X86::IMUL16r,     X86::IMUL16m, 1, 0 },
-    { X86::IMUL32r,     X86::IMUL32m, 1, 0 },
-    { X86::IMUL64r,     X86::IMUL64m, 1, 0 },
-    { X86::IMUL8r,      X86::IMUL8m, 1, 0 },
-    { X86::JMP32r,      X86::JMP32m, 1, 0 },
-    { X86::JMP64r,      X86::JMP64m, 1, 0 },
-    { X86::MOV16ri,     X86::MOV16mi, 0, 0 },
-    { X86::MOV16rr,     X86::MOV16mr, 0, 0 },
-    { X86::MOV32ri,     X86::MOV32mi, 0, 0 },
-    { X86::MOV32rr,     X86::MOV32mr, 0, 0 },
-    { X86::MOV64ri32,   X86::MOV64mi32, 0, 0 },
-    { X86::MOV64rr,     X86::MOV64mr, 0, 0 },
-    { X86::MOV8ri,      X86::MOV8mi, 0, 0 },
-    { X86::MOV8rr,      X86::MOV8mr, 0, 0 },
-    { X86::MOV8rr_NOREX, X86::MOV8mr_NOREX, 0, 0 },
-    { X86::MOVAPDrr,    X86::MOVAPDmr, 0, 16 },
-    { X86::MOVAPSrr,    X86::MOVAPSmr, 0, 16 },
-    { X86::MOVDQArr,    X86::MOVDQAmr, 0, 16 },
-    { X86::MOVPDI2DIrr, X86::MOVPDI2DImr, 0, 0 },
-    { X86::MOVPQIto64rr,X86::MOVPQI2QImr, 0, 0 },
-    { X86::MOVSDto64rr, X86::MOVSDto64mr, 0, 0 },
-    { X86::MOVSS2DIrr,  X86::MOVSS2DImr, 0, 0 },
-    { X86::MOVUPDrr,    X86::MOVUPDmr, 0, 0 },
-    { X86::MOVUPSrr,    X86::MOVUPSmr, 0, 0 },
-    { X86::MUL16r,      X86::MUL16m, 1, 0 },
-    { X86::MUL32r,      X86::MUL32m, 1, 0 },
-    { X86::MUL64r,      X86::MUL64m, 1, 0 },
-    { X86::MUL8r,       X86::MUL8m, 1, 0 },
-    { X86::SETAEr,      X86::SETAEm, 0, 0 },
-    { X86::SETAr,       X86::SETAm, 0, 0 },
-    { X86::SETBEr,      X86::SETBEm, 0, 0 },
-    { X86::SETBr,       X86::SETBm, 0, 0 },
-    { X86::SETEr,       X86::SETEm, 0, 0 },
-    { X86::SETGEr,      X86::SETGEm, 0, 0 },
-    { X86::SETGr,       X86::SETGm, 0, 0 },
-    { X86::SETLEr,      X86::SETLEm, 0, 0 },
-    { X86::SETLr,       X86::SETLm, 0, 0 },
-    { X86::SETNEr,      X86::SETNEm, 0, 0 },
-    { X86::SETNOr,      X86::SETNOm, 0, 0 },
-    { X86::SETNPr,      X86::SETNPm, 0, 0 },
-    { X86::SETNSr,      X86::SETNSm, 0, 0 },
-    { X86::SETOr,       X86::SETOm, 0, 0 },
-    { X86::SETPr,       X86::SETPm, 0, 0 },
-    { X86::SETSr,       X86::SETSm, 0, 0 },
-    { X86::TAILJMPr,    X86::TAILJMPm, 1, 0 },
-    { X86::TAILJMPr64,  X86::TAILJMPm64, 1, 0 },
-    { X86::TEST16ri,    X86::TEST16mi, 1, 0 },
-    { X86::TEST32ri,    X86::TEST32mi, 1, 0 },
-    { X86::TEST64ri32,  X86::TEST64mi32, 1, 0 },
-    { X86::TEST8ri,     X86::TEST8mi, 1, 0 }
+  // The fourth value is alignment.
+  // If the fifth value is true reg2mem is allowed.
+  // If the sixth value is true mem2reg is allowed.
+  static const unsigned OpTbl0[][6] = {
+    { X86::BT16ri8,       X86::BT16mi8,       1, 0,  true,    true },
+    { X86::BT32ri8,       X86::BT32mi8,       1, 0,  true,    true },
+    { X86::BT64ri8,       X86::BT64mi8,       1, 0,  true,    true },
+    { X86::CALL32r,       X86::CALL32m,       1, 0,  !isNaCl, true },
+    { X86::CALL64r,       X86::CALL64m,       1, 0,  !isNaCl, true },
+    { X86::WINCALL64r,    X86::WINCALL64m,    1, 0,  true,    true },
+    { X86::CMP16ri,       X86::CMP16mi,       1, 0,  true,    true },
+    { X86::CMP16ri8,      X86::CMP16mi8,      1, 0,  true,    true },
+    { X86::CMP16rr,       X86::CMP16mr,       1, 0,  true,    true },
+    { X86::CMP32ri,       X86::CMP32mi,       1, 0,  true,    true },
+    { X86::CMP32ri8,      X86::CMP32mi8,      1, 0,  true,    true },
+    { X86::CMP32rr,       X86::CMP32mr,       1, 0,  true,    true },
+    { X86::CMP64ri32,     X86::CMP64mi32,     1, 0,  true,    true },
+    { X86::CMP64ri8,      X86::CMP64mi8,      1, 0,  true,    true },
+    { X86::CMP64rr,       X86::CMP64mr,       1, 0,  true,    true },
+    { X86::CMP8ri,        X86::CMP8mi,        1, 0,  true,    true },
+    { X86::CMP8rr,        X86::CMP8mr,        1, 0,  true,    true },
+    { X86::DIV16r,        X86::DIV16m,        1, 0,  true,    true },
+    { X86::DIV32r,        X86::DIV32m,        1, 0,  true,    true },
+    { X86::DIV64r,        X86::DIV64m,        1, 0,  true,    true },
+    { X86::DIV8r,         X86::DIV8m,         1, 0,  true,    true },
+    { X86::EXTRACTPSrr,   X86::EXTRACTPSmr,   0, 16, true,    true },
+    { X86::FsMOVAPDrr,    X86::MOVSDmr | TB_NOT_REVERSABLE,       0, 0,  true,    false },
+    { X86::FsMOVAPSrr,    X86::MOVSSmr | TB_NOT_REVERSABLE,       0, 0,  true,    false },
+    { X86::IDIV16r,       X86::IDIV16m,       1, 0,  true,    true },
+    { X86::IDIV32r,       X86::IDIV32m,       1, 0,  true,    true },
+    { X86::IDIV64r,       X86::IDIV64m,       1, 0,  true,    true },
+    { X86::IDIV8r,        X86::IDIV8m,        1, 0,  true,    true },
+    { X86::IMUL16r,       X86::IMUL16m,       1, 0,  true,    true },
+    { X86::IMUL32r,       X86::IMUL32m,       1, 0,  true,    true },
+    { X86::IMUL64r,       X86::IMUL64m,       1, 0,  true,    true },
+    { X86::IMUL8r,        X86::IMUL8m,        1, 0,  true,    true },
+    { X86::JMP32r,        X86::JMP32m,        1, 0,  !isNaCl, true },
+    { X86::JMP64r,        X86::JMP64m,        1, 0,  !isNaCl, true },
+    { X86::MOV16ri,       X86::MOV16mi,       0, 0,  true,    true },
+    { X86::MOV16rr,       X86::MOV16mr,       0, 0,  true,    true },
+    { X86::MOV32ri,       X86::MOV32mi,       0, 0,  true,    true },
+    { X86::MOV32rr,       X86::MOV32mr,       0, 0,  true,    true },
+    { X86::MOV64ri32,     X86::MOV64mi32,     0, 0,  true,    true },
+    { X86::MOV64rr,       X86::MOV64mr,       0, 0,  true,    true },
+    { X86::MOV8ri,        X86::MOV8mi,        0, 0,  true,    true },
+    { X86::MOV8rr,        X86::MOV8mr,        0, 0,  true,    true },
+    { X86::MOV8rr_NOREX,  X86::MOV8mr_NOREX,  0, 0,  true,    true },
+    { X86::MOVAPDrr,      X86::MOVAPDmr,      0, 16, true,    true },
+    { X86::MOVAPSrr,      X86::MOVAPSmr,      0, 16, true,    true },
+    { X86::MOVDQArr,      X86::MOVDQAmr,      0, 16, true,    true },
+    { X86::MOVPDI2DIrr,   X86::MOVPDI2DImr,   0, 0,  true,    true },
+    { X86::MOVPQIto64rr,  X86::MOVPQI2QImr,   0, 0,  true,    true },
+    { X86::MOVSDto64rr,   X86::MOVSDto64mr,   0, 0,  true,    true },
+    { X86::MOVSS2DIrr,    X86::MOVSS2DImr,    0, 0,  true,    true },
+    { X86::MOVUPDrr,      X86::MOVUPDmr,      0, 0,  true,    true },
+    { X86::MOVUPSrr,      X86::MOVUPSmr,      0, 0,  true,    true },
+    { X86::MUL16r,        X86::MUL16m,        1, 0,  true,    true },
+    { X86::MUL32r,        X86::MUL32m,        1, 0,  true,    true },
+    { X86::MUL64r,        X86::MUL64m,        1, 0,  true,    true },
+    { X86::MUL8r,         X86::MUL8m,         1, 0,  true,    true },
+    { X86::SETAEr,        X86::SETAEm,        0, 0,  true,    true },
+    { X86::SETAr,         X86::SETAm,         0, 0,  true,    true },
+    { X86::SETBEr,        X86::SETBEm,        0, 0,  true,    true },
+    { X86::SETBr,         X86::SETBm,         0, 0,  true,    true },
+    { X86::SETEr,         X86::SETEm,         0, 0,  true,    true },
+    { X86::SETGEr,        X86::SETGEm,        0, 0,  true,    true },
+    { X86::SETGr,         X86::SETGm,         0, 0,  true,    true },
+    { X86::SETLEr,        X86::SETLEm,        0, 0,  true,    true },
+    { X86::SETLr,         X86::SETLm,         0, 0,  true,    true },
+    { X86::SETNEr,        X86::SETNEm,        0, 0,  true,    true },
+    { X86::SETNOr,        X86::SETNOm,        0, 0,  true,    true },
+    { X86::SETNPr,        X86::SETNPm,        0, 0,  true,    true },
+    { X86::SETNSr,        X86::SETNSm,        0, 0,  true,    true },
+    { X86::SETOr,         X86::SETOm,         0, 0,  true,    true },
+    { X86::SETPr,         X86::SETPm,         0, 0,  true,    true },
+    { X86::SETSr,         X86::SETSm,         0, 0,  true,    true },
+    { X86::TAILJMPr,      X86::TAILJMPm,      1, 0,  !isNaCl, true },
+    { X86::TAILJMPr64,    X86::TAILJMPm64,    1, 0,  !isNaCl, true },
+    { X86::TEST16ri,      X86::TEST16mi,      1, 0,  true,    true },
+    { X86::TEST32ri,      X86::TEST32mi,      1, 0,  true,    true },
+    { X86::TEST64ri32,    X86::TEST64mi32,    1, 0,  true,    true },
+    { X86::TEST8ri,       X86::TEST8mi,       1, 0,  true,    true }
   };
 
   for (unsigned i = 0, e = array_lengthof(OpTbl0); i != e; ++i) {
@@ -332,8 +339,12 @@
     unsigned MemOp      = OpTbl0[i][1] & ~TB_FLAGS;
     unsigned FoldedLoad = OpTbl0[i][2];
     unsigned Align      = OpTbl0[i][3];
+    unsigned AllowReg2Mem = OpTbl0[i][4];
+    unsigned AllowMem2Reg = OpTbl0[i][5];
+
     assert(!RegOp2MemOpTable0.count(RegOp) && "Duplicated entries?");
-    RegOp2MemOpTable0[RegOp] = std::make_pair(MemOp, Align);
+    if (AllowReg2Mem)
+      RegOp2MemOpTable0[RegOp] = std::make_pair(MemOp, Align);
 
     // If this is not a reversable operation (because there is a many->one)
     // mapping, don't insert the reverse of the operation into MemOp2RegOpTable.
@@ -2008,6 +2019,7 @@
 
   DEBUG(dbgs() << "Cannot copy " << RI.getName(SrcReg)
                << " to " << RI.getName(DestReg) << '\n');
+  MBB.dump();
   llvm_unreachable("Cannot emit physreg copy instruction");
 }
 
@@ -2033,6 +2045,7 @@
   case X86::GR32_NOREXRegClassID:
   case X86::GR32_NOSPRegClassID:
   case X86::GR32_TCRegClassID:
+  case X86::GR32_TC_64RegClassID: // @LOCALMOD
     return load ? X86::MOV32rm : X86::MOV32mr;
   case X86::GR16RegClassID:
   case X86::GR16_ABCDRegClassID:
diff -r 2b13dadc8fed lib/Target/X86/X86InstrInfo.h
--- a/lib/Target/X86/X86InstrInfo.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86InstrInfo.h	Thu Jun 09 18:06:40 2011 -0700
@@ -323,6 +323,7 @@
     /// manual, this operand is described as pntr16:32 and pntr16:16
     RawFrmImm16 = 44,
 
+    CustomFrm      = 62, // @LOCALMOD
     FormMask       = 63,
 
     //===------------------------------------------------------------------===//
@@ -543,6 +544,7 @@
     case X86II::MRMSrcReg:
     case X86II::RawFrmImm8:
     case X86II::RawFrmImm16:
+    case X86II::CustomFrm: // @LOCALMOD
        return -1;
     case X86II::MRMDestMem:
       return 0;
diff -r 2b13dadc8fed lib/Target/X86/X86InstrInfo.td
--- a/lib/Target/X86/X86InstrInfo.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86InstrInfo.td	Thu Jun 09 18:06:40 2011 -0700
@@ -383,6 +383,13 @@
   let ParserMatchClass = ImmSExti64i8AsmOperand;
 }
 
+// @LOCALMOD
+def lea64mem : Operand<i64> {
+  let PrintMethod = "printi64mem";
+  let MIOperandInfo = (ops GR64, i8imm, GR64_NOSP, i32imm, i8imm);
+  let ParserMatchClass = X86MemAsmOperand;
+}
+
 def lea64_32mem : Operand<i32> {
   let PrintMethod = "printi32mem";
   let AsmOperandLowerMethod = "lower_lea64_32mem";
@@ -398,7 +405,8 @@
 // Define X86 specific addressing mode.
 def addr      : ComplexPattern<iPTR, 5, "SelectAddr", [], [SDNPWantParent]>;
 def lea32addr : ComplexPattern<i32, 5, "SelectLEAAddr",
-                               [add, sub, mul, X86mul_imm, shl, or, frameindex],
+                               [add, sub, mul, X86mul_imm, shl, or, frameindex,
+                               X86WrapperRIP], // @LOCALMOD
                                []>;
 def tls32addr : ComplexPattern<i32, 5, "SelectTLSADDRAddr",
                                [tglobaltlsaddr], []>;
@@ -439,6 +447,12 @@
 def In64BitMode  : Predicate<"Subtarget->is64Bit()">, AssemblerPredicate;
 def IsWin64      : Predicate<"Subtarget->isTargetWin64()">;
 def NotWin64     : Predicate<"!Subtarget->isTargetWin64()">;
+// @LOCALMOD-BEGIN
+def IsNaCl       : Predicate<"Subtarget->isTargetNaCl()">, AssemblerPredicate;
+def IsNaCl32     : Predicate<"Subtarget->isTargetNaCl32()">, AssemblerPredicate;
+def IsNaCl64     : Predicate<"Subtarget->isTargetNaCl64()">, AssemblerPredicate;
+def NotNaCl      : Predicate<"!Subtarget->isTargetNaCl()">, AssemblerPredicate;
+// @LOCALMOD-END
 def SmallCode    : Predicate<"TM.getCodeModel() == CodeModel::Small">;
 def KernelCode   : Predicate<"TM.getCodeModel() == CodeModel::Kernel">;
 def FarData      : Predicate<"TM.getCodeModel() != CodeModel::Small &&"
@@ -1250,6 +1264,12 @@
 //===----------------------------------------------------------------------===//
 
 include "X86InstrArithmetic.td"
+
+//===----------------------------------------------------------------------===//
+// NaCl support (@LOCALMOD)
+//===----------------------------------------------------------------------===//
+
+include "X86InstrNaCl.td"
 include "X86InstrCMovSetCC.td"
 include "X86InstrExtension.td"
 include "X86InstrControl.td"
@@ -1281,8 +1301,10 @@
 // Assembler Mnemonic Aliases
 //===----------------------------------------------------------------------===//
 
-def : MnemonicAlias<"call", "calll">, Requires<[In32BitMode]>;
-def : MnemonicAlias<"call", "callq">, Requires<[In64BitMode]>;
+def : MnemonicAlias<"call", "calll">,
+      Requires<[In32BitMode,NotNaCl]>; // @LOCALMOD
+def : MnemonicAlias<"call", "callq">,
+      Requires<[In64BitMode,NotNaCl]>; // @LOCALMOD
 
 def : MnemonicAlias<"cbw",  "cbtw">;
 def : MnemonicAlias<"cwd",  "cwtd">;
diff -r 2b13dadc8fed lib/Target/X86/X86InstrNaCl.cpp
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/lib/Target/X86/X86InstrNaCl.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,367 @@
+//=== X86InstrNaCl.cpp - Expansion of NaCl pseudo-instructions  --*- C++ -*-=//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+//===----------------------------------------------------------------------===//
+#define DEBUG_TYPE "x86-sandboxing"
+
+#include "X86.h"
+#include "X86InstrInfo.h"
+#include "X86Subtarget.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+
+using namespace llvm;
+
+static unsigned PrefixSaved = 0;
+static bool PrefixPass = false;
+
+static void EmitDirectCall(const MCOperand &Op, bool Is64Bit,
+                           MCStreamer &Out) {
+  Out.EmitBundleAlignEnd();
+  Out.EmitBundleLock();
+  MCInst CALLInst;
+  CALLInst.setOpcode(Is64Bit ? X86::CALL64pcrel32 : X86::CALLpcrel32);
+  CALLInst.addOperand(Op);
+  Out.EmitInstruction(CALLInst);
+  Out.EmitBundleUnlock();
+}
+
+static void EmitRegTruncate(unsigned Reg64, MCStreamer &Out) {
+  unsigned Reg32 = getX86SubSuperRegister(Reg64, MVT::i32);
+  MCInst MOVInst;
+  MOVInst.setOpcode(X86::MOV32rr);
+  MOVInst.addOperand(MCOperand::CreateReg(Reg32));
+  MOVInst.addOperand(MCOperand::CreateReg(Reg32));
+  Out.EmitInstruction(MOVInst);
+}
+
+static void EmitIndirectBranch(const MCOperand &Op, bool Is64Bit, bool IsCall,
+                               MCStreamer &Out) {
+  unsigned Reg32 = Op.getReg();
+  unsigned Reg64 = getX86SubSuperRegister(Reg32, MVT::i64);
+
+  if (IsCall)
+    Out.EmitBundleAlignEnd();
+
+  Out.EmitBundleLock();
+
+  MCInst ANDInst;
+  ANDInst.setOpcode(X86::AND32ri8);
+  ANDInst.addOperand(MCOperand::CreateReg(Reg32));
+  ANDInst.addOperand(MCOperand::CreateReg(Reg32));
+  ANDInst.addOperand(MCOperand::CreateImm(-32));
+  Out.EmitInstruction(ANDInst);
+
+  if (Is64Bit) {
+    MCInst InstADD;
+    InstADD.setOpcode(X86::ADD64rr);
+    InstADD.addOperand(MCOperand::CreateReg(Reg64));
+    InstADD.addOperand(MCOperand::CreateReg(Reg64));
+    InstADD.addOperand(MCOperand::CreateReg(X86::R15));
+    Out.EmitInstruction(InstADD);
+  }
+
+  if (IsCall) {
+    MCInst CALLInst;
+    CALLInst.setOpcode(Is64Bit ? X86::CALL64r : X86::CALL32r);
+    CALLInst.addOperand(MCOperand::CreateReg(Is64Bit ? Reg64 : Reg32));
+    Out.EmitInstruction(CALLInst);
+  } else {
+    MCInst JMPInst;
+    JMPInst.setOpcode(Is64Bit ? X86::JMP64r : X86::JMP32r);
+    JMPInst.addOperand(MCOperand::CreateReg(Is64Bit ? Reg64 : Reg32));
+    Out.EmitInstruction(JMPInst);
+  }
+
+  Out.EmitBundleUnlock();
+}
+
+static void EmitRet(const MCOperand *AmtOp, bool Is64Bit, MCStreamer &Out) {
+  MCInst POPInst;
+  POPInst.setOpcode(Is64Bit ? X86::POP64r : X86::POP32r);
+  POPInst.addOperand(MCOperand::CreateReg(Is64Bit ? X86::RCX : X86::ECX));
+  Out.EmitInstruction(POPInst);
+
+  if (AmtOp) {
+    assert(!Is64Bit);
+    MCInst ADDInst;
+    unsigned ADDReg = X86::ESP;
+    ADDInst.setOpcode(X86::ADD32ri);
+    ADDInst.addOperand(MCOperand::CreateReg(ADDReg));
+    ADDInst.addOperand(MCOperand::CreateReg(ADDReg));
+    ADDInst.addOperand(*AmtOp);
+    Out.EmitInstruction(ADDInst);
+  }
+
+  MCInst JMPInst;
+  JMPInst.setOpcode(Is64Bit ? X86::NACL_JMP64r : X86::NACL_JMP32r);
+  JMPInst.addOperand(MCOperand::CreateReg(X86::ECX));
+  Out.EmitInstruction(JMPInst);
+}
+
+void EmitTrap(bool Is64Bit, MCStreamer &Out) {
+  // Rewrite to:
+  //    X86-32:  mov $0, 0
+  //    X86-64:  mov $0, (%r15)
+  unsigned BaseReg = Is64Bit ? X86::R15 : 0;
+  MCInst Tmp;
+  Tmp.setOpcode(X86::MOV32mi);
+  Tmp.addOperand(MCOperand::CreateReg(BaseReg)); // BaseReg
+  Tmp.addOperand(MCOperand::CreateImm(1)); // Scale
+  Tmp.addOperand(MCOperand::CreateReg(0)); // IndexReg
+  Tmp.addOperand(MCOperand::CreateImm(0)); // Offset
+  Tmp.addOperand(MCOperand::CreateReg(0)); // SegmentReg
+  Tmp.addOperand(MCOperand::CreateImm(0)); // Value
+
+  Out.EmitInstruction(Tmp);
+}
+
+// Fix a register after being truncated to 32-bits.
+static void EmitRegFix(unsigned Reg64, MCStreamer &Out) {
+  // lea (%rsp, %r15, 1), %rsp
+  MCInst Tmp;
+  Tmp.setOpcode(X86::LEA64r);
+  Tmp.addOperand(MCOperand::CreateReg(Reg64)); // DestReg
+  Tmp.addOperand(MCOperand::CreateReg(Reg64)); // BaseReg
+  Tmp.addOperand(MCOperand::CreateImm(1)); // Scale
+  Tmp.addOperand(MCOperand::CreateReg(X86::R15)); // IndexReg
+  Tmp.addOperand(MCOperand::CreateImm(0)); // Offset
+  Tmp.addOperand(MCOperand::CreateReg(0)); // SegmentReg
+  Out.EmitInstruction(Tmp);
+}
+
+static void EmitSPArith(unsigned Opc, const MCOperand &ImmOp,
+                        MCStreamer &Out) {
+  Out.EmitBundleLock();
+
+  MCInst Tmp;
+  Tmp.setOpcode(Opc);
+  Tmp.addOperand(MCOperand::CreateReg(X86::RSP));
+  Tmp.addOperand(MCOperand::CreateReg(X86::RSP));
+  Tmp.addOperand(ImmOp);
+  Out.EmitInstruction(Tmp);
+
+  EmitRegFix(X86::RSP, Out);
+  Out.EmitBundleUnlock();
+}
+
+static void EmitSPAdj(const MCOperand &ImmOp, MCStreamer &Out) {
+  Out.EmitBundleLock();
+
+  MCInst Tmp;
+  Tmp.setOpcode(X86::LEA64_32r);
+  Tmp.addOperand(MCOperand::CreateReg(X86::RSP)); // DestReg
+  Tmp.addOperand(MCOperand::CreateReg(X86::RBP)); // BaseReg
+  Tmp.addOperand(MCOperand::CreateImm(1)); // Scale
+  Tmp.addOperand(MCOperand::CreateReg(0)); // IndexReg
+  Tmp.addOperand(ImmOp); // Offset
+  Tmp.addOperand(MCOperand::CreateReg(0)); // SegmentReg
+  Out.EmitInstruction(Tmp);
+
+  EmitRegFix(X86::RSP, Out);
+  Out.EmitBundleUnlock();
+}
+
+static void EmitREST(const MCInst &Inst, unsigned Reg32, bool IsMem, MCStreamer &Out) {
+  unsigned Reg64 = getX86SubSuperRegister(Reg32, MVT::i64);
+
+  Out.EmitBundleLock();
+  MCInst MOVInst;
+  if (!IsMem) {
+    MOVInst.setOpcode(X86::MOV32rr);
+    MOVInst.addOperand(MCOperand::CreateReg(Reg32));
+    MOVInst.addOperand(Inst.getOperand(0));
+  } else {
+    // Do load/store sandbox also if needed
+    unsigned SegmentReg = Inst.getOperand(4).getReg();
+    if (SegmentReg == X86::PSEUDO_NACL_SEG) {
+      unsigned IndexReg = Inst.getOperand(2).getReg();
+      EmitRegTruncate(IndexReg, Out);
+      SegmentReg = 0;
+    }
+    MOVInst.setOpcode(X86::MOV32rm);
+    MOVInst.addOperand(MCOperand::CreateReg(Reg32));
+    MOVInst.addOperand(Inst.getOperand(0)); // BaseReg
+    MOVInst.addOperand(Inst.getOperand(1)); // Scale
+    MOVInst.addOperand(Inst.getOperand(2)); // IndexReg
+    MOVInst.addOperand(Inst.getOperand(3)); // Offset
+    MOVInst.addOperand(MCOperand::CreateReg(SegmentReg)); // Segment
+  }
+  Out.EmitInstruction(MOVInst);
+
+  EmitRegFix(Reg64, Out);
+  Out.EmitBundleUnlock();
+}
+
+static void EmitPrefix(unsigned Opc, MCStreamer &Out) {
+  assert(PrefixSaved == 0);
+  assert(PrefixPass == false);
+
+  MCInst PrefixInst;
+  PrefixInst.setOpcode(Opc);
+  PrefixPass = true;
+  Out.EmitInstruction(PrefixInst);
+
+  assert(PrefixSaved == 0);
+  assert(PrefixPass == false);
+}
+
+namespace llvm {
+// CustomExpandInstNaCl -
+//   If Inst is a NaCl pseudo instruction, emits the substitute
+//   expansion to the MCStreamer and returns true.
+//   Otherwise, returns false.
+//
+//   NOTE: Each time this function calls Out.EmitInstruction(), it will be
+//   called again recursively to rewrite the new instruction being emitted.
+//   Care must be taken to ensure that this does not result in an infinite
+//   loop. Also, global state must be managed carefully so that it is
+//   consistent during recursive calls.
+//
+//   We need global state to keep track of the explicit prefix (PREFIX_*)
+//   instructions. Unfortunately, the assembly parser prefers to generate
+//   these instead of combined instructions. At this time, having only
+//   one explicit prefix is supported.
+bool CustomExpandInstNaCl(const MCInst &Inst, MCStreamer &Out) {
+  // If we are emitting to .s, just emit all pseudo-instructions directly.
+  if (Out.hasRawTextSupport()) {
+    return false;
+  }
+  unsigned Opc = Inst.getOpcode();
+  DEBUG(dbgs() << "CustomExpandInstNaCl("; Inst.dump(); dbgs() << ")\n");
+  switch (Opc) {
+  case X86::LOCK_PREFIX:
+  case X86::REP_PREFIX:
+  case X86::REPNE_PREFIX:
+  case X86::REX64_PREFIX:
+    // Ugly hack because LLVM AsmParser is not smart enough to combine
+    // prefixes back into the instruction they modify.
+    if (PrefixPass) {
+      PrefixPass = false;
+      PrefixSaved = 0;
+      return false;
+    }
+    assert(PrefixSaved == 0);
+    PrefixSaved = Opc;
+    return true;
+  case X86::NACL_TRAP32:
+    assert(PrefixSaved == 0);
+    EmitTrap(false, Out);
+    return true;
+  case X86::NACL_TRAP64:
+    assert(PrefixSaved == 0);
+    EmitTrap(true, Out);
+    return true;
+  case X86::NACL_CALL32d:
+    assert(PrefixSaved == 0);
+    EmitDirectCall(Inst.getOperand(0), false, Out);
+    return true;
+  case X86::NACL_CALL64d:
+    assert(PrefixSaved == 0);
+    EmitDirectCall(Inst.getOperand(0), true, Out);
+    return true;
+  case X86::NACL_CALL32r:
+    assert(PrefixSaved == 0);
+    EmitIndirectBranch(Inst.getOperand(0), false, true, Out);
+    return true;
+  case X86::NACL_CALL64r:
+    assert(PrefixSaved == 0);
+    EmitIndirectBranch(Inst.getOperand(0), true, true, Out);
+    return true;
+  case X86::NACL_JMP32r:
+    assert(PrefixSaved == 0);
+    EmitIndirectBranch(Inst.getOperand(0), false, false, Out);
+    return true;
+  case X86::NACL_JMP64r:
+    assert(PrefixSaved == 0);
+    EmitIndirectBranch(Inst.getOperand(0), true, false, Out);
+    return true;
+  case X86::NACL_RET32:
+    assert(PrefixSaved == 0);
+    EmitRet(NULL, false, Out);
+    return true;
+  case X86::NACL_RET64:
+    assert(PrefixSaved == 0);
+    EmitRet(NULL, true, Out);
+    return true;
+  case X86::NACL_RETI32:
+    assert(PrefixSaved == 0);
+    EmitRet(&Inst.getOperand(0), false, Out);
+    return true;
+  case X86::NACL_ASPi8:
+    assert(PrefixSaved == 0);
+    EmitSPArith(X86::ADD32ri8, Inst.getOperand(0), Out);
+    return true;
+  case X86::NACL_ASPi32:
+    assert(PrefixSaved == 0);
+    EmitSPArith(X86::ADD32ri, Inst.getOperand(0), Out);
+    return true;
+  case X86::NACL_SSPi8:
+    assert(PrefixSaved == 0);
+    EmitSPArith(X86::SUB32ri8, Inst.getOperand(0), Out);
+    return true;
+  case X86::NACL_SSPi32:
+    assert(PrefixSaved == 0);
+    EmitSPArith(X86::SUB32ri, Inst.getOperand(0), Out);
+    return true;
+  case X86::NACL_SPADJi32:
+    assert(PrefixSaved == 0);
+    EmitSPAdj(Inst.getOperand(0), Out);
+    return true;
+  case X86::NACL_RESTBPm:
+    assert(PrefixSaved == 0);
+    EmitREST(Inst, X86::EBP, true, Out);
+    return true;
+  case X86::NACL_RESTBPr:
+    assert(PrefixSaved == 0);
+    EmitREST(Inst, X86::EBP, false, Out);
+    return true;
+  case X86::NACL_RESTSPm:
+    assert(PrefixSaved == 0);
+    EmitREST(Inst, X86::ESP, true, Out);
+    return true;
+  case X86::NACL_RESTSPr:
+    assert(PrefixSaved == 0);
+    EmitREST(Inst, X86::ESP, false, Out);
+    return true;
+  }
+
+  for (unsigned i=0, e = Inst.getNumOperands(); i != e; i++) {
+    if (Inst.getOperand(i).isReg() &&
+        Inst.getOperand(i).getReg() == X86::PSEUDO_NACL_SEG) {
+      // Sandbox memory access
+      unsigned IndexReg = Inst.getOperand(i-2).getReg();
+
+      MCInst InstClean = Inst;
+      InstClean.getOperand(i).setReg(0);
+
+      unsigned PrefixLocal = PrefixSaved;
+      PrefixSaved = 0;
+
+      Out.EmitBundleLock();
+      EmitRegTruncate(IndexReg, Out);
+      if (PrefixLocal)
+        EmitPrefix(PrefixLocal, Out);
+      Out.EmitInstruction(InstClean);
+      Out.EmitBundleUnlock();
+      return true;
+    }
+  }
+
+  if (PrefixSaved) {
+    unsigned PrefixLocal = PrefixSaved;
+    PrefixSaved = 0;
+    EmitPrefix(PrefixLocal, Out);
+  }
+  return false;
+}
+
+} // namespace llvm
diff -r 2b13dadc8fed lib/Target/X86/X86InstrNaCl.h
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/lib/Target/X86/X86InstrNaCl.h	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,19 @@
+//===-- X86InstrNaCl.h - Prototype for CustomExpandInstNaCl    ---*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef X86_INSTRNACL_H
+#define X86_INSTRNACL_H
+
+namespace llvm {
+  class MCInst;
+  class MCStreamer;
+  bool CustomExpandInstNaCl(const MCInst &Inst, MCStreamer &Out);
+}
+
+#endif
diff -r 2b13dadc8fed lib/Target/X86/X86InstrNaCl.td
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/lib/Target/X86/X86InstrNaCl.td	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,316 @@
+//====- X86InstrNaCl.td - Describe NaCl Instructions ----*- tablegen -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file describes the modifications to the X86 instruction set needed for
+// Native Client code generation.
+//
+//===----------------------------------------------------------------------===//
+
+
+//===----------------------------------------------------------------------===//
+//
+//                       Native Client Pseudo-Instructions
+//
+// These instructions implement the Native Client pseudo-instructions, such
+// as nacljmp and naclasp.
+//
+// TableGen and MC consider these to be "real" instructions. They can be
+// parsed by the AsmParser and emitted by the AsmStreamer as if they
+// were just regular instructions. They are not marked "Pseudo" because
+// this would imply isCodeGenOnly=1, which would stop them from being
+// parsed by the assembler.
+//
+// These instructions cannot be encoded (written into an object file) by the
+// MCCodeEmitter. Instead, during direct object emission, they get lowered to
+// a sequence of streamer emits. (see X86InstrNaCl.cpp)
+// 
+// These instructions should not be used in CodeGen. They have no pattern
+// and lack CodeGen metadata. Instead, the X86NaClRewritePass should
+// generate these instructions after CodeGen is finished.
+//
+//===----------------------------------------------------------------------===//
+
+
+//===----------------------------------------------------------------------===//
+// 32-bit Native Client Pseudo Instructions
+//===----------------------------------------------------------------------===//
+
+class NaClPI32<dag outs, dag ins, string asm>
+  : I<0, CustomFrm, outs, ins, asm, []>, Requires<[IsNaCl32]>;
+
+let isTerminator = 1, isBarrier = 1, hasCtrlDep = 1, isAsmParserOnly = 1 in {
+  def NACL_TRAP32  : NaClPI32<(outs), (ins), "nacltrap">;
+}
+
+let isTerminator = 1, isReturn = 1, isBarrier = 1,
+    hasCtrlDep = 1, FPForm = SpecialFP, isAsmParserOnly = 1 in {
+  def NACL_RET32  : NaClPI32<(outs), (ins), "naclret">;
+  def NACL_RETI32 : NaClPI32<(outs), (ins i16imm:$amt), "naclreti\t$amt">;
+}
+
+let isBranch = 1, isTerminator = 1, isBarrier = 1, isIndirectBranch = 1,
+    isAsmParserOnly = 1 in {
+  def NACL_JMP32r : NaClPI32<(outs), (ins GR32:$dst), "nacljmp\t$dst">;
+}
+
+let isCall = 1, isAsmParserOnly = 1 in {
+  def NACL_CALL32d : NaClPI32<(outs), (ins i32imm_pcrel:$dst),
+                     "call\t$dst">;
+  def NACL_CALL32r : NaClPI32<(outs), (ins GR32:$dst),
+                     "naclcall\t$dst">;
+}
+
+//===----------------------------------------------------------------------===//
+// 64-bit Native Client Pseudo Instructions
+//===----------------------------------------------------------------------===//
+
+class NaClPI64<dag outs, dag ins, string asm>
+  : I<0, CustomFrm, outs, ins, asm, []>, Requires<[IsNaCl64]>;
+
+let isTerminator = 1, isBarrier = 1, hasCtrlDep = 1, isAsmParserOnly = 1 in {
+  def NACL_TRAP64  : NaClPI64<(outs), (ins), "nacltrap">;
+}
+
+let isTerminator = 1, isReturn = 1, isBarrier = 1,
+    hasCtrlDep = 1, FPForm = SpecialFP, isAsmParserOnly = 1 in {
+  def NACL_RET64  : NaClPI64<(outs), (ins), "naclret">;
+}
+
+let isBranch = 1, isTerminator = 1, isBarrier = 1, isIndirectBranch = 1,
+    isAsmParserOnly = 1 in {
+  def NACL_JMP64r : NaClPI64<(outs), (ins GR32:$dst, GR64:$rZP),
+                    "nacljmp\t{$dst, $rZP|$rZP, $dst}">;
+}
+
+
+let isCall = 1, isAsmParserOnly = 1 in {
+  def NACL_CALL64d : NaClPI64<(outs), (ins i32imm_pcrel:$dst),
+                     "call\t$dst">;
+  def NACL_CALL64r : NaClPI64<(outs), (ins GR32:$dst, GR64:$rZP),
+                     "naclcall\t$dst,$rZP">;
+}
+
+let Defs = [RSP, EFLAGS], Uses = [RSP], isAsmParserOnly = 1 in {
+  def NACL_ASPi8 : NaClPI32<(outs), (ins i64i8imm:$off, GR64:$rZP),
+                   "naclasp{q}\t{$off, $rZP|$rZP, $off}">;
+
+  def NACL_ASPi32: NaClPI64<(outs), (ins i64i32imm:$off, GR64:$rZP),
+                   "naclasp{q}\t{$off, $rZP|$rZP, $off}">;
+
+  def NACL_SSPi8 : NaClPI64<(outs), (ins i64i8imm:$off, GR64:$rZP),
+                   "naclssp{q}\t{$off, $rZP|$rZP, $off}">;
+
+  def NACL_SSPi32: NaClPI64<(outs), (ins i64i32imm:$off, GR64:$rZP),
+                   "naclssp{q}\t{$off, $rZP|$rZP, $off}">;
+}
+
+let Defs = [RSP], Uses = [RBP], isAsmParserOnly = 1 in {
+  def NACL_SPADJi32  : NaClPI64<(outs), (ins i64i32imm:$off, GR64:$rZP),
+                       "naclspadj\t{$off, $rZP|$rZP, $off}">;
+}
+
+let Defs = [RSP], isAsmParserOnly = 1 in {
+  def NACL_RESTSPr   : NaClPI64<(outs), (ins GR32:$src, GR64:$rZP),
+                       "naclrestsp_noflags\t{$src, $rZP|$rZP, $src}">;
+  def NACL_RESTSPm   : NaClPI64<(outs), (ins i32mem:$src, GR64:$rZP),
+                       "naclrestsp_noflags\t{$src, $rZP|$rZP, $src}">;
+}
+
+def : MnemonicAlias<"naclrestsp", "naclrestsp_noflags">;
+
+let Defs = [RBP], isAsmParserOnly = 1 in {
+  def NACL_RESTBPr   : NaClPI64<(outs), (ins GR32:$src, GR64:$rZP),
+                       "naclrestbp\t{$src, $rZP|$rZP, $src}">;
+  def NACL_RESTBPm   : NaClPI64<(outs), (ins i32mem:$src, GR64:$rZP),
+                       "naclrestbp\t{$src, $rZP|$rZP, $src}">;
+}
+
+//===----------------------------------------------------------------------===//
+//
+// Code Generator Instructions (isCodeGenOnly == 1)
+//
+// These instructions exists to make CodeGen work with Native Client's
+// modifications.
+//
+// Many of these instructions exist because of limitations in CodeGen
+// or TableGen, and may become unnecessary in the future.
+//===----------------------------------------------------------------------===//
+
+
+//===----------------------------------------------------------------------===//
+//
+// CodeGen 32-bit
+//
+//===----------------------------------------------------------------------===//
+
+
+// To avoid a naming conflict between call/naclcall, we have to
+// disable the real CALLpcrel32 and CALL32r instructions when targeting
+// for NaCl. Thus, they need to be produced here.
+
+let isCall = 1 in
+  // All calls clobber the non-callee saved registers. ESP is marked as
+  // a use to prevent stack-pointer assignments that appear immediately
+  // before calls from potentially appearing dead. Uses for argument
+  // registers are added manually.
+  let Defs = [EAX, ECX, EDX, FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0,
+              MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7,
+              XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7,
+              XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15, EFLAGS],
+      Uses = [ESP] in {
+
+    def NACL_CG_CALLpcrel32 : I<0, Pseudo,
+                              (outs), (ins i32imm_pcrel:$dst, variable_ops),
+                              "call\t$dst", []>, Requires<[IsNaCl32]>;
+    def NACL_CG_CALL32r     : I<0, Pseudo,
+                              (outs), (ins GR32:$dst, variable_ops),
+                              "naclcall\t$dst", [(X86call GR32:$dst)]>,
+                              Requires<[IsNaCl32]>;
+}
+
+// Normal calls, with various flavors of addresses.
+def : Pat<(X86call (i32 tglobaladdr:$dst)),
+          (NACL_CG_CALLpcrel32 tglobaladdr:$dst)>,
+          Requires<[IsNaCl32]>;
+def : Pat<(X86call (i32 texternalsym:$dst)),
+          (NACL_CG_CALLpcrel32 texternalsym:$dst)>,
+          Requires<[IsNaCl32]>;
+def : Pat<(X86call (i32 imm:$dst)),
+          (NACL_CG_CALLpcrel32 imm:$dst)>,
+          Requires<[IsNaCl32,CallImmAddr]>;
+
+//===----------------------------------------------------------------------===//
+//
+// CodeGen 64-bit
+//
+//===----------------------------------------------------------------------===//
+
+
+// Because pointers are 32-bit on X86-64 Native Client, we need to
+// produce new versions of the JMP64/CALL64 instructions which can accept
+// addresses which are i32 instead of i64.
+
+let isBranch = 1, isTerminator = 1, isBarrier = 1, isIndirectBranch = 1 in {
+  def NACL_CG_JMP64r     : I<0, Pseudo, (outs), (ins GR32:$dst, variable_ops),
+                           "nacljmp\t$dst",
+                           [(brind GR32:$dst)]>,
+                           Requires<[IsNaCl64]>;
+}
+
+let isCall = 1 in
+  // All calls clobber the non-callee saved registers. RSP is marked as
+  // a use to prevent stack-pointer assignments that appear immediately
+  // before calls from potentially appearing dead. Uses for argument
+  // registers are added manually.
+  let Defs = [RAX, RCX, RDX, RSI, RDI, R8, R9, R10, R11,
+              FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0, ST1,
+              MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7,
+              XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7,
+              XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15, EFLAGS],
+      Uses = [RSP] in {
+
+    def NACL_CG_CALL64pcrel32 : I<0, Pseudo, (outs),
+                                (ins i32imm_pcrel:$dst, variable_ops),
+                                "call\t$dst", []>,
+                                Requires<[IsNaCl64]>;
+
+    def NACL_CG_CALL64r       : I<0, Pseudo, (outs), (ins GR32:$dst, variable_ops),
+                                "naclcall\t$dst,%r15",
+                                [(X86call GR32:$dst)]>,
+                                Requires<[IsNaCl64]>;
+}
+
+def : Pat<(X86call (i32 tglobaladdr:$dst)),
+          (NACL_CG_CALL64pcrel32 tglobaladdr:$dst)>, Requires<[IsNaCl64]>;
+def : Pat<(X86call (i32 texternalsym:$dst)),
+          (NACL_CG_CALL64pcrel32 texternalsym:$dst)>, Requires<[IsNaCl64]>;
+
+// Tail calls
+// Also needed due to the i64 / i32 pointer problem.
+let isCall = 1, isTerminator = 1, isReturn = 1, isBarrier = 1,
+    isCodeGenOnly = 1 in
+  let Defs = [RAX, RCX, RDX, RSI, RDI, R8, R9, R10, R11,
+              FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0, ST1,
+              MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7,
+              XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7,
+              XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15, EFLAGS],
+      Uses = [RSP] in {
+
+  def NACL_CG_TCRETURNdi64 : I<0, Pseudo, (outs),
+                             (ins i32imm_pcrel:$dst, i32imm:$offset, 
+                             variable_ops),
+                             "#TC_RETURN $dst $offset", []>,
+                          Requires<[IsNaCl64]>;
+  def NACL_CG_TCRETURNri64 : I<0, Pseudo, (outs),
+                            (ins GR32_TC_64:$dst, i32imm:$offset,
+                             variable_ops),
+                            "#TC_RETURN $dst $offset", []>,
+                            Requires<[IsNaCl64]>;
+
+  def NACL_CG_TAILJMPd64 : I<0, Pseudo, (outs),
+                           (ins i32imm_pcrel:$dst, variable_ops),
+                           "jmp\t$dst  # TAILCALL", []>,
+                           Requires<[IsNaCl64]>;
+  def NACL_CG_TAILJMPr64 : I<0, Pseudo, (outs),
+                           (ins GR32_TC_64:$dst, variable_ops),
+                           "nacljmp\t$dst,%r15  # TAILCALL", []>,
+                           Requires<[IsNaCl64]>;
+}
+
+def : Pat<(X86tcret (i32 tglobaladdr:$dst), imm:$off),
+          (NACL_CG_TCRETURNdi64 tglobaladdr:$dst, imm:$off)>,
+	  Requires<[IsNaCl64]>;
+
+def : Pat<(X86tcret (i32 texternalsym:$dst), imm:$off),
+          (NACL_CG_TCRETURNdi64 texternalsym:$dst, imm:$off)>,
+	  Requires<[IsNaCl64]>;
+
+def : Pat<(X86tcret GR32_TC_64:$dst, imm:$off),
+          (NACL_CG_TCRETURNri64 GR32_TC_64:$dst, imm:$off)>,
+	  Requires<[IsNaCl64]>;
+
+// ELF TLS Support
+// The linker expects this lea+call sequence to be directly adjacent.
+let Defs = [EAX, ECX, EDX, FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0,
+            MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7,
+            XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7,
+            XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15, EFLAGS],
+    Uses = [ESP] in
+def NACL_CG_TLS_addr32 : I<0, Pseudo, (outs), (ins i32mem:$sym),
+                         ".bundle_align_end"
+                         ".bundle_lock"
+                         "leal\t$sym, %eax; "
+                         "call\t___tls_get_addr@PLT"
+                         ".bundle_unlock",
+                         [(X86tlsaddr tls32addr:$sym)]>,
+                         Requires<[In32BitMode, IsNaCl32]>;
+
+// The NaCl loader cannot set the FS or GS segmentation registers
+// on Windows, so we must use a NaCl syscall.
+let Defs = [RAX, RCX, RDX, RSI, RDI, R8, R9, R10, R11,
+            FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0, ST1,
+            MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7,
+            XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7,
+            XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15, EFLAGS],
+    Uses = [RSP] in
+def NACL_CG_TLS_addr64 : I<0, Pseudo, (outs), (ins i32mem:$sym),
+                         "movq\t$$$sym, %rdi; "
+                         "call\t__tls_get_addr@PLT",
+                         [(X86tlsaddr tls32addr:$sym)]>,
+                         Requires<[IsNaCl64]>;
+
+let usesCustomInserter = 1, Defs = [EFLAGS] in
+def NACL_CG_VAARG_64 : I<0, Pseudo,
+                     (outs GR32:$dst),
+                     (ins i8mem:$ap, i32imm:$size, i8imm:$mode, i32imm:$align),
+                     "#NACL_VAARG_64 $dst, $ap, $size, $mode, $align",
+                     [(set GR32:$dst,
+                     (X86vaarg64 addr:$ap, imm:$size, imm:$mode, imm:$align)),
+                     (implicit EFLAGS)]>,
+                     Requires<[IsNaCl64]>;
diff -r 2b13dadc8fed lib/Target/X86/X86JITInfo.cpp
--- a/lib/Target/X86/X86JITInfo.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86JITInfo.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -25,11 +25,15 @@
 using namespace llvm;
 
 // Determine the platform we're running on
+// @LOCALMOD - This disables the JIT related inline asm code 
+//             from getting compiled into llvm-tools
+#if !defined(__native_client__)
 #if defined (__x86_64__) || defined (_M_AMD64) || defined (_M_X64)
 # define X86_64_JIT
 #elif defined(__i386__) || defined(i386) || defined(_M_IX86)
 # define X86_32_JIT
 #endif
+#endif
 
 void X86JITInfo::replaceMachineCodeForFunction(void *Old, void *New) {
   unsigned char *OldByte = (unsigned char *)Old;
diff -r 2b13dadc8fed lib/Target/X86/X86MCCodeEmitter.cpp
--- a/lib/Target/X86/X86MCCodeEmitter.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86MCCodeEmitter.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -848,6 +848,10 @@
     assert(0 && "Unknown FormMask value in X86MCCodeEmitter!");
   case X86II::Pseudo:
     assert(0 && "Pseudo instruction shouldn't be emitted");
+  // @LOCALMOD-BEGIN
+  case X86II::CustomFrm:
+    assert(0 && "CustomFrm instruction shouldn't be emitted");
+  // @LOCALMOD-END
   case X86II::RawFrm:
     EmitByte(BaseOpcode, CurByte, OS);
     break;
diff -r 2b13dadc8fed lib/Target/X86/X86MCInstLower.cpp
--- a/lib/Target/X86/X86MCInstLower.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86MCInstLower.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -636,7 +636,13 @@
     
     // Emit the call.
     MCSymbol *PICBase = MF->getPICBaseSymbol();
-    TmpInst.setOpcode(X86::CALLpcrel32);
+    // @LOCALMOD-BEGIN
+    // For NaCl, the call should be aligned to the end of a bundle. Since the
+    // call is at the end of the bundle, there should be no padding between
+    // the call and the next instruction (the label should still make sense).
+    TmpInst.setOpcode(getSubtarget().isTargetNaCl() ?
+                      X86::NACL_CALL32d : X86::CALLpcrel32);
+    // @LOCALMOD-END
     // FIXME: We would like an efficient form for this, so we don't have to do a
     // lot of extra uniquing.
     TmpInst.addOperand(MCOperand::CreateExpr(MCSymbolRefExpr::Create(PICBase,
diff -r 2b13dadc8fed lib/Target/X86/X86NaClRewritePass.cpp
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/lib/Target/X86/X86NaClRewritePass.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,661 @@
+//=== X86NaClRewritePAss.cpp - Rewrite instructions for NaCl SFI --*- C++ -*-=//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains a pass that ensures stores and loads and stack/frame
+// pointer addresses are within the NaCl sandbox (for x86-64).
+// It also ensures that indirect control flow follows NaCl requirments.
+//===----------------------------------------------------------------------===//
+#define DEBUG_TYPE "x86-sandboxing"
+
+#include "X86.h"
+#include "X86InstrInfo.h"
+#include "X86Subtarget.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineJumpTableInfo.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/FormattedStream.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Support/CommandLine.h"
+
+using namespace llvm;
+
+namespace {
+  class X86NaClRewritePass : public MachineFunctionPass {
+  public:
+    static char ID;
+    X86NaClRewritePass() : MachineFunctionPass(ID) {}
+
+    virtual bool runOnMachineFunction(MachineFunction &Fn);
+
+    virtual const char *getPassName() const {
+      return "NaCl Rewrites";
+    }
+
+  private:
+
+    const TargetMachine *TM;
+    const TargetInstrInfo *TII;
+    const TargetRegisterInfo *TRI;
+    const X86Subtarget *Subtarget;
+    bool Is64Bit;
+
+    bool runOnMachineBasicBlock(MachineBasicBlock &MBB);
+
+    void TraceLog(const char *func,
+                  const MachineBasicBlock &MBB,
+                  const MachineBasicBlock::iterator MBBI) const;
+
+    bool ApplyRewrites(MachineBasicBlock &MBB,
+                      MachineBasicBlock::iterator MBBI);
+    bool ApplyStackSFI(MachineBasicBlock &MBB,
+                       MachineBasicBlock::iterator MBBI);
+
+    bool ApplyMemorySFI(MachineBasicBlock &MBB,
+                        MachineBasicBlock::iterator MBBI);
+
+    bool ApplyFrameSFI(MachineBasicBlock &MBB,
+                       MachineBasicBlock::iterator MBBI);
+
+    bool ApplyControlSFI(MachineBasicBlock &MBB,
+                         MachineBasicBlock::iterator MBBI);
+
+    void PassLightWeightValidator(MachineBasicBlock &MBB);
+    bool AlignJumpTableTargets(MachineFunction &MF);
+  };
+
+  char X86NaClRewritePass::ID = 0;
+
+}
+
+static void DumpInstructionVerbose(const MachineInstr &MI);
+
+static bool IsPushPop(MachineInstr &MI) {
+  const unsigned Opcode = MI.getOpcode();
+  switch (Opcode) {
+   default:
+    return false;
+   case X86::PUSH64r:
+   case X86::POP64r:
+    return true;
+  }
+}
+
+static bool IsSandboxed(MachineInstr &MI);
+
+static bool IsStore(MachineInstr &MI) {
+  return MI.getDesc().mayStore();
+}
+
+static bool IsLoad(MachineInstr &MI) {
+  return MI.getDesc().mayLoad();
+}
+
+static bool IsFrameChange(MachineInstr &MI) {
+  return MI.modifiesRegister(X86::EBP, NULL) ||
+         MI.modifiesRegister(X86::RBP, NULL);
+}
+
+static bool IsStackChange(MachineInstr &MI) {
+  return MI.modifiesRegister(X86::ESP, NULL) ||
+         MI.modifiesRegister(X86::RSP, NULL);
+}
+
+
+static bool HasControlFlow(const MachineInstr &MI) {
+ return MI.getDesc().isBranch() ||
+        MI.getDesc().isCall() ||
+        MI.getDesc().isReturn() ||
+        MI.getDesc().isTerminator() ||
+        MI.getDesc().isBarrier();
+}
+
+static bool IsDirectBranch(const MachineInstr &MI) {
+  return  MI.getDesc().isBranch() &&
+         !MI.getDesc().isIndirectBranch();
+}
+
+static bool IsRegAbsolute(unsigned Reg) {
+  return (Reg == X86::RSP || Reg == X86::RBP ||
+          Reg == X86::R15 || Reg == X86::RIP);
+}
+
+static unsigned FindMemoryOperand(const MachineInstr &MI) {
+  int NumFound = 0;
+  unsigned MemOp = 0;
+  for (unsigned i = 0; i < MI.getNumOperands(); ) {
+    if (isMem(&MI, i)) {
+      NumFound++;
+      MemOp = i;
+      i += X86::AddrNumOperands;
+    } else {
+      i++;
+    }
+  }
+
+  if (NumFound == 0)
+    llvm_unreachable("Unable to find memory operands in load/store!");
+
+  if (NumFound > 1)
+    llvm_unreachable("Too many memory operands in instruction!");
+
+  return MemOp;
+}
+
+static unsigned PromoteRegTo64(unsigned RegIn) {
+  if (RegIn == 0)
+    return 0;
+  unsigned RegOut = getX86SubSuperRegister(RegIn, MVT::i64, false);
+  assert(RegOut != 0);
+  return RegOut;
+}
+
+static unsigned DemoteRegTo32(unsigned RegIn) {
+  if (RegIn == 0)
+    return 0;
+  unsigned RegOut = getX86SubSuperRegister(RegIn, MVT::i32, false);
+  assert(RegOut != 0);
+  return RegOut;
+}
+
+
+//
+// True if this MI restores RSP from RBP with a slight adjustment offset.
+//
+static bool MatchesSPAdj(const MachineInstr &MI) {
+  assert (MI.getOpcode() == X86::LEA64r && "Call to MatchesSPAdj w/ non LEA");
+  const MachineOperand &DestReg = MI.getOperand(0);
+  const MachineOperand &BaseReg = MI.getOperand(1);
+  const MachineOperand &Scale = MI.getOperand(2);
+  const MachineOperand &IndexReg = MI.getOperand(3);
+  const MachineOperand &Offset = MI.getOperand(4);
+  return (DestReg.isReg() && DestReg.getReg() == X86::RSP &&
+          BaseReg.isReg() && BaseReg.getReg() == X86::RBP &&
+          Scale.getImm() == 1 &&
+          IndexReg.isReg() && IndexReg.getReg() == 0 &&
+          Offset.isImm());
+}
+
+void
+X86NaClRewritePass::TraceLog(const char *func,
+                             const MachineBasicBlock &MBB,
+                             const MachineBasicBlock::iterator MBBI) const {
+  DEBUG(dbgs() << "@" << func << "(" << MBB.getName() << ", " << (*MBBI) << ")\n");
+}
+
+bool X86NaClRewritePass::ApplyStackSFI(MachineBasicBlock &MBB,
+                                       MachineBasicBlock::iterator MBBI) {
+  TraceLog("ApplyStackSFI", MBB, MBBI);
+  assert(Is64Bit);
+  MachineInstr &MI = *MBBI;
+
+  if (!IsStackChange(MI))
+    return false;
+
+  if (IsPushPop(MI))
+    return false;
+
+  unsigned Opc = MI.getOpcode();
+  DebugLoc DL = MI.getDebugLoc();
+  unsigned DestReg = MI.getOperand(0).getReg();
+  assert(DestReg == X86::ESP || DestReg == X86::RSP);
+
+  unsigned NewOpc = 0;
+  switch (Opc) {
+  case X86::ADD64ri8 : NewOpc = X86::NACL_ASPi8; break;
+  case X86::ADD64ri32: NewOpc = X86::NACL_ASPi32; break;
+  case X86::SUB64ri8 : NewOpc = X86::NACL_SSPi8; break;
+  case X86::SUB64ri32: NewOpc = X86::NACL_SSPi32; break;
+  }
+  if (NewOpc) {
+    BuildMI(MBB, MBBI, DL, TII->get(NewOpc))
+      .addImm(MI.getOperand(2).getImm())
+      .addReg(X86::R15);
+    MI.eraseFromParent();
+    return true;
+  }
+
+  // Promote "MOV ESP, EBP" to a 64-bit move
+  if (Opc == X86::MOV32rr && MI.getOperand(1).getReg() == X86::EBP) {
+    MI.getOperand(0).setReg(X86::RSP);
+    MI.getOperand(1).setReg(X86::RBP);
+    MI.setDesc(TII->get(X86::MOV64rr));
+    Opc = X86::MOV64rr;
+  }
+
+  // "MOV RBP, RSP" is already safe
+  if (Opc == X86::MOV64rr && MI.getOperand(1).getReg() == X86::RBP) {
+    return true;
+  }
+
+  //  Promote 32-bit lea to 64-bit lea (does this ever happen?)
+  assert(Opc != X86::LEA32r && "Invalid opcode in 64-bit mode!");
+  if (Opc == X86::LEA64_32r) {
+    unsigned DestReg = MI.getOperand(0).getReg();
+    unsigned BaseReg = MI.getOperand(1).getReg();
+    unsigned Scale   = MI.getOperand(2).getImm();
+    unsigned IndexReg = MI.getOperand(3).getReg();
+    assert(DestReg == X86::ESP);
+    assert(Scale == 1);
+    assert(BaseReg == X86::EBP);
+    assert(IndexReg == 0);
+    MI.getOperand(0).setReg(X86::RSP);
+    MI.getOperand(1).setReg(X86::RBP);
+    MI.setDesc(TII->get(X86::LEA64r));
+    Opc = X86::LEA64r;
+  }
+
+  if (Opc == X86::LEA64r && MatchesSPAdj(MI)) {
+    const MachineOperand &Offset = MI.getOperand(4);
+    BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_SPADJi32))
+      .addImm(Offset.getImm())
+      .addReg(X86::R15);
+    MI.eraseFromParent();
+    return true;
+  }
+
+  if (Opc == X86::MOV32rr || Opc == X86::MOV64rr) {
+    BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_RESTSPr))
+      .addReg(DemoteRegTo32(MI.getOperand(1).getReg()))
+      .addReg(X86::R15);
+    MI.eraseFromParent();
+    return true;
+  }
+
+  if (Opc == X86::MOV32rm) {
+    BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_RESTSPm))
+      .addOperand(MI.getOperand(1)) // Base
+      .addOperand(MI.getOperand(2)) // Scale
+      .addOperand(MI.getOperand(3)) // Index
+      .addOperand(MI.getOperand(4)) // Offset
+      .addOperand(MI.getOperand(5)) // Segment
+      .addReg(X86::R15);
+    MI.eraseFromParent();
+    return true;
+  }
+
+  DumpInstructionVerbose(MI);
+  llvm_unreachable("Unhandled Stack SFI");
+}
+
+bool X86NaClRewritePass::ApplyFrameSFI(MachineBasicBlock &MBB,
+                                       MachineBasicBlock::iterator MBBI) {
+  TraceLog("ApplyFrameSFI", MBB, MBBI);
+  assert(Is64Bit);
+  MachineInstr &MI = *MBBI;
+
+  if (!IsFrameChange(MI))
+    return false;
+
+  unsigned Opc = MI.getOpcode();
+
+  // MOV RBP, RSP is safe
+  if (Opc == X86::MOV64rr) {
+    assert(MI.getOperand(0).getReg() == X86::RBP);
+    assert(MI.getOperand(1).getReg() == X86::RSP);
+    return false;
+  }
+
+  // Popping onto RBP
+  // Rewrite to:
+  //   naclrestbp (%rsp), %r15
+  //   naclasp $8, %r15
+  //
+  // TODO(pdox): Consider rewriting to this instead:
+  //   .bundle_lock
+  //   pop %rbp
+  //   mov %ebp,%ebp
+  //   add %r15, %rbp
+  //   .bundle_unlock
+  if (Opc == X86::POP64r) {
+    assert(MI.getOperand(0).getReg() == X86::RBP);
+    DebugLoc DL = MI.getDebugLoc();
+
+    BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_RESTBPm))
+      .addReg(X86::RSP)  // Base
+      .addImm(1)  // Scale
+      .addReg(0)  // Index
+      .addImm(0)  // Offset
+      .addReg(0)  // Segment
+      .addReg(X86::R15); // rZP
+
+    BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_ASPi8))
+      .addImm(8)
+      .addReg(X86::R15);
+
+    MI.eraseFromParent();
+    return true;
+  }
+
+  DumpInstructionVerbose(MI);
+  llvm_unreachable("Unhandled Frame SFI");
+}
+
+bool X86NaClRewritePass::ApplyControlSFI(MachineBasicBlock &MBB,
+                                         MachineBasicBlock::iterator MBBI) {
+  TraceLog("ApplyControlSFI", MBB, MBBI);
+  MachineInstr &MI = *MBBI;
+
+  if (!HasControlFlow(MI))
+    return false;
+
+  // Direct branches are OK
+  if (IsDirectBranch(MI))
+    return false;
+
+  DebugLoc DL = MI.getDebugLoc();
+  unsigned Opc = MI.getOpcode();
+
+  // Rewrite indirect jump/call instructions
+  unsigned NewOpc = 0;
+  switch (Opc) {
+  // 32-bit
+  case X86::JMP32r               : NewOpc = X86::NACL_JMP32r; break;
+  case X86::TAILJMPr             : NewOpc = X86::NACL_JMP32r; break;
+  case X86::NACL_CG_CALL32r      : NewOpc = X86::NACL_CALL32r; break;
+  // 64-bit
+  case X86::NACL_CG_JMP64r       : NewOpc = X86::NACL_JMP64r; break;
+  case X86::NACL_CG_CALL64r      : NewOpc = X86::NACL_CALL64r; break;
+  case X86::NACL_CG_TAILJMPr64   : NewOpc = X86::NACL_JMP64r; break;
+  }
+  if (NewOpc) {
+    MachineInstrBuilder NewMI =
+     BuildMI(MBB, MBBI, DL, TII->get(NewOpc))
+       .addOperand(MI.getOperand(0));
+    if (Is64Bit) {
+      NewMI.addReg(X86::R15);
+    }
+    MI.eraseFromParent();
+    return true;
+  }
+
+  // EH_RETURN has a single argment which is not actually used directly.
+  // The argument gives the location where to reposition the stack pointer
+  // before returning. EmitPrologue takes care of that repositioning.
+  // So EH_RETURN just ultimately emits a plain "ret"
+  if (Opc == X86::RET || Opc == X86::EH_RETURN || Opc == X86::EH_RETURN64) {
+    // To maintain compatibility with nacl-as, for now we don't emit naclret.
+    // MI.setDesc(TII->get(Is64Bit ? X86::NACL_RET64 : X86::NACL_RET32));
+    if (Is64Bit) {
+      BuildMI(MBB, MBBI, DL, TII->get(X86::POP64r), X86::RCX);
+      BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_JMP64r))
+        .addReg(X86::ECX)
+        .addReg(X86::R15);
+    } else {
+      BuildMI(MBB, MBBI, DL, TII->get(X86::POP32r), X86::ECX);
+      BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_JMP32r))
+        .addReg(X86::ECX);
+    }
+    MI.eraseFromParent();
+    return true;
+  }
+
+  if (Opc == X86::RETI) {
+    // To maintain compatibility with nacl-as, for now we don't emit naclret.
+    // MI.setDesc(TII->get(X86::NACL_RETI32));
+    assert(!Is64Bit);
+    BuildMI(MBB, MBBI, DL, TII->get(X86::POP32r), X86::ECX);
+    BuildMI(MBB, MBBI, DL, TII->get(X86::ADD32ri), X86::ESP)
+      .addReg(X86::ESP)
+      .addOperand(MI.getOperand(0));
+    BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_JMP32r))
+      .addReg(X86::ECX);
+    MI.eraseFromParent();
+    return true;
+  }
+
+  // Rewrite trap
+  if (Opc == X86::TRAP) {
+    // To maintain compatibility with nacl-as, for now we don't emit nacltrap.
+    // MI.setDesc(TII->get(Is64Bit ? X86::NACL_TRAP64 : X86::NACL_TRAP32));
+    BuildMI(MBB, MBBI, DL, TII->get(X86::MOV32mi))
+      .addReg(Is64Bit ? X86::R15 : 0) // Base
+      .addImm(1) // Scale
+      .addReg(0) // Index
+      .addImm(0) // Offset
+      .addReg(0) // Segment
+      .addImm(0); // Value
+    MI.eraseFromParent();
+    return true;
+  }
+
+  DumpInstructionVerbose(MI);
+  llvm_unreachable("Unhandled Control SFI");
+}
+
+//
+// Sandboxes loads and stores (64-bit only)
+//
+bool X86NaClRewritePass::ApplyMemorySFI(MachineBasicBlock &MBB,
+                                        MachineBasicBlock::iterator MBBI) {
+  TraceLog("ApplyMemorySFI", MBB, MBBI);
+  assert(Is64Bit);
+  MachineInstr &MI = *MBBI;
+
+  if (!IsLoad(MI) && !IsStore(MI))
+    return false;
+
+  if (IsPushPop(MI))
+    return false;
+
+  unsigned MemOp = FindMemoryOperand(MI);
+  assert(isMem(&MI, MemOp));
+  MachineOperand &BaseReg  = MI.getOperand(MemOp + 0);
+  MachineOperand &Scale = MI.getOperand(MemOp + 1);
+  MachineOperand &IndexReg  = MI.getOperand(MemOp + 2);
+  //MachineOperand &Disp = MI.getOperand(MemOp + 3);
+  MachineOperand &SegmentReg = MI.getOperand(MemOp + 4);
+
+  // Make sure the base and index are 64-bit registers.
+  IndexReg.setReg(PromoteRegTo64(IndexReg.getReg()));
+  BaseReg.setReg(PromoteRegTo64(BaseReg.getReg()));
+  assert(IndexReg.getSubReg() == 0);
+  assert(BaseReg.getSubReg() == 0);
+
+  bool AbsoluteBase = IsRegAbsolute(BaseReg.getReg());
+  bool AbsoluteIndex = IsRegAbsolute(IndexReg.getReg());
+  unsigned AddrReg = 0;
+
+  if (AbsoluteBase && AbsoluteIndex) {
+    llvm_unreachable("Unexpected absolute register pair");
+  } else if (AbsoluteBase) {
+    AddrReg = IndexReg.getReg();
+  } else if (AbsoluteIndex) {
+    assert(!BaseReg.getReg() && "Unexpected base register");
+    assert(Scale.getImm() == 1);
+    AddrReg = 0;
+  } else {
+    assert(!BaseReg.getReg() && "Unexpected relative register pair");
+    BaseReg.setReg(X86::R15);
+    AddrReg = IndexReg.getReg();
+  }
+
+  if (AddrReg) {
+    assert(!SegmentReg.getReg() && "Unexpected segment register");
+    SegmentReg.setReg(X86::PSEUDO_NACL_SEG);
+    return true;
+  }
+
+  return false;
+}
+
+bool X86NaClRewritePass::ApplyRewrites(MachineBasicBlock &MBB,
+                                       MachineBasicBlock::iterator MBBI) {
+
+  MachineInstr &MI = *MBBI;
+  DebugLoc DL = MI.getDebugLoc();
+  unsigned Opc = MI.getOpcode();
+
+  // These direct jumps need their opcode rewritten
+  // and variable operands removed.
+  unsigned NewOpc = 0;
+  switch (Opc) {
+  case X86::NACL_CG_CALLpcrel32  : NewOpc = X86::NACL_CALL32d; break;
+  case X86::TAILJMPd             : NewOpc = X86::JMP_4; break;
+  case X86::NACL_CG_TAILJMPd64   : NewOpc = X86::JMP_4; break;
+  case X86::NACL_CG_CALL64pcrel32: NewOpc = X86::NACL_CALL64d; break;
+  }
+  if (NewOpc) {
+    BuildMI(MBB, MBBI, DL, TII->get(NewOpc))
+      .addOperand(MI.getOperand(0));
+    MI.eraseFromParent();
+    return true;
+  }
+
+  if (Opc == X86::NACL_CG_TLS_addr32) {
+    // Rewrite to:
+    //   .bundle_align_end
+    //   .bundle_lock
+    //   leal\t$sym, %eax;
+    //   call\t___tls_get_addr@PLT
+    //   .bundle_unlock
+    BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::BUNDLE_ALIGN_END));
+    BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::BUNDLE_LOCK));
+    BuildMI(MBB, MBBI, DL, TII->get(X86::LEA32r), X86::EAX)
+      .addOperand(MI.getOperand(0))  // Base
+      .addOperand(MI.getOperand(1))  // Scale
+      .addOperand(MI.getOperand(2))  // Index
+      .addGlobalAddress(MI.getOperand(3).getGlobal(), 0, X86II::MO_TLSGD)
+      .addOperand(MI.getOperand(4)); // Segment
+    BuildMI(MBB, MBBI, DL, TII->get(X86::CALLpcrel32))
+      .addExternalSymbol("___tls_get_addr", X86II::MO_PLT);
+    BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::BUNDLE_UNLOCK));
+    MI.eraseFromParent();
+    return true;
+  }
+
+  if (Opc == X86::NACL_CG_TLS_addr64) {
+    // Rewrite to:
+    //   movq $sym, %rdi
+    //   call __tls_get_addr@PLT   // sandbox separately
+    BuildMI(MBB, MBBI, DL, TII->get(X86::MOV64ri32), X86::RDI)
+      .addGlobalAddress(MI.getOperand(3).getGlobal(), 0,
+                        MI.getOperand(3).getTargetFlags());
+    BuildMI(MBB, MBBI, DL, TII->get(X86::NACL_CALL64d))
+      .addExternalSymbol("__tls_get_addr", X86II::MO_PLT);
+    MI.eraseFromParent();
+    return true;
+  }
+  return false;
+}
+
+bool X86NaClRewritePass::AlignJumpTableTargets(MachineFunction &MF) {
+  bool Modified = true;
+
+  MF.setAlignment(5); // log2, 32 = 2^5
+
+  MachineJumpTableInfo *JTI = MF.getJumpTableInfo();
+  if (JTI != NULL) {
+    const std::vector<MachineJumpTableEntry> &JT = JTI->getJumpTables();
+    for (unsigned i = 0; i < JT.size(); ++i) {
+      const std::vector<MachineBasicBlock*> &MBBs = JT[i].MBBs;
+      for (unsigned j = 0; j < MBBs.size(); ++j) {
+        MBBs[j]->setAlignment(32); // in bits
+        Modified |= true;
+      }
+    }
+  }
+  return Modified;
+}
+
+bool X86NaClRewritePass::runOnMachineFunction(MachineFunction &MF) {
+  bool Modified = false;
+
+  TM = &MF.getTarget();
+  TII = TM->getInstrInfo();
+  TRI = TM->getRegisterInfo();
+  Subtarget = &TM->getSubtarget<X86Subtarget>();
+  Is64Bit = Subtarget->is64Bit();
+
+  assert(Subtarget->isTargetNaCl() && "Unexpected target in NaClRewritePass!");
+
+  DEBUG(dbgs() << "*************** NaCl Rewrite Pass ***************\n");
+  for (MachineFunction::iterator MFI = MF.begin(), E = MF.end();
+       MFI != E;
+       ++MFI) {
+    Modified |= runOnMachineBasicBlock(*MFI);
+  }
+  Modified |= AlignJumpTableTargets(MF);
+  DEBUG(dbgs() << "*************** NaCl Rewrite DONE  ***************\n");
+  return Modified;
+}
+
+bool X86NaClRewritePass::runOnMachineBasicBlock(MachineBasicBlock &MBB) {
+  bool Modified = false;
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), NextMBBI = MBBI;
+       MBBI != MBB.end(); MBBI = NextMBBI) {
+    ++NextMBBI;
+    // When one of these methods makes a change,
+    // it returns true, skipping the others.
+    if (ApplyRewrites(MBB, MBBI) ||
+        (Is64Bit && ApplyStackSFI(MBB, MBBI)) ||
+        (Is64Bit && ApplyMemorySFI(MBB, MBBI)) ||
+        (Is64Bit && ApplyFrameSFI(MBB, MBBI)) ||
+        ApplyControlSFI(MBB, MBBI)) {
+      Modified = true;
+    }
+  }
+  return Modified;
+}
+
+static bool IsSandboxed(MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+  // 32-bit
+  case X86::NACL_TRAP32:
+  case X86::NACL_RET32:
+  case X86::NACL_RETI32:
+  case X86::NACL_JMP32r:
+  case X86::NACL_CALL32d:
+  case X86::NACL_CALL32r:
+
+  // 64-bit
+  case X86::NACL_TRAP64:
+  case X86::NACL_RET64:
+  case X86::NACL_JMP64r:
+  case X86::NACL_CALL64r:
+  case X86::NACL_CALL64d:
+
+  case X86::NACL_ASPi8:
+  case X86::NACL_ASPi32:
+  case X86::NACL_SSPi8:
+  case X86::NACL_SSPi32:
+  case X86::NACL_SPADJi32:
+  case X86::NACL_RESTSPr:
+  case X86::NACL_RESTSPm:
+  case X86::NACL_RESTBPr:
+  case X86::NACL_RESTBPm:
+    return true;
+
+  case X86::MOV64rr:
+    // copy from safe regs
+    const MachineOperand &DestReg = MI.getOperand(0);
+    const MachineOperand &SrcReg = MI.getOperand(1);
+    return DestReg.getReg() == X86::RSP && SrcReg.getReg() == X86::RBP;
+  }
+  return false;
+}
+
+static void DumpInstructionVerbose(const MachineInstr &MI) {
+  dbgs() << MI;
+  dbgs() << MI.getNumOperands() << " operands:" << "\n";
+  for (unsigned i = 0; i < MI.getNumOperands(); ++i) {
+    const MachineOperand& op = MI.getOperand(i);
+    dbgs() << "  " << i << "(" << op.getType() << "):" << op << "\n";
+  }
+  dbgs() << "\n";
+}
+
+/// createX86NaClRewritePassPass - returns an instance of the pass.
+namespace llvm {
+  FunctionPass* createX86NaClRewritePass() {
+    return new X86NaClRewritePass();
+  }
+}
diff -r 2b13dadc8fed lib/Target/X86/X86RegisterInfo.cpp
--- a/lib/Target/X86/X86RegisterInfo.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86RegisterInfo.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -310,14 +310,18 @@
 
 const TargetRegisterClass *
 X86RegisterInfo::getPointerRegClass(unsigned Kind) const {
+  // @LOCALMOD-BEGIN
+  bool isPTR64Bit = TM.getSubtarget<X86Subtarget>().has64BitPointers();
+  // @LOCALMOD-END
+
   switch (Kind) {
   default: llvm_unreachable("Unexpected Kind in getPointerRegClass!");
   case 0: // Normal GPRs.
-    if (TM.getSubtarget<X86Subtarget>().is64Bit())
+    if (isPTR64Bit)   // @LOCALMOD
       return &X86::GR64RegClass;
     return &X86::GR32RegClass;
   case 1: // Normal GPRs except the stack pointer (for encoding reasons).
-    if (TM.getSubtarget<X86Subtarget>().is64Bit())
+    if (isPTR64Bit)   // @LOCALMOD
       return &X86::GR64_NOSPRegClass;
     return &X86::GR32_NOSPRegClass;
   case 2: // Available for tailcall (not callee-saved GPRs).
@@ -426,6 +430,21 @@
   Reserved.set(X86::ST5);
   Reserved.set(X86::ST6);
   Reserved.set(X86::ST7);
+
+  // @LOCALMOD-START
+  const X86Subtarget& Subtarget = MF.getTarget().getSubtarget<X86Subtarget>();
+  if (Subtarget.isTargetNaCl64()) {
+    Reserved.set(X86::R15);
+    Reserved.set(X86::R15D);
+    Reserved.set(X86::R15W);
+    Reserved.set(X86::R15B);
+    Reserved.set(X86::RBP);
+    Reserved.set(X86::EBP);
+    Reserved.set(X86::BP);
+    Reserved.set(X86::BPL);
+  }
+  // @LOCALMOD-END
+
   return Reserved;
 }
 
diff -r 2b13dadc8fed lib/Target/X86/X86RegisterInfo.td
--- a/lib/Target/X86/X86RegisterInfo.td	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86RegisterInfo.td	Thu Jun 09 18:06:40 2011 -0700
@@ -252,6 +252,9 @@
   // Pseudo index registers
   def EIZ : Register<"eiz">;
   def RIZ : Register<"riz">;
+  
+  def PSEUDO_NACL_SEG : Register<"nacl">; // @LOCALMOD
+
 }
 
 
@@ -489,6 +492,12 @@
 def GR32_TC   : RegisterClass<"X86", [i32], 32, [EAX, ECX, EDX]> {
   let SubRegClasses = [(GR8 sub_8bit, sub_8bit_hi), (GR16 sub_16bit)];
 }
+// @LOCALMOD-START
+def GR32_TC_64: RegisterClass<"X86", [i32], 32, [EAX, ECX, EDX, ESI, EDI,
+                                                 R8D, R9D, R11D]> {
+  let SubRegClasses = [(GR8 sub_8bit, sub_8bit_hi), (GR16 sub_16bit)];
+}
+// @LOCALMOD-END
 def GR64_TC   : RegisterClass<"X86", [i64], 64, [RAX, RCX, RDX, RSI, RDI,
                                                  R8, R9, R11]> {
   let SubRegClasses = [(GR8 sub_8bit, sub_8bit_hi),
diff -r 2b13dadc8fed lib/Target/X86/X86SelectionDAGInfo.cpp
--- a/lib/Target/X86/X86SelectionDAGInfo.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86SelectionDAGInfo.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -35,6 +35,14 @@
                                          MachinePointerInfo DstPtrInfo) const {
   ConstantSDNode *ConstantSize = dyn_cast<ConstantSDNode>(Size);
 
+  // @LOCALMOD-BEGIN
+  if (Subtarget->isTargetNaCl()) {
+    // TODO: Can we allow this optimization for Native Client?
+    // At the very least, pointer size needs to be fixed below.
+    return SDValue();
+  }
+  // @LOCALMOD-END
+
   // If to a segment-relative address space, use the default lowering.
   if (DstPtrInfo.getAddrSpace() >= 256)
     return SDValue();
@@ -187,6 +195,13 @@
   if (!AlwaysInline && SizeVal > Subtarget->getMaxInlineSizeThreshold())
     return SDValue();
 
+  // @LOCALMOD-BEGIN
+  if (Subtarget->isTargetNaCl()) {
+    // TODO(pdox): Allow use of the NaCl pseudo-instruction for REP MOV
+    return SDValue();
+  }
+  // @LOCALMOD-END
+
   /// If not DWORD aligned, it is more efficient to call the library.  However
   /// if calling the library is not allowed (AlwaysInline), then soldier on as
   /// the code generated here is better than the long load-store sequence we
diff -r 2b13dadc8fed lib/Target/X86/X86Subtarget.h
--- a/lib/Target/X86/X86Subtarget.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86Subtarget.h	Thu Jun 09 18:06:40 2011 -0700
@@ -139,6 +139,9 @@
 
   bool is64Bit() const { return Is64Bit; }
 
+  // @LOCALMOD
+  bool has64BitPointers() const { return is64Bit() && !isTargetNaCl(); }
+
   PICStyles::Style getPICStyle() const { return PICStyle; }
   void setPICStyle(PICStyles::Style Style)  { PICStyle = Style; }
 
@@ -174,6 +177,10 @@
   }
   bool isTargetLinux() const { return TargetTriple.getOS() == Triple::Linux; }
 
+  bool isTargetNaCl() const { return TargetTriple.getOS() == Triple::NativeClient; }
+  bool isTargetNaCl32() const { return isTargetNaCl() && !is64Bit(); }
+  bool isTargetNaCl64() const { return isTargetNaCl() && is64Bit(); }
+
   bool isTargetWindows() const { return TargetTriple.getOS() == Triple::Win32; }
   bool isTargetMingw() const { return TargetTriple.getOS() == Triple::MinGW32; }
   bool isTargetCygwin() const { return TargetTriple.getOS() == Triple::Cygwin; }
diff -r 2b13dadc8fed lib/Target/X86/X86TargetMachine.cpp
--- a/lib/Target/X86/X86TargetMachine.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86TargetMachine.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -96,11 +96,13 @@
                                          const std::string &FS)
   : X86TargetMachine(T, TT, FS, false),
     DataLayout(getSubtargetImpl()->isTargetDarwin() ?
-               "e-p:32:32-f64:32:64-i64:32:64-f80:128:128-n8:16:32" :
-               (getSubtargetImpl()->isTargetCygMing() ||
-                getSubtargetImpl()->isTargetWindows()) ?
-               "e-p:32:32-f64:64:64-i64:64:64-f80:32:32-n8:16:32" :
-               "e-p:32:32-f64:32:64-i64:32:64-f80:32:32-n8:16:32"),
+      "e-p:32:32-f64:32:64-i64:32:64-f80:128:128-n8:16:32" :
+      (getSubtargetImpl()->isTargetCygMing() ||
+       getSubtargetImpl()->isTargetWindows()) ?
+      "e-p:32:32-f64:64:64-i64:64:64-f80:32:32-n8:16:32" :
+      getSubtargetImpl()->isTargetNaCl() ?
+      "e-p:32:32-s:32-f64:64:64-f32:32:32-f80:128:128-i64:64:64-n8:16:32" :
+      "e-p:32:32-f64:32:64-i64:32:64-f80:32:32-n8:16:32"),
     InstrInfo(*this),
     TSInfo(*this),
     TLInfo(*this),
@@ -111,7 +113,9 @@
 X86_64TargetMachine::X86_64TargetMachine(const Target &T, const std::string &TT,
                                          const std::string &FS)
   : X86TargetMachine(T, TT, FS, true),
-    DataLayout("e-p:64:64-s:64-f64:64:64-i64:64:64-f80:128:128-n8:16:32:64"),
+    DataLayout(getSubtargetImpl()->isTargetNaCl() ?
+      "e-p:32:32-s:64-f64:64:64-f32:32:32-f80:128:128-i64:64:64-n8:16:32:64" :
+      "e-p:64:64-s:64-f64:64:64-i64:64:64-f80:128:128-n8:16:32:64"),
     InstrInfo(*this),
     TSInfo(*this),
     TLInfo(*this),
@@ -220,11 +224,20 @@
 
 bool X86TargetMachine::addPreEmitPass(PassManagerBase &PM,
                                       CodeGenOpt::Level OptLevel) {
+  bool Modified = false;
   if (OptLevel != CodeGenOpt::None && Subtarget.hasSSE2()) {
     PM.add(createSSEDomainFixPass());
-    return true;
+    Modified = true;
   }
-  return false;
+
+  // @LOCALMOD-START
+  if (Subtarget.isTargetNaCl()) {
+    PM.add(createX86NaClRewritePass());
+    Modified = true;
+  }
+  // @LOCALMOD-END
+
+  return Modified;
 }
 
 bool X86TargetMachine::addCodeEmitter(PassManagerBase &PM,
diff -r 2b13dadc8fed lib/Target/X86/X86TargetObjectFile.cpp
--- a/lib/Target/X86/X86TargetObjectFile.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86TargetObjectFile.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -9,9 +9,11 @@
 
 #include "X86TargetObjectFile.h"
 #include "X86TargetMachine.h"
+#include "X86Subtarget.h"  // @LOCALMOD
 #include "llvm/CodeGen/MachineModuleInfoImpls.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCSectionELF.h" // @LOCALMOD
 #include "llvm/MC/MCSectionMachO.h"
 #include "llvm/Target/Mangler.h"
 #include "llvm/ADT/SmallString.h"
@@ -116,3 +118,48 @@
 
   return DW_EH_PE_absptr;
 }
+
+// @LOCALMOD-START
+// NOTE: this was largely lifted from
+// lib/Target/ARM/ARMTargetObjectFile.cpp
+//
+// The default is .ctors/.dtors while the arm backend uses
+// .init_array/.fini_array
+//
+// Without this the linker defined symbols __fini_array_start and
+// __fini_array_end do not have useful values. c.f.:
+// http://code.google.com/p/nativeclient/issues/detail?id=805
+void X8664_ELFTargetObjectFile::Initialize(MCContext &Ctx,
+                                           const TargetMachine &TM) {
+  TargetLoweringObjectFileELF::Initialize(Ctx, TM);
+  if (TM.getSubtarget<X86Subtarget>().isTargetNaCl()) {
+    StaticCtorSection =
+      getContext().getELFSection(".init_array", ELF::SHT_INIT_ARRAY,
+                                 ELF::SHF_WRITE |
+                                 ELF::SHF_ALLOC,
+                                 SectionKind::getDataRel());
+    StaticDtorSection =
+      getContext().getELFSection(".fini_array", ELF::SHT_FINI_ARRAY,
+                                 ELF::SHF_WRITE |
+                                 ELF::SHF_ALLOC,
+                                 SectionKind::getDataRel());
+  }
+}
+
+void X8632_ELFTargetObjectFile::Initialize(MCContext &Ctx,
+                                           const TargetMachine &TM) {
+  TargetLoweringObjectFileELF::Initialize(Ctx, TM);
+  if (TM.getSubtarget<X86Subtarget>().isTargetNaCl()) {
+    StaticCtorSection =
+      getContext().getELFSection(".init_array", ELF::SHT_INIT_ARRAY,
+                                 ELF::SHF_WRITE |
+                                 ELF::SHF_ALLOC,
+                                 SectionKind::getDataRel());
+    StaticDtorSection =
+      getContext().getELFSection(".fini_array", ELF::SHT_FINI_ARRAY,
+                                 ELF::SHF_WRITE |
+                                 ELF::SHF_ALLOC,
+                                 SectionKind::getDataRel());
+  }
+}
+// @LOCALMOD-END
diff -r 2b13dadc8fed lib/Target/X86/X86TargetObjectFile.h
--- a/lib/Target/X86/X86TargetObjectFile.h	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Target/X86/X86TargetObjectFile.h	Thu Jun 09 18:06:40 2011 -0700
@@ -36,6 +36,7 @@
     virtual unsigned getLSDAEncoding() const;
     virtual unsigned getFDEEncoding() const;
     virtual unsigned getTTypeEncoding() const;
+    virtual void Initialize(MCContext &Ctx, const TargetMachine &TM); // @LOCALMOD
   };
 
   class X8664_ELFTargetObjectFile : public TargetLoweringObjectFileELF {
@@ -47,6 +48,7 @@
     virtual unsigned getLSDAEncoding() const;
     virtual unsigned getFDEEncoding() const;
     virtual unsigned getTTypeEncoding() const;
+    virtual void Initialize(MCContext &Ctx, const TargetMachine &TM); // @LOCALMOD
   };
 
 } // end namespace llvm
diff -r 2b13dadc8fed lib/Transforms/Makefile
--- a/lib/Transforms/Makefile	Sat Feb 19 00:38:40 2011 +0000
+++ b/lib/Transforms/Makefile	Thu Jun 09 18:06:40 2011 -0700
@@ -10,6 +10,10 @@
 LEVEL = ../..
 PARALLEL_DIRS = Utils Instrumentation Scalar InstCombine IPO Hello
 
+ifeq ($(NACL_SANDBOX),1)
+  PARALLEL_DIRS := $(filter-out Hello, $(PARALLEL_DIRS))
+endif
+
 include $(LEVEL)/Makefile.config
 
 # No support for plugins on windows targets
diff -r 2b13dadc8fed tools/Makefile
--- a/tools/Makefile	Sat Feb 19 00:38:40 2011 +0000
+++ b/tools/Makefile	Thu Jun 09 18:06:40 2011 -0700
@@ -15,10 +15,13 @@
 # NOTE: The tools are organized into five groups of four consisting of one
 # large and three small executables. This is done to minimize memory load
 # in parallel builds.  Please retain this ordering.
+# NOTE: In NaCl we avoid building llvm-ld, llvm-ar and llvm-ranlib.
+# These tools have big problems that have hurt us in the past.
+# See http://codereview.appspot.com/2151043/
 DIRS := llvm-config 
 PARALLEL_DIRS := opt llvm-as llvm-dis \
-                 llc llvm-ranlib llvm-ar llvm-nm \
-                 llvm-ld llvm-prof llvm-link \
+                 llc llvm-nm \
+                 llvm-prof llvm-link \
                  lli llvm-extract llvm-mc \
                  bugpoint llvm-bcanalyzer llvm-stub \
                  llvmc llvm-diff macho-dump llvm-objdump
diff -r 2b13dadc8fed tools/llc/CMakeLists.txt
--- a/tools/llc/CMakeLists.txt	Sat Feb 19 00:38:40 2011 +0000
+++ b/tools/llc/CMakeLists.txt	Thu Jun 09 18:06:40 2011 -0700
@@ -1,5 +1,9 @@
 set(LLVM_LINK_COMPONENTS ${LLVM_TARGETS_TO_BUILD} bitreader asmparser)
 
 add_llvm_tool(llc
+# LOCALMOD BEGIN
+# This file provides wrappers to lseek(2), read(2), etc. 
+  nacl_file.cpp
+# LOCALMOD END
   llc.cpp
   )
diff -r 2b13dadc8fed tools/llc/Makefile
--- a/tools/llc/Makefile	Sat Feb 19 00:38:40 2011 +0000
+++ b/tools/llc/Makefile	Thu Jun 09 18:06:40 2011 -0700
@@ -17,5 +17,9 @@
 
 LINK_COMPONENTS := $(TARGETS_TO_BUILD) bitreader asmparser
 
+ifeq ($(NACL_SRPC),1)
+  LDFLAGS += -Wl,--wrap=open,--wrap=read,--wrap=write,--wrap=close,--wrap=lseek
+endif
+
 include $(LLVM_SRC_ROOT)/Makefile.rules
 
diff -r 2b13dadc8fed tools/llc/llc.cpp
--- a/tools/llc/llc.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/tools/llc/llc.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -26,7 +26,9 @@
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/FormattedStream.h"
 #include "llvm/Support/ManagedStatic.h"
+#if !defined(__native_client__)
 #include "llvm/Support/PluginLoader.h"
+#endif
 #include "llvm/Support/PrettyStackTrace.h"
 #include "llvm/Support/ToolOutputFile.h"
 #include "llvm/Support/Host.h"
@@ -37,6 +39,14 @@
 #include "llvm/Target/TargetRegistry.h"
 #include "llvm/Target/TargetSelect.h"
 #include <memory>
+
+#if defined(__native_client__) && defined(NACL_SRPC)
+#include <fcntl.h>
+#include <sys/nacl_syscalls.h>
+
+extern size_t get_file_size(int dd);
+#endif
+
 using namespace llvm;
 
 // General options for llc.  Other pass-specific options are specified
@@ -191,7 +201,7 @@
 
 // main - Entry point for the llc compiler.
 //
-int main(int argc, char **argv) {
+int llc_main(int argc, char **argv) {
   sys::PrintStackTraceOnErrorSignal();
   PrettyStackTraceProgram X(argc, argv);
 
@@ -212,7 +222,26 @@
   SMDiagnostic Err;
   std::auto_ptr<Module> M;
 
+  // This code opens and passes input file size to the
+  // MemoryBuffer::getOpenFile. This helps prevent llc
+  // from mmap'ing the input bitcode file contents itself.
+#if defined(__native_client__) && defined(NACL_SRPC)
+  std::string ErrMsg;
+  int OpenFlags = O_RDONLY;
+#ifdef O_BINARY
+  OpenFlags |= O_BINARY;  // Open input file in binary mode on win32.
+#endif
+  int FD = ::open(InputFilename.c_str(), OpenFlags);
+  MemoryBuffer *F = MemoryBuffer::getOpenFile(FD, InputFilename.c_str(), &ErrMsg,
+                                                 get_file_size(FD));
+  if (F == 0) {
+    Err = SMDiagnostic(InputFilename.c_str(), "Could not open input file: " + ErrMsg);
+    return 0;
+  }
+  M.reset(ParseIR(F, Err, Context));
+#else
   M.reset(ParseIRFile(InputFilename, Err, Context));
+#endif
   if (M.get() == 0) {
     Err.Print(argv[0], errs());
     return 1;
@@ -346,3 +375,13 @@
 
   return 0;
 }
+
+#if !defined(NACL_SRPC)
+int
+main (int argc, char **argv) {
+  return llc_main(argc, argv);
+}
+#else
+// main() is in nacl_file.cpp.
+#endif
+
diff -r 2b13dadc8fed tools/llc/nacl_file.cpp
--- /dev/null	Thu Jan 01 00:00:00 1970 +0000
+++ b/tools/llc/nacl_file.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -0,0 +1,568 @@
+/* Copyright 2010 The Native Client Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can
+ * be found in the LICENSE file.
+
+ * This file provides wrappers to lseek(2), read(2), etc. that read bytes from
+ * an mmap()'ed buffer.  There are three steps required:
+ *    1. Use linker aliasing to wrap lseek(), etc.  This is done in the
+ *       Makefile using the "-XLinker --wrap -Xlinker lseek" arguments to
+ *       nacl-gcc.  Note that this makes *all* calls to things like read() go
+ *       through these wrappers, so if you also need to read() from, say, a
+ *       socket, this code will not work as-is.
+ *    2. Use lseek(), read() etc as you normally would for a file.
+ *
+ * Note: This code is very temporary and will disappear when the Pepper 2 API
+ * is available in Native Client.
+ */
+
+#if defined(__native_client__) && defined(NACL_SRPC)
+
+#include <errno.h>
+#include <fcntl.h>
+#include <pthread.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/mman.h>
+#include <sys/nacl_syscalls.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+#include <nacl/nacl_srpc.h>
+
+#define MAX_NACL_FILES 256
+#define MMAP_PAGE_SIZE 64 * 1024
+
+extern "C" int __real_open(const char *pathname, int oflags, int mode);
+extern "C" int __wrap_open(const char *pathname, int oflags, int mode);
+extern "C" int __real_close(int dd);
+extern "C" int __wrap_close(int dd);
+extern "C" int __real_read(int dd, void *, size_t);
+extern "C" int __wrap_read(int dd, void *, size_t);
+extern "C" int __real_write(int dd, const void *, size_t);
+extern "C" int __wrap_write(int dd, const void *, size_t);
+extern "C" off_t __real_lseek(int dd, off_t offset, int whence);
+extern "C" off_t __wrap_lseek(int dd, off_t offset, int whence);
+extern int llc_main(int argc, char **argv);
+
+static int nacl_file_initialized = 0;
+
+struct NaCl_file_map {
+  char *filename;
+  int real_fd;
+  pthread_mutex_t mu;
+  size_t size;
+  struct NaCl_file_map *next;
+};
+
+struct NaCl_file_map *nacl_fs = NULL;
+static pthread_mutex_t nacl_fs_mu = PTHREAD_MUTEX_INITIALIZER;
+
+struct NaCl_file {
+  int mode;
+  off_t pos;
+  pthread_mutex_t mu;
+  struct NaCl_file_map *file_ptr;
+};
+
+static struct NaCl_file nacl_files[MAX_NACL_FILES];
+
+/* Check to see the |dd| is a valid NaCl shm file descriptor */
+static int IsValidDescriptor(int dd) {
+  return nacl_file_initialized && (dd >= 3) && (dd < MAX_NACL_FILES);
+}
+
+static size_t roundToNextPageSize(size_t size) {
+  size_t count_up = size + (MMAP_PAGE_SIZE-1);
+  return (count_up & ~(MMAP_PAGE_SIZE-1));
+}
+
+/* Create a new entry representing the shm file descriptor.
+   Returns 0 on success. */
+int NaClFile_fd(char *pathname, int fd) {
+  int i;
+  struct stat stb;
+  struct NaCl_file_map *entry;
+
+  if (0 != fstat(fd, &stb)) {
+    errno = EBADF;
+    return -1;
+  }
+
+  if (S_IFSHM != (stb.st_mode & S_IFMT)) {
+    printf("nacl_file: %d normal file?!\n", fd);
+    return -1;
+  }
+
+  entry = (struct NaCl_file_map*)(malloc(sizeof *entry));
+
+  if (NULL == entry) {
+    fprintf(stderr, "nacl_file: No memory for file map for %s\n", pathname);
+    exit(1);
+  }
+  if (NULL == (entry->filename = strdup(pathname))) {
+    fprintf(stderr, "nacl_file: No memory for file path %s\n", pathname);
+    exit(1);
+  }
+  entry->real_fd = fd;
+  entry->size = stb.st_size;
+  pthread_mutex_init(&(entry->mu), NULL);
+
+  pthread_mutex_lock(&nacl_fs_mu);
+
+  entry->next = nacl_fs;
+  nacl_fs = entry;
+
+  if (!nacl_file_initialized) {
+    for (i = 0; i < MAX_NACL_FILES; ++i) {
+      pthread_mutex_init(&nacl_files[i].mu, NULL);
+      nacl_files[i].file_ptr = NULL;
+    }
+    nacl_file_initialized = 1;
+  }
+
+  pthread_mutex_unlock(&nacl_fs_mu);
+
+  return 0;
+}
+
+/* Create a new file and return the fd for it.
+   Returns 0 on success. */
+int NaClFile_new(char *pathname) {
+  int fd = imc_mem_obj_create(MMAP_PAGE_SIZE);
+  if (fd < 0) {
+    printf("nacl_file: imc_mem_obj_create failed %d\n", fd);
+    return -1;
+  }
+  return NaClFile_fd(pathname, fd);
+}
+
+/* Create a new file for the specified size and return the fd for it.
+   Returns 0 on success. */
+int NaClFile_new_of_size(char *pathname, size_t size) {
+  int fd;
+  size_t count_up = roundToNextPageSize(size);
+
+  fd = imc_mem_obj_create(count_up);
+  if (fd < 0) {
+    printf("nacl_file: imc_mem_obj_create failed %d\n", fd);
+    return -1;
+  }
+
+  return NaClFile_fd(pathname, fd);
+}
+
+size_t get_file_size(int dd) {
+  size_t file_size;
+
+  if (!IsValidDescriptor(dd)) {
+    errno = EBADF;
+    return dd;
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].mu);
+
+  if (NULL == nacl_files[dd].file_ptr) {
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    errno = EBADF;
+    return dd;
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].file_ptr->mu);
+
+  file_size = nacl_files[dd].file_ptr->size;
+
+  pthread_mutex_unlock(&nacl_files[dd].file_ptr->mu);
+  pthread_mutex_unlock(&nacl_files[dd].mu);
+
+  return file_size;
+}
+
+int get_real_fd(int dd) {
+  int fd;
+
+  if (!IsValidDescriptor(dd)) {
+    errno = EBADF;
+    return dd;
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].mu);
+
+  if (NULL == nacl_files[dd].file_ptr) {
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    errno = EBADF;
+    return dd;
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].file_ptr->mu);
+
+  fd = nacl_files[dd].file_ptr->real_fd;
+
+  pthread_mutex_unlock(&nacl_files[dd].file_ptr->mu);
+  pthread_mutex_unlock(&nacl_files[dd].mu);
+
+  return fd;
+}
+
+int get_real_fd_by_name(const char *pathname) {
+  int fd = -1;
+  struct NaCl_file_map *entry;
+
+  for (entry = nacl_fs; NULL != entry; entry = entry->next) {
+    if (!strcmp(pathname, entry->filename)) {
+      fd = entry->real_fd;
+      break;
+    }
+  }
+
+  if (-1 == fd) {
+    errno = EBADF;
+  }
+
+  return fd;
+}
+
+/* Adjust the size of a nacl file.
+   Changes the real_fd of a file.
+   Returns 0 on success. */
+static int
+adjust_file_size(int dd, size_t new_size) {
+  int new_fd = -1;
+  off_t base_pos;
+  size_t count;
+  size_t final_base;
+  uint8_t *new_data;
+  uint8_t *old_data;
+  struct NaCl_file_map *entry;
+
+  if (!IsValidDescriptor(dd)) {
+    errno = EBADF;
+    return -1;
+  }
+
+  /* TODO(abetul): check if caller has already acquired the mutex for file */
+
+  if (NULL == nacl_files[dd].file_ptr) {
+    errno = EBADF;
+    return -1;
+  }
+
+  entry = nacl_files[dd].file_ptr;
+  new_fd = imc_mem_obj_create(new_size);
+  if (new_fd < 0) {
+    printf("nacl_file: imc_mem_obj_create failed %d\n", new_fd);
+    return -1;
+  }
+
+  /* copy contents over */
+  final_base = entry->size & ~(MMAP_PAGE_SIZE-1);
+  for (base_pos = 0; (size_t) base_pos < final_base;
+       base_pos += MMAP_PAGE_SIZE) {
+    old_data = (uint8_t *) mmap(NULL, MMAP_PAGE_SIZE, PROT_READ, MAP_SHARED,
+                               entry->real_fd, base_pos);
+    new_data = (uint8_t *) mmap(NULL, MMAP_PAGE_SIZE, PROT_WRITE, MAP_SHARED,
+                               new_fd, base_pos);
+    if (NULL != old_data && NULL != new_data) {
+      memcpy(new_data, old_data, MMAP_PAGE_SIZE);
+      munmap(old_data, MMAP_PAGE_SIZE);
+      munmap(new_data, MMAP_PAGE_SIZE);
+    } else {
+      printf("nacl_file: mmap call failed!\n");
+      return -1;
+    }
+  }
+
+  count = entry->size - final_base;
+
+  if (count > 0) {
+    old_data = (uint8_t *) mmap(NULL, MMAP_PAGE_SIZE, PROT_READ, MAP_SHARED,
+                               entry->real_fd, base_pos);
+    new_data = (uint8_t *) mmap(NULL, MMAP_PAGE_SIZE, PROT_WRITE, MAP_SHARED,
+                               new_fd, base_pos);
+    if (NULL != old_data && NULL != new_data) {
+      memcpy(new_data, old_data, count);
+      munmap(old_data, MMAP_PAGE_SIZE);
+      munmap(new_data, MMAP_PAGE_SIZE);
+    } else {
+      printf("nacl_file: mmap call failed!\n");
+      return -1;
+    }
+  }
+
+  if (__real_close(entry->real_fd) < 0) {
+    printf("nacl_file: close in size adjust failed!\n");
+    return -1;
+  }
+
+  entry->real_fd = new_fd;
+  entry->size = new_size;
+
+  return 0;
+}
+
+int __wrap_open(const char *pathname, int oflags, int mode) {
+  int dd = -1;
+  int i;
+  struct NaCl_file_map *entry;
+
+  for (entry = nacl_fs; NULL != entry; entry = entry->next) {
+    if (!strcmp(pathname, entry->filename)) {
+      break;
+    }
+  }
+
+  if (NULL == entry) {
+    return __real_open(pathname, oflags, mode);
+  }
+
+  for (i = 3; i < MAX_NACL_FILES; i++) {
+    if (NULL == nacl_files[i].file_ptr) {
+      dd = i;
+      break;
+    }
+  }
+
+  if (-1 == dd) {
+    fprintf(stderr, "nacl_file: Max open file count has been reached\n");
+    return -1;
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].mu);
+
+  nacl_files[dd].pos = 0;
+  nacl_files[dd].mode = oflags;
+  nacl_files[dd].file_ptr = entry;
+
+  pthread_mutex_unlock(&nacl_files[dd].mu);
+
+  return dd;
+}
+
+int __wrap_close(int dd) {
+
+  if (!IsValidDescriptor(dd)) {
+    return __real_close(dd);
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].mu);
+
+  if (NULL == nacl_files[dd].file_ptr) {
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    return __real_close(dd);
+  }
+
+  nacl_files[dd].file_ptr = NULL;
+
+  pthread_mutex_unlock(&nacl_files[dd].mu);
+
+  return 0;
+}
+
+int __wrap_read(int dd, void *buf, size_t count) {
+  int got = 0;
+  uint8_t *data;
+  off_t base_pos;
+  off_t adj;
+  size_t count_up;
+
+  if (!IsValidDescriptor(dd)) {
+    return __real_read(dd, buf, count);
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].mu);
+
+  if (NULL == nacl_files[dd].file_ptr) {
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    return __real_read(dd, buf, count);
+  }
+
+  if ((nacl_files[dd].mode & !O_RDONLY) != O_RDONLY) {
+    printf("nacl_file: invalid mode %d\n", nacl_files[dd].mode);
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    return -1;
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].file_ptr->mu);
+
+  /* make sure we don't read beyond end of file */
+  if ((nacl_files[dd].pos + count) > nacl_files[dd].file_ptr->size) {
+    if ((nacl_files[dd].file_ptr->size - nacl_files[dd].pos) < 0)
+      count = 0;
+    else
+      count = nacl_files[dd].file_ptr->size - nacl_files[dd].pos;
+    printf("nacl_file: warning, attempting read outside of file!\n");
+  }
+
+  /* use mmap to read data */
+  base_pos = nacl_files[dd].pos & ~(MMAP_PAGE_SIZE-1);
+  adj = nacl_files[dd].pos - base_pos;
+  /* round count value to next 64KB */
+  count_up = roundToNextPageSize(count + adj);
+  data = (uint8_t *) mmap(NULL, count_up, PROT_READ, MAP_SHARED,
+                         nacl_files[dd].file_ptr->real_fd, base_pos);
+  if (NULL != data) {
+    memcpy(buf, data + adj, count);
+    munmap(data, count_up);
+    got = count;
+  } else {
+    printf("nacl_file: mmap call failed!\n");
+  }
+
+  if (got > 0) {
+    nacl_files[dd].pos += got;
+  }
+
+  pthread_mutex_unlock(&nacl_files[dd].file_ptr->mu);
+  pthread_mutex_unlock(&nacl_files[dd].mu);
+
+  return got;
+}
+
+int __wrap_write(int dd, const void *buf, size_t count) {
+  int got = 0;
+  uint8_t *data;
+  off_t base_pos;
+  off_t adj;
+  size_t count_up;
+  size_t new_size;
+
+  if (!IsValidDescriptor(dd)) {
+    return __real_write(dd, buf, count);
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].mu);
+
+  if (NULL == nacl_files[dd].file_ptr) {
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    return __real_write(dd, buf, count);
+  }
+
+  if ((nacl_files[dd].mode & (O_WRONLY | O_RDWR)) == 0) {
+    printf("nacl_file: invalid mode %d\n", nacl_files[dd].mode);
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    return -1;
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].file_ptr->mu);
+
+  /* adjust file size if writing past the current end */
+  new_size = nacl_files[dd].file_ptr->size;
+  while ((nacl_files[dd].pos + count) > new_size) {
+    /* double the file size */
+    new_size <<= 1;
+  }
+
+  if (new_size > nacl_files[dd].file_ptr->size) {
+    if (adjust_file_size(dd, new_size) != 0) {
+      pthread_mutex_unlock(&nacl_files[dd].file_ptr->mu);
+      pthread_mutex_unlock(&nacl_files[dd].mu);
+      printf("nacl_file: failed to adjust file size %d\n", dd);
+      return -1;
+    }
+  }
+
+  /* use mmap to write data */
+  base_pos = nacl_files[dd].pos & ~(MMAP_PAGE_SIZE-1);
+  adj = nacl_files[dd].pos - base_pos;
+  /* round count value to next 64KB */
+  count_up = roundToNextPageSize(count + adj);
+  data = (uint8_t *) mmap(NULL, count_up, PROT_WRITE, MAP_SHARED,
+                         nacl_files[dd].file_ptr->real_fd, base_pos);
+  if (NULL != data) {
+    memcpy(data + adj, buf, count);
+    munmap(data, count_up);
+    got = count;
+  } else {
+    printf("nacl_file: mmap call failed!\n");
+  }
+
+  if (got > 0) {
+    nacl_files[dd].pos += got;
+  }
+
+  pthread_mutex_unlock(&nacl_files[dd].file_ptr->mu);
+  pthread_mutex_unlock(&nacl_files[dd].mu);
+
+  return got;
+}
+
+off_t __wrap_lseek(int dd, off_t offset, int whence) {
+  if (!IsValidDescriptor(dd)) {
+    return __real_lseek(dd, offset, whence);
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].mu);
+
+  if (NULL == nacl_files[dd].file_ptr) {
+    pthread_mutex_unlock(&nacl_files[dd].mu);
+    return __real_lseek(dd, offset, whence);
+  }
+
+  pthread_mutex_lock(&nacl_files[dd].file_ptr->mu);
+
+  switch (whence) {
+    case SEEK_SET:
+      break;
+    case SEEK_CUR:
+      offset = nacl_files[dd].pos + offset;
+      break;
+    case SEEK_END:
+      offset = nacl_files[dd].file_ptr->size + offset;
+      break;
+  }
+  if (offset < 0) {
+    offset = -1;
+  }
+  if (-1 != offset) {
+    nacl_files[dd].pos = offset;
+  }
+
+  pthread_mutex_unlock(&nacl_files[dd].file_ptr->mu);
+  pthread_mutex_unlock(&nacl_files[dd].mu);
+
+  return offset;
+}
+
+void
+translate(NaClSrpcRpc *rpc,
+          NaClSrpcArg **in_args,
+          NaClSrpcArg **out_args,
+          NaClSrpcClosure *done) {
+  char *argv[] = {"llc", "-march=x86-64", "-mcpu=core2",
+                  "-asm-verbose=false", "-filetype=obj",
+                  "bitcode_combined", "-o", "obj_combined"};
+  int kArgvLength = sizeof argv / sizeof argv[0];
+  /* Input bitcode file. */
+  NaClFile_fd("bitcode_combined", in_args[0]->u.hval);
+
+  /* Define output file. */
+  NaClFile_new("obj_combined");
+
+  /* Call main. */
+  llc_main(kArgvLength, argv);
+
+  /* Save obj fd for return. */
+  out_args[0]->u.hval = get_real_fd_by_name("obj_combined");
+
+  rpc->result = NACL_SRPC_RESULT_OK;
+  done->Run(done);
+}
+
+const struct NaClSrpcHandlerDesc srpc_methods[] = {
+  { "Translate:h:h", translate },
+  { NULL, NULL },
+};
+
+int
+main() {
+  if (!NaClSrpcModuleInit()) {
+    return 1;
+  }
+  if (!NaClSrpcAcceptClientConnection(srpc_methods)) {
+    return 1;
+  }
+  NaClSrpcModuleFini();
+  return 0;
+}
+
+#endif
diff -r 2b13dadc8fed tools/lto/lto.exports
--- a/tools/lto/lto.exports	Sat Feb 19 00:38:40 2011 +0000
+++ b/tools/lto/lto.exports	Thu Jun 09 18:06:40 2011 -0700
@@ -18,6 +18,9 @@
 lto_codegen_compile
 lto_codegen_create
 lto_codegen_dispose
+lto_codegen_set_assembler_args
+lto_codegen_set_assembler_path
+lto_codegen_set_cpu
 lto_codegen_set_debug_model
 lto_codegen_set_pic_model
 lto_codegen_write_merged_modules
diff -r 2b13dadc8fed utils/Makefile
--- a/utils/Makefile	Sat Feb 19 00:38:40 2011 +0000
+++ b/utils/Makefile	Thu Jun 09 18:06:40 2011 -0700
@@ -11,6 +11,11 @@
 PARALLEL_DIRS := FileCheck FileUpdate TableGen PerfectShuffle \
 	      count fpcmp llvm-lit not unittest
 
+ifeq ($(NACL_SANDBOX),1)
+  PARALLEL_DIRS := $(filter-out FileCheck FileUpdate PerfectShuffle \
+                count fpcmp llvm-lit not unittest, $(PARALLEL_DIRS))
+endif
+
 EXTRA_DIST := cgiplotNLT.pl check-each-file codegen-diff countloc.sh \
               DSAclean.py DSAextract.py emacs findsym.pl GenLibDeps.pl \
 	      getsrcs.sh importNLT.pl llvmdo llvmgrep llvm-native-gcc \
diff -r 2b13dadc8fed utils/TableGen/CodeGenTarget.cpp
--- a/utils/TableGen/CodeGenTarget.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/utils/TableGen/CodeGenTarget.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -355,6 +355,12 @@
     "DBG_VALUE",
     "REG_SEQUENCE",
     "COPY",
+    // @LOCALMOD-BEGIN
+    "BUNDLE_ALIGN_START",
+    "BUNDLE_ALIGN_END",
+    "BUNDLE_LOCK",
+    "BUNDLE_UNLOCK",
+    // @LOCALMOD-END
     0
   };
   const DenseMap<const Record*, CodeGenInstruction*> &Insts = getInstructions();
diff -r 2b13dadc8fed utils/TableGen/EDEmitter.cpp
--- a/utils/TableGen/EDEmitter.cpp	Sat Feb 19 00:38:40 2011 +0000
+++ b/utils/TableGen/EDEmitter.cpp	Thu Jun 09 18:06:40 2011 -0700
@@ -263,6 +263,7 @@
   REG("FR32");
   REG("RFP32");
   REG("GR64");
+  REG("GR32_TC_64"); // @LOCALMOD
   REG("GR64_TC");
   REG("FR64");
   REG("VR64");
